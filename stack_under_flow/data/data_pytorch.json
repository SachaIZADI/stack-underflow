[{"title": "How does the &quot;view&quot; method work in PyTorch?", "question_body": "<p>I am confused about the method <code>view()</code> in the following code snippet.</p>\n\n<pre><code>class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool  = nn.MaxPool2d(2,2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1   = nn.Linear(16*5*5, 120)\n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16*5*5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\n</code></pre>\n\n<p>My confusion is regarding the following line.</p>\n\n<pre><code>x = x.view(-1, 16*5*5)\n</code></pre>\n\n<p>What does <code>tensor.view()</code> function do? I have seen its usage in many places, but I can't understand how it interprets its parameters. </p>\n\n<p>What happens if I give negative values as parameters to the <code>view()</code> function? For example, what happens if I call, <code>tensor_variable.view(1, 1, -1)</code>?</p>\n\n<p>Can anyone explain the main principle of <code>view()</code> function with some examples?</p>\n", "link": "https://stackoverflow.com/questions/42479902/how-does-the-view-method-work-in-pytorch", "question_id": 42479902, "accepted_answer_id": 42482819, "answer_body": "<p>The view function is meant to reshape the tensor. </p>\n\n<p>Say you have a tensor</p>\n\n<pre><code>import torch\na = torch.range(1, 16)\n</code></pre>\n\n<p><code>a</code> is a tensor that has 16 elements from 1 to 16(included). If you want to reshape this tensor to make it a <code>4 x 4</code> tensor then you can use </p>\n\n<pre><code>a = a.view(4, 4)\n</code></pre>\n\n<p>Now <code>a</code> will be a <code>4 x 4</code> tensor. <em>Note that after the reshape the total number of elements need to remain the same. Reshaping the tensor <code>a</code> to a <code>3 x 5</code> tensor would not be appropriate.</em></p>\n\n<h3>What is the meaning of parameter -1?</h3>\n\n<p>If there is any situation that you don't know how many rows you want but are sure of the number of columns, then you can specify this with a -1. (<em>Note that you can extend this to tensors with more dimensions. Only one of the axis value can be -1</em>). This is a way of telling the library: \"give me a tensor that has these many columns and you compute the appropriate number of rows that is necessary to make this happen\".</p>\n\n<p>This can be seen in the neural network code that you have given above. After the line <code>x = self.pool(F.relu(self.conv2(x)))</code> in the forward function, you will have a 16 depth feature map. You have to flatten this to give it to the fully connected layer. So you tell pytorch to reshape the tensor you obtained to have specific number of columns and tell it to decide the number of rows by itself.</p>\n\n<p>Drawing a similarity between numpy and pytorch, <code>view</code> is similar to numpy's <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html\" rel=\"noreferrer\">reshape</a> function. </p>\n"}, {"title": "Best way to save a trained model in PyTorch?", "question_body": "<p>I was looking for alternative ways to save a trained model in PyTorch. So far, I have found two alternatives.</p>\n\n<ol>\n<li><a href=\"https://github.com/torch/torch7/blob/master/doc/serialization.md#torchsavefilename-object--format-referenced\" rel=\"noreferrer\">torch.save()</a> to save a model and <a href=\"https://github.com/torch/torch7/blob/master/doc/serialization.md#object-torchloadfilename--format-referenced\" rel=\"noreferrer\">torch.load()</a> to load a model.</li>\n<li><a href=\"http://pytorch.org/docs/nn.html#torch.nn.Module.state_dict\" rel=\"noreferrer\">model.state_dict()</a> to save a trained model and <a href=\"http://pytorch.org/docs/nn.html#torch.nn.Module.load_state_dict\" rel=\"noreferrer\">model.load_state_dict()</a> to load the saved model.</li>\n</ol>\n\n<p>I have come across to this <a href=\"https://discuss.pytorch.org/t/how-to-save-load-torch-models/718\" rel=\"noreferrer\">discussion</a> where approach 2 is recommended over approach 1.</p>\n\n<p>My question is, why the second approach is preferred? Is it only because <a href=\"http://pytorch.org/docs/nn.html\" rel=\"noreferrer\">torch.nn</a> modules have those two function and we are encouraged to use them?</p>\n", "link": "https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch", "question_id": 42703500, "accepted_answer_id": 43819235, "answer_body": "<p>I've found <a href=\"https://github.com/pytorch/pytorch/blob/761d6799beb3afa03657a71776412a2171ee7533/docs/source/notes/serialization.rst\" rel=\"noreferrer\">this page</a> on their github repo, I'll just paste the content here.</p>\n\n<hr>\n\n<h1>Recommended approach for saving a model</h1>\n\n<p>There are two main approaches for serializing and restoring a model.</p>\n\n<p>The first (recommended) saves and loads only the model parameters:</p>\n\n<pre><code>torch.save(the_model.state_dict(), PATH)\n</code></pre>\n\n<p>Then later:</p>\n\n<pre><code>the_model = TheModelClass(*args, **kwargs)\nthe_model.load_state_dict(torch.load(PATH))\n</code></pre>\n\n<p>The second saves and loads the entire model:</p>\n\n<pre><code>torch.save(the_model, PATH)\n</code></pre>\n\n<p>Then later:</p>\n\n<pre><code>the_model = torch.load(PATH)\n</code></pre>\n\n<p>However in this case, the serialized data is bound to the specific classes\nand the exact directory structure used, so it can break in various ways when\nused in other projects, or after some serious refactors.</p>\n"}, {"title": "Pytorch, what are the gradient arguments", "question_body": "<p>I am reading through the documentation of PyTorch and found an example where they write </p>\n\n<pre><code>gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\ny.backward(gradients)\nprint(x.grad)\n</code></pre>\n\n<p>where x was an initial variable, from which y was constructed (a 3-vector). The question is, what are the 0.1, 1.0 and 0.0001 arguments of the gradients tensor ? The documentation is not very clear on that.</p>\n", "link": "https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments", "question_id": 43451125, "accepted_answer_id": 57540363, "answer_body": "<blockquote>\n  <p>The original code I haven't found on PyTorch website anymore.</p>\n</blockquote>\n\n<pre><code>gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\ny.backward(gradients)\nprint(x.grad)\n</code></pre>\n\n<p>The problem with the code above there is no function based on what to calculate the gradients. This means we don't know how many parameters (arguments the function takes) and the dimension of parameters. </p>\n\n<p>To fully understand this I created several examples close to the original:</p>\n\n<blockquote>\n  <p>Example 1:</p>\n</blockquote>\n\n<pre><code>a = torch.tensor([1.0, 2.0, 3.0], requires_grad = True)\nb = torch.tensor([3.0, 4.0, 5.0], requires_grad = True)\nc = torch.tensor([6.0, 7.0, 8.0], requires_grad = True)\n\ny=3*a + 2*b*b + torch.log(c)    \ngradients = torch.FloatTensor([0.1, 1.0, 0.0001])\ny.backward(gradients,retain_graph=True)    \n\nprint(a.grad) # tensor([3.0000e-01, 3.0000e+00, 3.0000e-04])\nprint(b.grad) # tensor([1.2000e+00, 1.6000e+01, 2.0000e-03])\nprint(c.grad) # tensor([1.6667e-02, 1.4286e-01, 1.2500e-05])\n</code></pre>\n\n<p>As you can see I assumed in the first example our function is <code>y=3*a + 2*b*b + torch.log(c)</code> and the parameters are tensors with three elements inside.</p>\n\n<p>But there is another option:</p>\n\n<blockquote>\n  <p>Example 2:</p>\n</blockquote>\n\n<pre><code>import torch\n\na = torch.tensor(1.0, requires_grad = True)\nb = torch.tensor(1.0, requires_grad = True)\nc = torch.tensor(1.0, requires_grad = True)\n\ny=3*a + 2*b*b + torch.log(c)    \ngradients = torch.FloatTensor([0.1, 1.0, 0.0001])\ny.backward(gradients)\n\nprint(a.grad) # tensor(3.3003)\nprint(b.grad) # tensor(4.4004)\nprint(c.grad) # tensor(1.1001)\n</code></pre>\n\n<p>The <code>gradients = torch.FloatTensor([0.1, 1.0, 0.0001])</code> is the accumulator.</p>\n\n<p>The next example would provide identical results. </p>\n\n<blockquote>\n  <p>Example 3: </p>\n</blockquote>\n\n<pre><code>a = torch.tensor(1.0, requires_grad = True)\nb = torch.tensor(1.0, requires_grad = True)\nc = torch.tensor(1.0, requires_grad = True)\n\ny=3*a + 2*b*b + torch.log(c)\n\ngradients = torch.FloatTensor([0.1])\ny.backward(gradients,retain_graph=True)\ngradients = torch.FloatTensor([1.0])\ny.backward(gradients,retain_graph=True)\ngradients = torch.FloatTensor([0.0001])\ny.backward(gradients)\n\nprint(a.grad) # tensor(3.3003)\nprint(b.grad) # tensor(4.4004)\nprint(c.grad) # tensor(1.1001)\n</code></pre>\n\n<p>As you may hear PyTorch autograd system calculation is equivalent to Jacobian product.</p>\n\n<p><a href=\"https://i.stack.imgur.com/sDlmj.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/sDlmj.png\" alt=\"Jacobian\"></a> </p>\n\n<p>In case you have a function, like we did:</p>\n\n<pre><code>y=3*a + 2*b*b + torch.log(c)\n</code></pre>\n\n<p>Jacobian would be <code>[3, 4*b, 1/c]</code>. However, this <a href=\"https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant\" rel=\"nofollow noreferrer\">Jacobian</a> is not how PyTorch is doing things to calculate the gradients at certain point.</p>\n\n<p>For the previous function PyTorch would do for instance <code>\u03b4y/\u03b4b</code>, for <code>b=1</code> and <code>b=1+\u03b5</code> where \u03b5 is small. So there is nothing like symbolic math involved.</p>\n\n<p>If you don't use gradients in <code>y.backward()</code>:</p>\n\n<blockquote>\n  <p>Example 4</p>\n</blockquote>\n\n<pre><code>a = torch.tensor(0.1, requires_grad = True)\nb = torch.tensor(1.0, requires_grad = True)\nc = torch.tensor(0.1, requires_grad = True)\ny=3*a + 2*b*b + torch.log(c)\n\ny.backward()\n\nprint(a.grad) # tensor(3.)\nprint(b.grad) # tensor(4.)\nprint(c.grad) # tensor(10.)\n</code></pre>\n\n<p>You will simple get the result at a point, based on how you set your <code>a</code>, <code>b</code>, <code>c</code> tensors initially.</p>\n\n<p>Be careful how you initialize your <code>a</code>, <code>b</code>, <code>c</code>:</p>\n\n<blockquote>\n  <p>Example 5:</p>\n</blockquote>\n\n<pre><code>a = torch.empty(1, requires_grad = True, pin_memory=True)\nb = torch.empty(1, requires_grad = True, pin_memory=True)\nc = torch.empty(1, requires_grad = True, pin_memory=True)\n\ny=3*a + 2*b*b + torch.log(c)\n\ngradients = torch.FloatTensor([0.1, 1.0, 0.0001])\ny.backward(gradients)\n\nprint(a.grad) # tensor([3.3003])\nprint(b.grad) # tensor([0.])\nprint(c.grad) # tensor([inf])\n</code></pre>\n\n<p>If you use <code>torch.empty()</code> and don't use <code>pin_memory=True</code> you may have different results every time.</p>\n\n<p>Also, note gradients are like accumulators so zero them when needed.</p>\n\n<blockquote>\n  <p>Example 6:</p>\n</blockquote>\n\n<pre><code>a = torch.tensor(1.0, requires_grad = True)\nb = torch.tensor(1.0, requires_grad = True)\nc = torch.tensor(1.0, requires_grad = True)\ny=3*a + 2*b*b + torch.log(c)\n\ny.backward(retain_graph=True)\ny.backward()\n\nprint(a.grad) # tensor(6.)\nprint(b.grad) # tensor(8.)\nprint(c.grad) # tensor(2.)\n</code></pre>\n\n<p>Lastly I just wanted to state some terms PyTorch uses: </p>\n\n<p>PyTorch creates a <strong>dynamic computational graph</strong> when calculating the gradients. This looks much like a tree.</p>\n\n<p>So you will often hear the <em>leaves</em> of this tree are <strong>input tensors</strong> and the <em>root</em> is <strong>output tensor</strong>.</p>\n\n<p>Gradients are calculated by tracing the graph from the root to the leaf and multiplying every gradient in the way using the <strong>chain rule</strong>.</p>\n"}, {"title": "How to check if pytorch is using the GPU?", "question_body": "<p>I would like to know if <code>pytorch</code> is using my GPU. It's possible to detect with <code>nvidia-smi</code> if there is any activity from the GPU during the process, but I want something written in a <code>python</code> script.</p>\n\n<p>Is there a way to do so?</p>\n", "link": "https://stackoverflow.com/questions/48152674/how-to-check-if-pytorch-is-using-the-gpu", "question_id": 48152674, "accepted_answer_id": 48152675, "answer_body": "<p>This is going to work :</p>\n\n<pre><code>In [1]: import torch\n\nIn [2]: torch.cuda.current_device()\nOut[2]: 0\n\nIn [3]: torch.cuda.device(0)\nOut[3]: &lt;torch.cuda.device at 0x7efce0b03be0&gt;\n\nIn [4]: torch.cuda.device_count()\nOut[4]: 1\n\nIn [5]: torch.cuda.get_device_name(0)\nOut[5]: 'GeForce GTX 950M'\n\nIn [6]: torch.cuda.is_available()\nOut[6]: True\n</code></pre>\n\n<p>This tells me the GPU <code>GeForce GTX 950M</code> is being used by <code>PyTorch</code>.</p>\n"}, {"title": "Model summary in pytorch", "question_body": "<p>Is there any way, I can print the summary of a model in PyTorch like <code>model.summary()</code> method does in Keras as follows?</p>\n\n<pre><code>Model Summary:\n____________________________________________________________________________________________________\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\ninput_1 (InputLayer)             (None, 1, 15, 27)     0                                            \n____________________________________________________________________________________________________\nconvolution2d_1 (Convolution2D)  (None, 8, 15, 27)     872         input_1[0][0]                    \n____________________________________________________________________________________________________\nmaxpooling2d_1 (MaxPooling2D)    (None, 8, 7, 27)      0           convolution2d_1[0][0]            \n____________________________________________________________________________________________________\nflatten_1 (Flatten)              (None, 1512)          0           maxpooling2d_1[0][0]             \n____________________________________________________________________________________________________\ndense_1 (Dense)                  (None, 1)             1513        flatten_1[0][0]                  \n====================================================================================================\nTotal params: 2,385\nTrainable params: 2,385\nNon-trainable params: 0\n</code></pre>\n", "link": "https://stackoverflow.com/questions/42480111/model-summary-in-pytorch", "question_id": 42480111, "accepted_answer_id": 42616812, "answer_body": "<p>While you will not get as detailed information about the model as in Keras' model.summary, simply printing the model will give you some idea about the different layers involved and their specifications.</p>\n\n<p>For instance:</p>\n\n<pre><code>from torchvision import models\nmodel = models.vgg16()\nprint(model)\n</code></pre>\n\n<p>The output in this case would be something as follows:</p>\n\n<pre><code>VGG (\n  (features): Sequential (\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU (inplace)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU (inplace)\n    (4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU (inplace)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU (inplace)\n    (9): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU (inplace)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU (inplace)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU (inplace)\n    (16): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU (inplace)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU (inplace)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU (inplace)\n    (23): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU (inplace)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU (inplace)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU (inplace)\n    (30): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n  )\n  (classifier): Sequential (\n    (0): Dropout (p = 0.5)\n    (1): Linear (25088 -&gt; 4096)\n    (2): ReLU (inplace)\n    (3): Dropout (p = 0.5)\n    (4): Linear (4096 -&gt; 4096)\n    (5): ReLU (inplace)\n    (6): Linear (4096 -&gt; 1000)\n  )\n)\n</code></pre>\n\n<p>Now you could, as mentioned by <a href=\"https://stackoverflow.com/users/2704763/kashyap\">Kashyap</a>, use the <code>state_dict</code> method to get the weights of the different layers. But using this listing of the layers would perhaps provide more direction is creating a helper function to get that Keras like model summary! Hope this helps!</p>\n"}, {"title": "How to initialize weights in PyTorch?", "question_body": "<p>How to initialize the weights and biases (for example, with He or Xavier initialization) in a network in PyTorch? </p>\n", "link": "https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch", "question_id": 49433936, "accepted_answer_id": 49433937, "answer_body": "<h1>Single layer</h1>\n\n<p>To initialize the weights of a single layer, use a function from <a href=\"http://pytorch.org/docs/master/nn.html#torch-nn-init\" rel=\"noreferrer\"><code>torch.nn.init</code></a>. For instance:</p>\n\n<pre><code>conv1 = torch.nn.Conv2d(...)\ntorch.nn.init.xavier_uniform(conv1.weight)\n</code></pre>\n\n<p>Alternatively, you can modify the parameters by writing to <code>conv1.weight.data</code> (which is a <a href=\"http://pytorch.org/docs/master/tensors.html#torch.Tensor\" rel=\"noreferrer\"><code>torch.Tensor</code></a>). Example:</p>\n\n<pre><code>conv1.weight.data.fill_(0.01)\n</code></pre>\n\n<p>The same applies for biases: </p>\n\n<pre><code>conv1.bias.data.fill_(0.01)\n</code></pre>\n\n<h2><code>nn.Sequential</code> or custom <code>nn.Module</code></h2>\n\n<p>Pass an initialization function to <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Module.apply\" rel=\"noreferrer\"><code>torch.nn.Module.apply</code></a>. It will initialize the weights in the entire <code>nn.Module</code> recursively.</p>\n\n<blockquote>\n  <p><strong>apply(<em>fn</em>):</strong> Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also torch-nn-init).</p>\n</blockquote>\n\n<p>Example:</p>\n\n<pre><code>def init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.xavier_uniform(m.weight)\n        m.bias.data.fill_(0.01)\n\nnet = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\nnet.apply(init_weights)\n</code></pre>\n"}, {"title": "why do we &quot;pack&quot; the sequences in pytorch?", "question_body": "<p>I was trying to replicate <a href=\"https://discuss.pytorch.org/t/simple-working-example-how-to-use-packing-for-variable-length-sequence-inputs-for-rnn/2120\" rel=\"noreferrer\">How to use packing for variable-length sequence inputs for rnn</a> but I guess I first need to understand why we need to \"pack\" the sequence. </p>\n\n<p>I understand why we need to \"pad\" them but why is \"packing\" ( through <code>pack_padded_sequence</code>) necessary? </p>\n\n<p>Any high-level explanation would be appreciated!</p>\n", "link": "https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch", "question_id": 51030782, "accepted_answer_id": 51030945, "answer_body": "<p>I have stumbled upon this problem too and below is what I figured out. </p>\n\n<p>When training RNN (LSTM or GRU or vanilla-RNN), it is difficult to batch the variable length sequences. For ex: if length of sequences in a size 8 batch is [4,6,8,5,4,3,7,8], you will pad all the sequences and that will results in 8 sequences of length 8. You would end up doing 64 computation (8x8), but you needed to do only 45 computations. Moreover, if you wanted to do something fancy like using a bidirectional-RNN it would be harder to do batch computations just by padding and you might end up doing more computations than required. </p>\n\n<p>Instead, pytorch allows us to pack the sequence, internally packed sequence is a tuple of two lists. One contains the elements of sequences. Elements are interleaved by time steps (see example below) and other contains the <s>size of each sequence</s> the batch size at each step. This is helpful in recovering the actual sequences as well as telling RNN what is the batch size at each time step. This has been pointed by @Aerin.  This can be passed to RNN and it will internally optimize the computations. </p>\n\n<p>I might have been unclear at some points, so let me know and I can add more explanations. </p>\n\n<p>Here a code example:</p>\n\n<pre><code> a = [torch.tensor([1,2,3]), torch.tensor([3,4])]\n b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)\n &gt;&gt;&gt;&gt;\n tensor([[ 1,  2,  3],\n    [ 3,  4,  0]])\n torch.nn.utils.rnn.pack_padded_sequence(b, batch_first=True, lengths=[3,2])\n &gt;&gt;&gt;&gt;PackedSequence(data=tensor([ 1,  3,  2,  4,  3]), batch_sizes=tensor([ 2,  2,  1]))\n</code></pre>\n"}, {"title": "PyTorch: How to use DataLoaders for custom Datasets", "question_body": "<p>How to make use of the <code>torch.utils.data.Dataset</code> and <code>torch.utils.data.DataLoader</code> on your own data (not just the <code>torchvision.datasets</code>)?</p>\n\n<p>Is there a way to use the inbuilt <code>DataLoaders</code> which they use on <code>TorchVisionDatasets</code> to be used on any dataset?</p>\n", "link": "https://stackoverflow.com/questions/41924453/pytorch-how-to-use-dataloaders-for-custom-datasets", "question_id": 41924453, "accepted_answer_id": 42054194, "answer_body": "<p>Yes, that is possible. Just create the objects by yourself, e.g.</p>\n\n<pre><code>import torch.utils.data as data_utils\n\ntrain = data_utils.TensorDataset(features, targets)\ntrain_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)\n</code></pre>\n\n<p>where <code>features</code> and <code>targets</code> are tensors. <code>features</code> has to be 2-D, i.e. a matrix where each line represents one training sample, and <code>targets</code> may be 1-D or 2-D, depending on whether you are trying to predict a scalar or a vector.</p>\n\n<p>Hope that helps!</p>\n\n<hr>\n\n<p><strong>EDIT</strong>: response to @sarthak's question</p>\n\n<p>Basically yes. If you create an object of type <code>TensorData</code>, then the constructor investigates whether the first dimensions of the feature tensor (which is actually called <code>data_tensor</code>) and the target tensor (called <code>target_tensor</code>) have the same length:</p>\n\n<pre><code>assert data_tensor.size(0) == target_tensor.size(0)\n</code></pre>\n\n<p>However, if you want to feed these data into a neural network subsequently, then you need to be careful. While convolution layers work on data like yours, (I think) all of the other types of layers expect the data to be given in matrix form. So, if you run into an issue like this, then an easy solution would be to convert your 4D-dataset (given as some kind of tensor, e.g. <code>FloatTensor</code>) into a matrix by using the method <code>view</code>. For your 5000xnxnx3 dataset, this would look like this:</p>\n\n<pre><code>2d_dataset = 4d_dataset.view(5000, -1)\n</code></pre>\n\n<p>(The value <code>-1</code> tells PyTorch to figure out the length of the second dimension automatically.)</p>\n"}, {"title": "PyTorch - contiguous()", "question_body": "<p>I was going through this example of a LSTM language model on github <a href=\"https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model/main.py\" rel=\"noreferrer\">(link)</a>.\nWhat it does in general is pretty clear to me. But I'm still struggling to understand what calling <code>contiguous()</code> does, which occurs several times in the code.</p>\n\n<p>For example in line 74/75 of the code input and target sequences of the LSTM are created.\nData (stored in <code>ids</code>) is 2-dimensional where first dimension is the batch size.</p>\n\n\n\n<pre class=\"lang-python prettyprint-override\"><code>for i in range(0, ids.size(1) - seq_length, seq_length):\n    # Get batch inputs and targets\n    inputs = Variable(ids[:, i:i+seq_length])\n    targets = Variable(ids[:, (i+1):(i+1)+seq_length].contiguous())\n</code></pre>\n\n<p>So as a simple example, when using batch size 1 and <code>seq_length</code> 10 <code>inputs</code> and <code>targets</code> looks like this:</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>inputs Variable containing:\n0     1     2     3     4     5     6     7     8     9\n[torch.LongTensor of size 1x10]\n\ntargets Variable containing:\n1     2     3     4     5     6     7     8     9    10\n[torch.LongTensor of size 1x10]\n</code></pre>\n\n<p>So in general my question is, what does <code>contiguous()</code> and why do I need it?</p>\n\n<p>Further I don't understand why the method is called for the target sequence and but not the input sequence as both variables are comprised of the same data.</p>\n\n<p>How could <code>targets</code> be uncontiguous and <code>inputs</code> still be contiguous?</p>\n\n<p><strong>EDIT:</strong>\n<em>I tried to leave out calling <code>contiguous()</code>, but this leads to an  error message when computing the loss.</em></p>\n\n<pre class=\"lang-python prettyprint-override\"><code>RuntimeError: invalid argument 1: input is not contiguous at .../src/torch/lib/TH/generic/THTensor.c:231\n</code></pre>\n\n<p><em>So obviously calling <code>contiguous()</code> in this example is necessary.</em> </p>\n\n<p>(For keeping this readable I avoided posting the full code here, it can be found by using the GitHub link above.)</p>\n\n<p>Thanks in advance!</p>\n", "link": "https://stackoverflow.com/questions/48915810/pytorch-contiguous", "question_id": 48915810, "accepted_answer_id": 52229694, "answer_body": "<p>There are few operations on Tensor in PyTorch that do not really change the content of the tensor, but only how to convert indices in to tensor to byte location. These operations include:</p>\n\n<blockquote>\n  <p><code>narrow()</code>, <code>view()</code>, <code>expand()</code> and <code>transpose()</code></p>\n</blockquote>\n\n<p><em>For example:</em> when you call <code>transpose()</code>, PyTorch doesn't generate new tensor with new layout, it just modifies meta information in Tensor object so offset and stride are for new shape. The transposed tensor and original tensor are indeed sharing the memory!</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>x = torch.randn(3,2)\ny = torch.transpose(x, 0, 1)\nx[0, 0] = 42\nprint(y[0,0])\n# prints 42\n</code></pre>\n\n<p>This is where the concept of <em>contiguous</em> comes in. Above <code>x</code> is contiguous but <code>y</code> is not because its memory layout is different than a tensor of same shape made from scratch. Note that the word <em>\"contiguous\"</em> is bit misleading because its not that the content of tensor is spread out around disconnected blocks of memory. Here bytes are still allocated in one block of memory but the order of the elements is different!</p>\n\n<p>When you call <code>contiguous()</code>, it actually makes a copy of tensor so the order of elements would be same as if tensor of same shape created from scratch.</p>\n\n<p>Normally you don't need to worry about this. If PyTorch expects contiguous tensor but if its not then you will get <code>RuntimeError: input is not contiguous</code> and then you just add a call to <code>contiguous()</code>.</p>\n"}, {"title": "What&#39;s the difference between &quot;hidden&quot; and &quot;output&quot; in PyTorch LSTM?", "question_body": "<p>I'm having trouble understanding the documentation for PyTorch's LSTM module (and also RNN and GRU, which are similar). Regarding the outputs, it says:</p>\n\n<blockquote>\n  <p>Outputs: output, (h_n, c_n)</p>\n  \n  <ul>\n  <li>output (seq_len, batch, hidden_size * num_directions): tensor containing the output features (h_t) from the last layer of the RNN, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.</li>\n  <li>h_n (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t=seq_len</li>\n  <li>c_n (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t=seq_len</li>\n  </ul>\n</blockquote>\n\n<p>It seems that the variables <code>output</code> and <code>h_n</code> both give the values of the hidden state. Does <code>h_n</code> just redundantly provide the last time step that's already included in <code>output</code>, or is there something more to it than that?</p>\n", "link": "https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm", "question_id": 48302810, "accepted_answer_id": 48305882, "answer_body": "<p>I made a diagram. The names follow the <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.LSTM\" rel=\"noreferrer\">PyTorch docs</a>, although I renamed <code>num_layers</code> to <code>w</code>.</p>\n\n<p><code>output</code> comprises all the hidden states in the last layer (\"last\" depth-wise, not time-wise). <code>(h_n, c_n)</code> comprises the hidden states after the last timestep, <em>t</em> = <em>n</em>, so you could potentially feed them into another LSTM.</p>\n\n<p><a href=\"https://i.stack.imgur.com/SjnTl.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/SjnTl.png\" alt=\"LSTM diagram\"></a></p>\n\n<p>The batch dimension is not included.</p>\n"}, {"title": "Why do we need to explicitly call zero_grad()?", "question_body": "<p>Why do we need to explicitly zero the gradients in PyTorch? Why can't gradients be zeroed when <code>loss.backward()</code> is called? What scenario is served by keeping the gradients on the graph and asking the user to explicitly zero the gradients?</p>\n", "link": "https://stackoverflow.com/questions/44732217/why-do-we-need-to-explicitly-call-zero-grad", "question_id": 44732217, "accepted_answer_id": 44732271, "answer_body": "<p>We explicitly need to call <code>zero_grad()</code> because, after <code>loss.backward()</code> (when gradients are computed), we need to use <code>optimizer.step()</code> to proceed gradient descent. More specifically, the gradients are not automatically zeroed because these two operations, <code>loss.backward()</code> and <code>optimizer.step()</code>, are separated, and <code>optimizer.step()</code> requires the just computed gradients.</p>\n\n<p>In addition, sometimes, we need to accumulate gradient among some batches; to do that, we can simply call <code>backward</code> multiple times and optimize once.</p>\n"}, {"title": "What&#39;s the difference between reshape and view in pytorch?", "question_body": "<p>In numpy, we use <code>ndarray.reshape()</code> for reshaping an array.</p>\n\n<p>I noticed that in pytorch, people use <code>torch.view(...)</code> for the same purpose, but at the same time, there is also a <code>torch.reshape(...)</code> existing.</p>\n\n<p>So I am wondering what the differences are between them and when I should use either of them?</p>\n", "link": "https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch", "question_id": 49643225, "accepted_answer_id": 49644300, "answer_body": "<p><code>torch.view</code> has existed for a long time. It will return a tensor with the new shape. The returned tensor will share the underling data with the original tensor. \nSee the <a href=\"http://pytorch.org/docs/master/tensors.html?highlight=view#torch.Tensor.view\" rel=\"noreferrer\">documentation here</a>.</p>\n\n<p>On the other hand, it seems that <code>torch.reshape</code> <a href=\"https://github.com/pytorch/pytorch/pull/5575\" rel=\"noreferrer\">has been introduced recently in version 0.4</a>. According to the <a href=\"http://pytorch.org/docs/master/torch.html#torch.reshape\" rel=\"noreferrer\">document</a>, this method will</p>\n\n<blockquote>\n  <p>Returns a tensor with the same data and number of elements as input, but with the specified shape. When possible, the returned tensor will be a view of input. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.</p>\n</blockquote>\n\n<p>It means that <code>torch.reshape</code> may return a copy or a view of the original tensor. You can not count on that to return a view or a copy. According to the developer:</p>\n\n<blockquote>\n  <p>if you need a copy use clone() if you need the same storage use view(). The semantics of reshape() are that it may or may not share the storage and you don't know beforehand.</p>\n</blockquote>\n\n<p>Another difference is that <code>reshape()</code> can operate on both contiguous and non-contiguous tensor while <code>view()</code> can only operate on contiguous tensor. Also see <a href=\"https://stackoverflow.com/a/26999092/6064933\">here</a> about the meaning of <code>contiguous</code>.</p>\n"}, {"title": "How to do product of matrices in PyTorch", "question_body": "<p>In numpy I can do a simple matrix multiplication like this:</p>\n\n<pre><code>a = numpy.arange(2*3).reshape(3,2)\nb = numpy.arange(2).reshape(2,1)\nprint(a)\nprint(b)\nprint(a.dot(b))\n</code></pre>\n\n<p>However, when I am trying this with PyTorch Tensors, this does not work:</p>\n\n<pre><code>a = torch.Tensor([[1, 2, 3], [1, 2, 3]]).view(-1, 2)\nb = torch.Tensor([[2, 1]]).view(2, -1)\nprint(a)\nprint(a.size())\n\nprint(b)\nprint(b.size())\n\nprint(torch.dot(a, b))\n</code></pre>\n\n<p>This code throws the following error: </p>\n\n<blockquote>\n  <p>RuntimeError: inconsistent tensor size at\n  /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:503</p>\n</blockquote>\n\n<p>Any ideas how matrix multiplication can be conducted in PyTorch?</p>\n", "link": "https://stackoverflow.com/questions/44524901/how-to-do-product-of-matrices-in-pytorch", "question_id": 44524901, "accepted_answer_id": 44525687, "answer_body": "<p>You're looking for </p>\n\n<pre><code>torch.mm(a,b)\n</code></pre>\n\n<p>Note that <code>torch.dot()</code> behaves differently to <code>np.dot()</code>. There's been some discussion about what would be desirable <a href=\"https://github.com/pytorch/pytorch/issues/138\" rel=\"noreferrer\">here</a>. Specifically, <code>torch.dot()</code> treats both <code>a</code> and <code>b</code> as 1D vectors (irrespective of their original shape) and computes their inner product. The error is thrown, because this behaviour makes your <code>a</code> a vector of length 6 and your <code>b</code> a vector of length 2; hence their inner product can't be computed. For matrix multiplication in PyTorch, use <code>torch.mm()</code>. Numpy's <code>np.dot()</code> in contrast is more flexible; it computes the inner product for 1D arrays and performs matrix multiplication for 2D arrays. </p>\n"}, {"title": "How to get mini-batches in pytorch in a clean and efficient way?", "question_body": "<p>I was trying to do a simple thing which was train a linear model with Stochastic Gradient Descent (SGD) using torch:</p>\n\n<pre><code>import numpy as np\n\nimport torch\nfrom torch.autograd import Variable\n\nimport pdb\n\ndef get_batch2(X,Y,M,dtype):\n    X,Y = X.data.numpy(), Y.data.numpy()\n    N = len(Y)\n    valid_indices = np.array( range(N) )\n    batch_indices = np.random.choice(valid_indices,size=M,replace=False)\n    batch_xs = torch.FloatTensor(X[batch_indices,:]).type(dtype)\n    batch_ys = torch.FloatTensor(Y[batch_indices]).type(dtype)\n    return Variable(batch_xs, requires_grad=False), Variable(batch_ys, requires_grad=False)\n\ndef poly_kernel_matrix( x,D ):\n    N = len(x)\n    Kern = np.zeros( (N,D+1) )\n    for n in range(N):\n        for d in range(D+1):\n            Kern[n,d] = x[n]**d;\n    return Kern\n\n## data params\nN=5 # data set size\nDegree=4 # number dimensions/features\nD_sgd = Degree+1\n##\nx_true = np.linspace(0,1,N) # the real data points\ny = np.sin(2*np.pi*x_true)\ny.shape = (N,1)\n## TORCH\ndtype = torch.FloatTensor\n# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\nX_mdl = poly_kernel_matrix( x_true,Degree )\nX_mdl = Variable(torch.FloatTensor(X_mdl).type(dtype), requires_grad=False)\ny = Variable(torch.FloatTensor(y).type(dtype), requires_grad=False)\n## SGD mdl\nw_init = torch.zeros(D_sgd,1).type(dtype)\nW = Variable(w_init, requires_grad=True)\nM = 5 # mini-batch size\neta = 0.1 # step size\nfor i in range(500):\n    batch_xs, batch_ys = get_batch2(X_mdl,y,M,dtype)\n    # Forward pass: compute predicted y using operations on Variables\n    y_pred = batch_xs.mm(W)\n    # Compute and print loss using operations on Variables. Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape (1,); loss.data[0] is a scalar value holding the loss.\n    loss = (1/N)*(y_pred - batch_ys).pow(2).sum()\n    # Use autograd to compute the backward pass. Now w will have gradients\n    loss.backward()\n    # Update weights using gradient descent; w1.data are Tensors,\n    # w.grad are Variables and w.grad.data are Tensors.\n    W.data -= eta * W.grad.data\n    # Manually zero the gradients after updating weights\n    W.grad.data.zero_()\n\n#\nc_sgd = W.data.numpy()\nX_mdl = X_mdl.data.numpy()\ny = y.data.numpy()\n#\nXc_pinv = np.dot(X_mdl,c_sgd)\nprint('J(c_sgd) = ', (1/N)*(np.linalg.norm(y-Xc_pinv)**2) )\nprint('loss = ',loss.data[0])\n</code></pre>\n\n<p>the code runs fine and all though my <code>get_batch2</code> method seems really dum/naive, its probably because I am new to pytorch but I have not found a good place where they discuss how to retrieve data batches. I went through their tutorials (<a href=\"http://pytorch.org/tutorials/beginner/pytorch_with_examples.html\" rel=\"noreferrer\">http://pytorch.org/tutorials/beginner/pytorch_with_examples.html</a>) and through the data set (<a href=\"http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\" rel=\"noreferrer\">http://pytorch.org/tutorials/beginner/data_loading_tutorial.html</a>) with no luck. The tutorials all seem to assume that one already has the batch and batch-size at the beginning and then proceeds to train with that data without changing it (specifically look at <a href=\"http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-variables-and-autograd\" rel=\"noreferrer\">http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-variables-and-autograd</a>).</p>\n\n<p>So my question is do I really need to turn my data back into numpy so that I can fetch some random sample of it and then turn it back to pytorch with Variable to be able to train in memory? Is there no way to get mini-batches with torch?</p>\n\n<p>I looked at a few functions torch provides but with no luck:</p>\n\n<pre><code>#pdb.set_trace()\n#valid_indices = torch.arange(0,N).numpy()\n#valid_indices = np.array( range(N) )\n#batch_indices = np.random.choice(valid_indices,size=M,replace=False)\n#indices = torch.LongTensor(batch_indices)\n#batch_xs, batch_ys = torch.index_select(X_mdl, 0, indices), torch.index_select(y, 0, indices)\n#batch_xs,batch_ys = torch.index_select(X_mdl, 0, indices), torch.index_select(y, 0, indices)\n</code></pre>\n\n<p>even though the code I provided works fine I am worried that its not an efficient implementation AND that if I were to use GPUs that there would be a considerable further slow down (because my guess it putting things in memory and then fetching them back to put them GPU like that is silly).</p>\n\n<hr>\n\n<p>I implemented a new one based on the answer that suggested to use <code>torch.index_select()</code>:</p>\n\n<pre><code>def get_batch2(X,Y,M):\n    '''\n    get batch for pytorch model\n    '''\n    # TODO fix and make it nicer, there is pytorch forum question\n    #X,Y = X.data.numpy(), Y.data.numpy()\n    X,Y = X, Y\n    N = X.size()[0]\n    batch_indices = torch.LongTensor( np.random.randint(0,N+1,size=M) )\n    pdb.set_trace()\n    batch_xs = torch.index_select(X,0,batch_indices)\n    batch_ys = torch.index_select(Y,0,batch_indices)\n    return Variable(batch_xs, requires_grad=False), Variable(batch_ys, requires_grad=False)\n</code></pre>\n\n<p>however, this seems to have issues because it does not work if <code>X,Y</code> are NOT variables...which is really odd. I added this to the pytorch forum: <a href=\"https://discuss.pytorch.org/t/how-to-get-mini-batches-in-pytorch-in-a-clean-and-efficient-way/10322\" rel=\"noreferrer\">https://discuss.pytorch.org/t/how-to-get-mini-batches-in-pytorch-in-a-clean-and-efficient-way/10322</a></p>\n\n<p>Right now what I am struggling with is making this work for gpu. My most current version:</p>\n\n<pre><code>def get_batch2(X,Y,M,dtype):\n    '''\n    get batch for pytorch model\n    '''\n    # TODO fix and make it nicer, there is pytorch forum question\n    #X,Y = X.data.numpy(), Y.data.numpy()\n    X,Y = X, Y\n    N = X.size()[0]\n    if dtype ==  torch.cuda.FloatTensor:\n        batch_indices = torch.cuda.LongTensor( np.random.randint(0,N,size=M) )# without replacement\n    else:\n        batch_indices = torch.LongTensor( np.random.randint(0,N,size=M) ).type(dtype)  # without replacement\n    pdb.set_trace()\n    batch_xs = torch.index_select(X,0,batch_indices)\n    batch_ys = torch.index_select(Y,0,batch_indices)\n    return Variable(batch_xs, requires_grad=False), Variable(batch_ys, requires_grad=False)\n</code></pre>\n\n<p>the error:</p>\n\n<pre><code>RuntimeError: tried to construct a tensor from a int sequence, but found an item of type numpy.int64 at index (0)\n</code></pre>\n\n<p>I don't get it, do I really have to do:</p>\n\n<pre><code>ints = [ random.randint(0,N) for i i range(M)]\n</code></pre>\n\n<p>to get the integers?</p>\n\n<p>It would also be ideal if the data could be a variable. It seems that it <code>torch.index_select</code> does not work for <code>Variable</code> type data.</p>\n\n<p>this list of integers thing still doesn't work:</p>\n\n<pre><code>TypeError: torch.addmm received an invalid combination of arguments - got (int, torch.cuda.FloatTensor, int, torch.cuda.FloatTensor, torch.FloatTensor, out=torch.cuda.FloatTensor), but expected one of:\n * (torch.cuda.FloatTensor source, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (torch.cuda.FloatTensor source, float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n * (float beta, torch.cuda.FloatTensor source, float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n      didn't match because some of the arguments have invalid types: (int, torch.cuda.FloatTensor, int, torch.cuda.FloatTensor, torch.FloatTensor, out=torch.cuda.FloatTensor)\n * (float beta, torch.cuda.FloatTensor source, float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)\n      didn't match because some of the arguments have invalid types: (int, torch.cuda.FloatTensor, int, torch.cuda.FloatTensor, torch.FloatTensor, out=torch.cuda.FloatTensor)\n</code></pre>\n", "link": "https://stackoverflow.com/questions/45113245/how-to-get-mini-batches-in-pytorch-in-a-clean-and-efficient-way", "question_id": 45113245, "accepted_answer_id": null}, {"title": "Check the total number of parameters in a PyTorch model", "question_body": "<p>How to count the total number of parameters in a PyTorch model? Something similar to <code>model.count_params()</code> in Keras.</p>\n", "link": "https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model", "question_id": 49201236, "accepted_answer_id": 49201237, "answer_body": "<p>PyTorch doesn't have a function to calculate the total number of parameters as Keras does, but it's possible to sum the number of elements for every parameter group:</p>\n\n<pre><code>pytorch_total_params = sum(p.numel() for p in model.parameters())\n</code></pre>\n\n<p>If you want to calculate only the <em>trainable</em> parameters:</p>\n\n<pre><code>pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n</code></pre>\n\n<p><em>Answer inspired by this <a href=\"https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/9\" rel=\"noreferrer\">answer</a> on PyTorch Forums</em>.</p>\n\n<p><em>Note: I'm <a href=\"https://stackoverflow.blog/2011/07/01/its-ok-to-ask-and-answer-your-own-questions/\">answering my own question</a>. If anyone has a better solution, please share with us.</em></p>\n"}, {"title": "Why do we need to call zero_grad() in PyTorch?", "question_body": "<p>The method <code>zero_grad()</code> needs to be called during training. But the <a href=\"https://discuss.pytorch.org/t/minimal-working-example-of-optim-sgd/11623\" rel=\"noreferrer\">documentation</a> is not very helpful</p>\n\n<pre><code>|  zero_grad(self)\n|      Sets gradients of all model parameters to zero.\n</code></pre>\n\n<p>Why do we need to call this method?</p>\n", "link": "https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch", "question_id": 48001598, "accepted_answer_id": 48009142, "answer_body": "<p>In <a href=\"https://pytorch.org/\" rel=\"nofollow noreferrer\"><code>PyTorch</code></a>, we need to set the gradients to zero before starting to do backpropragation because PyTorch <em>accumulates the gradients</em> on subsequent backward passes. This is convenient while training RNNs. So, the default action is to <a href=\"https://pytorch.org/docs/stable/_modules/torch/autograd.html\" rel=\"nofollow noreferrer\">accumulate (i.e. sum) the gradients</a> on every <code>loss.backward()</code> call.</p>\n\n<p>Because of this, when you start your training loop, ideally you should <a href=\"https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer.zero_grad\" rel=\"nofollow noreferrer\"><code>zero out the gradients</code></a> so that you do the parameter update correctly. Else the gradient would point in some other direction than the intended direction towards the <em>minimum</em> (or <em>maximum</em>, in case of maximization objectives).</p>\n\n<p>Here is a simple example:</p>\n\n<pre><code>import torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\n\ndef linear_model(x, W, b):\n    return torch.matmul(x, W) + b\n\ndata, targets = ...\n\nW = Variable(torch.randn(4, 3), requires_grad=True)\nb = Variable(torch.randn(3), requires_grad=True)\n\noptimizer = optim.Adam([W, b])\n\nfor sample, target in zip(data, targets):\n    # clear out the gradients of all Variables \n    # in this optimizer (i.e. W, b)\n    optimizer.zero_grad()\n    output = linear_model(sample, W, b)\n    loss = (output - target) ** 2\n    loss.backward()\n    optimizer.step()\n</code></pre>\n\n<hr>\n\n<p>Alternatively, if you're doing a <em>vanilla gradient descent</em>, then:</p>\n\n<pre><code>W = Variable(torch.randn(4, 3), requires_grad=True)\nb = Variable(torch.randn(3), requires_grad=True)\n\nfor sample, target in zip(data, targets):\n    # clear out the gradients of Variables \n    # (i.e. W, b)\n    W.grad.data.zero_()\n    b.grad.data.zero_()\n\n    output = linear_model(sample, W, b)\n    loss = (output - target) ** 2\n    loss.backward()\n\n    W -= learning_rate * W.grad.data\n    b -= learning_rate * b.grad.data\n</code></pre>\n\n<hr>\n\n<p><strong>Note</strong>: The <em>accumulation</em> (i.e. <em>sum</em>) of gradients happen when <a href=\"https://pytorch.org/docs/stable/_modules/torch/autograd.html\" rel=\"nofollow noreferrer\"><code>.backward()</code> is called on the <code>loss</code> tensor</a>.</p>\n"}, {"title": "Pytorch reshape tensor dimension", "question_body": "<p>For example, I have 1D vector with dimension (5). I would like to reshape it into 2D matrix (1,5).</p>\n\n<p>Here is how I do it with numpy</p>\n\n<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; a = np.array([1,2,3,4,5])\n&gt;&gt;&gt; a.shape\n(5,)\n&gt;&gt;&gt; a = np.reshape(a, (1,5))\n&gt;&gt;&gt; a.shape\n(1, 5)\n&gt;&gt;&gt; a\narray([[1, 2, 3, 4, 5]])\n&gt;&gt;&gt; \n</code></pre>\n\n<p>But how can I do that with Pytorch Tensor (and Variable). I don't want to switch back to numpy and switch to Torch variable again, because it will loss backpropagation information.</p>\n\n<p>Here is what I have in Pytorch</p>\n\n<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch.autograd import Variable\n&gt;&gt;&gt; a = torch.Tensor([1,2,3,4,5])\n&gt;&gt;&gt; a\n\n 1\n 2\n 3\n 4\n 5\n[torch.FloatTensor of size 5]\n\n&gt;&gt;&gt; a.size()\n(5L,)\n&gt;&gt;&gt; a_var = variable(a)\n&gt;&gt;&gt; a_var = Variable(a)\n&gt;&gt;&gt; a_var.size()\n(5L,)\n.....do some calculation in forward function\n&gt;&gt;&gt; a_var.size()\n(5L,)\n</code></pre>\n\n<p>Now I want it size to be (1, 5).\nHow can I resize or reshape the dimension of pytorch tensor in Variable without loss grad information. (because I will feed into another model before backward) </p>\n", "link": "https://stackoverflow.com/questions/43328632/pytorch-reshape-tensor-dimension", "question_id": 43328632, "accepted_answer_id": 43451081, "answer_body": "<p>Use  <a href=\"http://pytorch.org/docs/torch.html?highlight=unsqueeze#torch.unsqueeze\" rel=\"noreferrer\">torch.unsqueeze(input, dim, out=None)</a></p>\n\n<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; a = torch.Tensor([1,2,3,4,5])\n&gt;&gt;&gt; a\n\n 1\n 2\n 3\n 4\n 5\n[torch.FloatTensor of size 5]\n\n&gt;&gt;&gt; a = a.unsqueeze(0)\n&gt;&gt;&gt; a\n\n 1  2  3  4  5\n[torch.FloatTensor of size 1x5]\n</code></pre>\n"}, {"title": "Understanding a simple LSTM pytorch", "question_body": "<pre><code>import torch,ipdb\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nrnn = nn.LSTM(input_size=10, hidden_size=20, num_layers=2)\ninput = Variable(torch.randn(5, 3, 10))\nh0 = Variable(torch.randn(2, 3, 20))\nc0 = Variable(torch.randn(2, 3, 20))\noutput, hn = rnn(input, (h0, c0))\n</code></pre>\n\n<p>This is the LSTM example from the docs. I don't know understand the following things:</p>\n\n<ol>\n<li>What is output-size and why is it not specified anywhere?</li>\n<li>Why does the input have 3 dimensions. What does 5 and 3 represent?</li>\n<li>What are 2 and 3 in h0 and c0, what do those represent?</li>\n</ol>\n\n<p>Edit:</p>\n\n<pre><code>import torch,ipdb\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nnum_layers=3\nnum_hyperparams=4\nbatch = 1\nhidden_size = 20\nrnn = nn.LSTM(input_size=num_hyperparams, hidden_size=hidden_size, num_layers=num_layers)\n\ninput = Variable(torch.randn(1, batch, num_hyperparams)) # (seq_len, batch, input_size)\nh0 = Variable(torch.randn(num_layers, batch, hidden_size)) # (num_layers, batch, hidden_size)\nc0 = Variable(torch.randn(num_layers, batch, hidden_size))\noutput, hn = rnn(input, (h0, c0))\naffine1 = nn.Linear(hidden_size, num_hyperparams)\n\nipdb.set_trace()\nprint output.size()\nprint h0.size()\n</code></pre>\n\n<blockquote>\n  <p>*** RuntimeError: matrices expected, got 3D, 2D tensors at</p>\n</blockquote>\n", "link": "https://stackoverflow.com/questions/45022734/understanding-a-simple-lstm-pytorch", "question_id": 45022734, "accepted_answer_id": 45023288, "answer_body": "<p>The output for the LSTM is the output for all the hidden nodes on the final layer.<br>\n<code>hidden_size</code> - the number of LSTM blocks per layer.<br>\n<code>input_size</code> - the number of input features per time-step.<br>\n<code>num_layers</code> - the number of hidden layers.<br>\nIn total there are <code>hidden_size * num_layers</code> LSTM blocks.</p>\n\n<p>The input dimensions are <code>(seq_len, batch, input_size)</code>.<br>\n<code>seq_len</code> - the number of time steps in each input stream.<br>\n<code>batch</code> - the size of each batch of input sequences.</p>\n\n<p>The hidden and cell dimensions are: <code>(num_layers, batch, hidden_size)</code></p>\n\n<blockquote>\n  <p><strong>output</strong> (seq_len, batch, hidden_size * num_directions): tensor containing the output features (h_t) from the last layer of the RNN, for each t.</p>\n</blockquote>\n\n<p>So there will be <code>hidden_size * num_directions</code> outputs. You didn't initialise the RNN to be bidirectional so <code>num_directions</code> is 1. So <code>output_size = hidden_size</code>.</p>\n\n<p><strong>Edit</strong>: You can change the number of outputs by using a linear layer:</p>\n\n<pre><code>out_rnn, hn = rnn(input, (h0, c0))\nlin = nn.Linear(hidden_size, output_size)\nv1 = nn.View(seq_len*batch, hidden_size)\nv2 = nn.View(seq_len, batch, output_size)\noutput = v2(lin(v1(out_rnn)))\n</code></pre>\n\n<p><strong>Note</strong>: for this answer I assumed that we're only talking about non-bidirectional LSTMs.</p>\n\n<p>Source: <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.LSTM\" rel=\"noreferrer\">PyTorch docs</a>.</p>\n"}, {"title": "Data Augmentation in PyTorch", "question_body": "<p>I am a little bit confused about the data augmentation performed in PyTorch. Now, as far as I know, when we are performing data augmentation, we are KEEPING our original dataset, and then adding other versions of it (Flipping, Cropping...etc). But that doesn't seem like happening in PyTorch. As far as I understood from the references, when we use <code>data.transforms</code> in PyTorch, then it applies them one by one. So for example:</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>data_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n</code></pre>\n\n<p>Here , for the training, we are first randomly cropping the image and resizing it to shape <code>(224,224)</code>. Then we are taking these <code>(224,224)</code> images and horizontally flipping them. Therefore, our dataset is now containing ONLY the horizontally flipped images, so our original images are lost in this case. </p>\n\n<p>Am I right? Is this understanding correct? If not, then where do we tell PyTorch in this code above (taken from Official Documentation) to keep the original images and resize them to the expected shape <code>(224,224)</code>?</p>\n\n<p>Thanks </p>\n", "link": "https://stackoverflow.com/questions/51677788/data-augmentation-in-pytorch", "question_id": 51677788, "accepted_answer_id": 51678124, "answer_body": "<p>The <code>transforms</code> operations are applied to your original images at every batch generation. So your dataset is left unchanged, only the batch images are copied and transformed every iteration.</p>\n\n<p>The confusion may come from the fact that often, like in your example, <code>transforms</code> are used both for data preparation (resizing/cropping to expected dimensions, normalizing values, etc.) and for data augmentation (randomizing the resizing/cropping, randomly flipping the images, etc.).</p>\n\n<hr>\n\n<p>What your <code>data_transforms['train']</code> does is:</p>\n\n<ul>\n<li>Randomly resize the provided image and randomly crop it to obtain a <code>(224, 224)</code> patch</li>\n<li>Apply or not a random horizontal flip to this patch, with a 50/50 chance</li>\n<li>Convert it to a <code>Tensor</code></li>\n<li>Normalize the resulting <code>Tensor</code>, given the mean and deviation values you provided</li>\n</ul>\n\n<p>What your <code>data_transforms['val']</code> does is:</p>\n\n<ul>\n<li>Resize your image to <code>(256, 256)</code></li>\n<li>Center crop the resized image to obtain a <code>(224, 224)</code> patch</li>\n<li>Convert it to a <code>Tensor</code></li>\n<li>Normalize the resulting <code>Tensor</code>, given the mean and deviation values you provided</li>\n</ul>\n\n<p>(i.e. the random resizing/cropping for the training data is replaced by a fixed operation for the validation one, to have reliable validation results)</p>\n\n<hr>\n\n<p>If you don't want your training images to be horizontally flipped with a 50/50 chance, just remove the <code>transforms.RandomHorizontalFlip()</code> line.</p>\n\n<p>Similarly, if you want your images to always be center-cropped, replace <code>transforms.RandomResizedCrop</code> by <code>transforms.Resize</code> and <code>transforms.CenterCrop</code>, as done for <code>data_transforms['val']</code>.</p>\n"}, {"title": "How to check if two Torch tensors or matrices are equal?", "question_body": "<p>I need a Torch command that checks if two tensors have the same content, and returns TRUE if they have the same content.</p>\n\n<p>For example:</p>\n\n<pre><code>local tens_a = torch.Tensor({9,8,7,6});\nlocal tens_b = torch.Tensor({9,8,7,6});\n\nif (tens_a EQUIVALENCE_COMMAND tens_b) then ... end\n</code></pre>\n\n<p>What should I use in this script instead of <code>EQUIVALENCE_COMMAND</code> ?</p>\n\n<p>I tried simply with <code>==</code> but it does not work. </p>\n", "link": "https://stackoverflow.com/questions/32996281/how-to-check-if-two-torch-tensors-or-matrices-are-equal", "question_id": 32996281, "accepted_answer_id": 33008837, "answer_body": "<p><a href=\"https://github.com/torch/torch7/blob/master/doc/maths.md#torcheqa-b\">https://github.com/torch/torch7/blob/master/doc/maths.md#torcheqa-b</a></p>\n\n<pre><code>torch.eq(a, b)\n</code></pre>\n\n<p>Implements == operator comparing each element in a with b (if b is a number) or each element in a with corresponding element in b.</p>\n\n<p>--UPDATE</p>\n\n<p>from @deltheil</p>\n\n<pre><code>torch.all(torch.eq(tens_a, tens_b))\n</code></pre>\n\n<p>or even simpler</p>\n\n<pre><code>torch.all(tens_a:eq(tens_b))\n</code></pre>\n"}, {"title": "What is the relationship between PyTorch and Torch?", "question_body": "<p>There are two PyTorch repositories :</p>\n\n<ul>\n<li><a href=\"https://github.com/hughperkins/pytorch\" rel=\"noreferrer\">https://github.com/hughperkins/pytorch</a></li>\n<li><a href=\"https://github.com/pytorch/pytorch\" rel=\"noreferrer\">https://github.com/pytorch/pytorch</a></li>\n</ul>\n\n<p>The first clearly requires Torch and lua and is a wrapper, but the second doesn't make any reference to the Torch project except with its name.</p>\n\n<p>How is it related to the Lua Torch (<a href=\"http://torch.ch/\" rel=\"noreferrer\">http://torch.ch/</a>)?</p>\n", "link": "https://stackoverflow.com/questions/44371560/what-is-the-relationship-between-pytorch-and-torch", "question_id": 44371560, "accepted_answer_id": 44382388, "answer_body": "<p>Here a short comparison on pytorch and torch.</p>\n\n<ul>\n<li><p><strong>Torch:</strong>  </p>\n\n<blockquote>\n  <p>A Tensor library like Numpy, unlike Numpy it has strong GPU support.\n  Lua is a wrapper for Torch (Yes! you need to have a good understanding of Lua), and for that you will need LuaRocks package manager.</p>\n</blockquote></li>\n<li><p><strong>PyTorch:</strong></p>\n\n<blockquote>\n  <p>No need of the LuaRocks package manager, no need to write code in Lua. And because we are using Python, we can develop Deep Learning models with utmost flexibility. We can also exploit major Python packages likes scipy, numpy, matplotlib and Cython with pytorch's own autograd.</p>\n</blockquote></li>\n</ul>\n\n<p>There is a detailed discussion on this on <a href=\"https://discuss.pytorch.org/t/roadmap-for-torch-and-pytorch/38\" rel=\"noreferrer\">pytorch forum</a>. Adding to that both PyTorch and Torch use <a href=\"https://github.com/torch/nn/tree/master/lib/THNN\" rel=\"noreferrer\">THNN</a>. Torch provides lua wrappers to the THNN library while Pytorch provides Python wrappers for the same.</p>\n\n<p>PyTorch's recurrent nets, weight sharing and memory usage with the flexibility of interfacing with C, and the current speed of Torch.</p>\n\n<p>For more insights, have a look at this discussion session <a href=\"https://discuss.pytorch.org/t/torch-autograd-vs-pytorch-autograd/1671\" rel=\"noreferrer\">here</a>.</p>\n"}, {"title": "How do I split a custom dataset into training and test datasets?", "question_body": "<pre><code>import pandas as pd\nimport numpy as np\nimport cv2\nfrom torch.utils.data.dataset import Dataset\n\nclass CustomDatasetFromCSV(Dataset):\n    def __init__(self, csv_path, transform=None):\n        self.data = pd.read_csv(csv_path)\n        self.labels = pd.get_dummies(self.data['emotion']).as_matrix()\n        self.height = 48\n        self.width = 48\n        self.transform = transform\n\n    def __getitem__(self, index):\n        pixels = self.data['pixels'].tolist()\n        faces = []\n        for pixel_sequence in pixels:\n            face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n            # print(np.asarray(face).shape)\n            face = np.asarray(face).reshape(self.width, self.height)\n            face = cv2.resize(face.astype('uint8'), (self.width, self.height))\n            faces.append(face.astype('float32'))\n        faces = np.asarray(faces)\n        faces = np.expand_dims(faces, -1)\n        return faces, self.labels\n\n    def __len__(self):\n        return len(self.data)\n</code></pre>\n\n<p>This is what I could manage to do by using references from other repositories. \nHowever, I want to split this dataset into train and test. </p>\n\n<p>How can I do that inside this class? Or do I need to make a separate class to do that?</p>\n", "link": "https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets", "question_id": 50544730, "accepted_answer_id": 50544887, "answer_body": "<p>Using Pytorch's <a href=\"https://pytorch.org/docs/master/data.html#torch.utils.data.SubsetRandomSampler\" rel=\"noreferrer\"><code>SubsetRandomSampler</code></a>:</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>import torch\nimport numpy as np\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\nclass CustomDatasetFromCSV(Dataset):\n    def __init__(self, csv_path, transform=None):\n        self.data = pd.read_csv(csv_path)\n        self.labels = pd.get_dummies(self.data['emotion']).as_matrix()\n        self.height = 48\n        self.width = 48\n        self.transform = transform\n\n    def __getitem__(self, index):\n        # This method should return only 1 sample and label \n        # (according to \"index\"), not the whole dataset\n        # So probably something like this for you:\n        pixel_sequence = self.data['pixels'][index]\n        face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n        face = np.asarray(face).reshape(self.width, self.height)\n        face = cv2.resize(face.astype('uint8'), (self.width, self.height))\n        label = self.labels[index]\n\n        return face, label\n\n    def __len__(self):\n        return len(self.labels)\n\n\ndataset = CustomDatasetFromCSV(my_path)\nbatch_size = 16\nvalidation_split = .2\nshuffle_dataset = True\nrandom_seed= 42\n\n# Creating data indices for training and validation splits:\ndataset_size = len(dataset)\nindices = list(range(dataset_size))\nsplit = int(np.floor(validation_split * dataset_size))\nif shuffle_dataset :\n    np.random.seed(random_seed)\n    np.random.shuffle(indices)\ntrain_indices, val_indices = indices[split:], indices[:split]\n\n# Creating PT data samplers and loaders:\ntrain_sampler = SubsetRandomSampler(train_indices)\nvalid_sampler = SubsetRandomSampler(val_indices)\n\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n                                           sampler=train_sampler)\nvalidation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                                sampler=valid_sampler)\n\n# Usage Example:\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    # Train:   \n    for batch_index, (faces, labels) in enumerate(train_loader):\n        # ...\n</code></pre>\n"}, {"title": "What does model.train() do in pytorch?", "question_body": "<p>Does it call <code>forward()</code> in <code>nn.Module</code>? I thought when we call the model, <code>forward</code> method is being used.\nWhy do we need to specify train()?</p>\n", "link": "https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch", "question_id": 51433378, "accepted_answer_id": 51433411, "answer_body": "<p><code>model.train()</code> tells your model that you are training the model. So effectively layers like dropout, batchnorm etc. which behave different on the train and test procedures know what is going on and hence can behave accordingly. </p>\n\n<p>More details: \nIt sets the mode to train \n(see <a href=\"https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.train\" rel=\"noreferrer\">source code</a>). You can call either model.eval() or model.train(mode=False) to tell that you are testing.\nIt is somewhat intuitive to expect <code>train</code> function to train model but it does not do that. It just sets the mode. </p>\n"}, {"title": "PyTorch memory model: &quot;torch.from_numpy()&quot; vs &quot;torch.Tensor()&quot;", "question_body": "<p>I'm trying to have an in-depth understanding of how PyTorch Tensor memory model works.</p>\n\n<pre><code># input numpy array\nIn [91]: arr = np.arange(10, dtype=float32).reshape(5, 2)\n\n# input tensors in two different ways\nIn [92]: t1, t2 = torch.Tensor(arr), torch.from_numpy(arr)\n\n# their types\nIn [93]: type(arr), type(t1), type(t2)\nOut[93]: (numpy.ndarray, torch.FloatTensor, torch.FloatTensor)\n\n# ndarray \nIn [94]: arr\nOut[94]: \narray([[ 0.,  1.],\n       [ 2.,  3.],\n       [ 4.,  5.],\n       [ 6.,  7.],\n       [ 8.,  9.]], dtype=float32)\n</code></pre>\n\n<hr>\n\n<p>I know that PyTorch tensors <em>share the memory buffer</em> of NumPy ndarrays. Thus, changing one will be reflected in the other. So, here I'm slicing and updating some values in the Tensor <code>t2</code></p>\n\n<pre><code>In [98]: t2[:, 1] = 23.0\n</code></pre>\n\n<p>And as expected, it's updated in <code>t2</code> and <code>arr</code> since they share the same memory buffer.</p>\n\n<pre><code>In [99]: t2\nOut[99]: \n\n  0  23\n  2  23\n  4  23\n  6  23\n  8  23\n[torch.FloatTensor of size 5x2]\n\n\nIn [101]: arr\nOut[101]: \narray([[  0.,  23.],\n       [  2.,  23.],\n       [  4.,  23.],\n       [  6.,  23.],\n       [  8.,  23.]], dtype=float32)\n</code></pre>\n\n<p>But, <strong><code>t1</code> is also updated</strong>. Remember that <code>t1</code> was constructed using <code>torch.Tensor()</code> whereas <code>t2</code> was constructed using <code>torch.from_numpy()</code></p>\n\n<pre><code>In [100]: t1\nOut[100]: \n\n  0  23\n  2  23\n  4  23\n  6  23\n  8  23\n[torch.FloatTensor of size 5x2]\n</code></pre>\n\n<p>So, no matter whether we use <a href=\"http://pytorch.org/docs/master/torch.html#torch.from_numpy\" rel=\"noreferrer\"><code>torch.from_numpy()</code></a> or <a href=\"http://pytorch.org/docs/master/tensors.html#torch-tensor\" rel=\"noreferrer\"><code>torch.Tensor()</code></a> to construct a tensor from an ndarray, <strong>all</strong> such tensors and ndarrays share the same memory buffer.</p>\n\n<p>Based on this understanding, my question is why does a dedicated function <a href=\"http://pytorch.org/docs/master/torch.html#torch.from_numpy\" rel=\"noreferrer\"><code>torch.from_numpy()</code></a> exists when simply <a href=\"http://pytorch.org/docs/master/tensors.html#torch-tensor\" rel=\"noreferrer\"><code>torch.Tensor()</code></a> can do the job?</p>\n\n<p>I looked at the PyTorch documentation but it doesn't mention anything about this? Any ideas/suggestions?</p>\n", "link": "https://stackoverflow.com/questions/48482787/pytorch-memory-model-torch-from-numpy-vs-torch-tensor", "question_id": 48482787, "accepted_answer_id": null}, {"title": "RuntimeError: Expected object of type torch.DoubleTensor but found type torch.FloatTensor for argument #2 &#39;weight&#39;", "question_body": "<p>My input tensor is torch.DoubleTensor type. But I got the RuntimeError below:</p>\n\n<pre><code>RuntimeError: Expected object of type torch.DoubleTensor but found type torch.FloatTensor for argument #2 'weight'\n</code></pre>\n\n<p>I didn't specify the type of the weight explicitly(i.e. I did not init my weight by myself. The weight is created by pytorch). What will influence the type of weight in the forward process?</p>\n\n<p>Thanks a lot!! </p>\n", "link": "https://stackoverflow.com/questions/49407303/runtimeerror-expected-object-of-type-torch-doubletensor-but-found-type-torch-fl", "question_id": 49407303, "accepted_answer_id": 49432639, "answer_body": "<p>The default type for <code>weights</code> and <code>biases</code> are <code>torch.FloatTensor</code>. So, you'll need to cast either your model to <code>torch.DoubleTensor</code> or cast your inputs to <code>torch.FloatTensor</code>. For casting your inputs you can do</p>\n\n<pre><code>X = X.float()\n</code></pre>\n\n<p>or cast your complete model to <code>DoubleTensor</code> as</p>\n\n<pre><code>model = model.double()\n</code></pre>\n\n<p>You can also set the default type for all tensors using</p>\n\n<pre><code>pytorch.set_default_tensor_type('torch.DoubleTensor')\n</code></pre>\n\n<p>It is better to convert your inputs to <code>float</code> rather than converting your model to <code>double</code>, because mathematical computations on <code>double</code> datatype is considerably slower on GPU.</p>\n"}, {"title": "What does the parameter retain_graph mean in the Variable&#39;s backward() method?", "question_body": "<p>I'm going through the <a href=\"http://pytorch.org/tutorials/advanced/neural_style_tutorial.html#packages\" rel=\"noreferrer\">neural transfer pytorch tutorial</a> and am confused about the use of <code>retain_variable</code>(deprecated, now referred to as <code>retain_graph</code>). The code example show:</p>\n\n<pre><code>class ContentLoss(nn.Module):\n\n    def __init__(self, target, weight):\n        super(ContentLoss, self).__init__()\n        self.target = target.detach() * weight\n        self.weight = weight\n        self.criterion = nn.MSELoss()\n\n    def forward(self, input):\n        self.loss = self.criterion(input * self.weight, self.target)\n        self.output = input\n        return self.output\n\n    def backward(self, retain_variables=True):\n        #Why is retain_variables True??\n        self.loss.backward(retain_variables=retain_variables)\n        return self.loss\n</code></pre>\n\n<p>From <a href=\"http://pytorch.org/docs/master/autograd.html?highlight=backward#torch.autograd.backward\" rel=\"noreferrer\">the documentation</a></p>\n\n<blockquote>\n  <p>retain_graph (bool, optional) \u2013 If False, the graph used to compute\n  the grad will be freed. Note that in nearly all cases setting this\n  option to True is not needed and often can be worked around in a much\n  more efficient way. Defaults to the value of create_graph.</p>\n</blockquote>\n\n<p>So by setting <code>retain_graph= True</code>, we're not freeing the memory allocated for the graph on the backward pass. What is the advantage of keeping this memory around, why do we need it?</p>\n", "link": "https://stackoverflow.com/questions/46774641/what-does-the-parameter-retain-graph-mean-in-the-variables-backward-method", "question_id": 46774641, "accepted_answer_id": 47174709, "answer_body": "<p>@cleros is pretty on the point about the use of <code>retain_graph=True</code>. In essence, it will retain any necessary information to calculate a certain variable, so that we can do backward pass on it.</p>\n\n<h2>An illustrative example</h2>\n\n<p><a href=\"https://i.stack.imgur.com/Stmud.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Stmud.png\" alt=\"enter image description here\"></a></p>\n\n<p>Suppose that we have a computation graph shown above. The variable <code>d</code> and <code>e</code> is the output, and <code>a</code> is the input. For example,</p>\n\n<pre><code>import torch\nfrom torch.autograd import Variable\na = Variable(torch.rand(1, 4), requires_grad=True)\nb = a**2\nc = b*2\nd = c.mean()\ne = c.sum()\n</code></pre>\n\n<p>when we do <code>d.backward()</code>, that is fine. After this computation, the part of graph that calculate <code>d</code> will be freed by default to save memory. So if we do <code>e.backward()</code>, the error message will pop up. In order to do <code>e.backward()</code>, we have to set the parameter <code>retain_graph</code> to <code>True</code> in <code>d.backward()</code>, i.e.,</p>\n\n<pre><code>d.backward(retain_graph=True)\n</code></pre>\n\n<p>As long as you use <code>retain_graph=True</code> in your backward method, you can do backward any time you want:</p>\n\n<pre><code>d.backward(retain_graph=True) # fine\ne.backward(retain_graph=True) # fine\nd.backward() # also fine\ne.backward() # error will occur!\n</code></pre>\n\n<p>More useful discussion can be found <a href=\"https://discuss.pytorch.org/t/runtimeerror-trying-to-backward-through-the-graph-a-second-time-but-the-buffers-have-already-been-freed-specify-retain-graph-true-when-calling-backward-the-first-time/6795/2?u=jdhao\" rel=\"noreferrer\">here</a>.</p>\n\n<h2>A real use case</h2>\n\n<p>Right now, a real use case is multi-task learning where you have multiple loss which maybe be at different layers. Suppose that you have 2 losses: <code>loss1</code> and <code>loss2</code> and they reside in different layers. In order to backprop the gradient of <code>loss1</code> and <code>loss2</code> w.r.t to the learnable weight of your network independently. You have to use <code>retain_graph=True</code> in <code>backward()</code> method in the first back-propagated loss.</p>\n\n<pre><code># suppose you first back-propagate loss1, then loss2 (you can also do the reverse)\nloss1.backward(retain_graph=True)\nloss2.backward() # now the graph is freed, and next process of batch gradient descent is ready\noptimizer.step() # update the network parameters\n</code></pre>\n"}, {"title": "Understanding torch.nn.Parameter", "question_body": "<p>I am new to pytorch and I have difficulty in understanding how <code>torch.nn.Parameter()</code> works.</p>\n\n<p>I have gone through the documentation in <a href=\"https://pytorch.org/docs/stable/nn.html\" rel=\"noreferrer\">https://pytorch.org/docs/stable/nn.html</a> but could make a very little sense out of it.</p>\n\n<p>Can someone please help?</p>\n\n<p>The code snippet that I am working on:</p>\n\n<pre><code>def __init__(self, weight):\n    super(Net, self).__init__()\n    # initializes the weights of the convolutional layer to be the weights of the 4 defined filters\n    k_height, k_width = weight.shape[2:]\n    # assumes there are 4 grayscale filters\n    self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False)\n    self.conv.weight = torch.nn.Parameter(weight)\n</code></pre>\n", "link": "https://stackoverflow.com/questions/50935345/understanding-torch-nn-parameter", "question_id": 50935345, "accepted_answer_id": null}, {"title": "nn.Dropout vs. F.dropout pyTorch", "question_body": "<p>By using pyTorch there is two ways to dropout\n <code>torch.nn.Dropout</code> and <code>torch.nn.F.Dropout</code>.</p>\n\n<p>I struggle to see the difference between the use of them <br>\n- when to use what?<br>\n- Does it make a difference?<br>\nI don't see any performance difference when I switched them around.</p>\n", "link": "https://stackoverflow.com/questions/53419474/nn-dropout-vs-f-dropout-pytorch", "question_id": 53419474, "accepted_answer_id": 53452827, "answer_body": "<p>The technical differences have already been shown in the other answer. However the main difference is that <code>nn.Dropout</code> is a torch Module itself which bears some convenience:</p>\n\n<p>A short example for illustration of some differences: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nimport torch.nn as nn\n\nclass Model1(nn.Module):\n    # Model 1 using functional dropout\n    def __init__(self, p=0.0):\n        super().__init__()\n        self.p = p\n\n    def forward(self, inputs):\n        return nn.functional.dropout(inputs, p=self.p, training=True)\n\nclass Model2(nn.Module):\n    # Model 2 using dropout module\n    def __init__(self, p=0.0):\n        super().__init__()\n        self.drop_layer = nn.Dropout(p=p)\n\n    def forward(self, inputs):\n        return self.drop_layer(inputs)\nmodel1 = Model1(p=0.5) # functional dropout \nmodel2 = Model2(p=0.5) # dropout module\n\n# creating inputs\ninputs = torch.rand(10)\n# forwarding inputs in train mode\nprint('Normal (train) model:')\nprint('Model 1', model1(inputs))\nprint('Model 2', model2(inputs))\nprint()\n\n# switching to eval mode\nmodel1.eval()\nmodel2.eval()\n\n# forwarding inputs in evaluation mode\nprint('Evaluation mode:')\nprint('Model 1', model1(inputs))\nprint('Model 2', model2(inputs))\n# show model summary\nprint('Print summary:')\nprint(model1)\nprint(model2)\n</code></pre>\n\n<p>Output:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>Normal (train) model:\nModel 1 tensor([ 1.5040,  0.0000,  0.0000,  0.8563,  0.0000,  0.0000,  1.5951,\n         0.0000,  0.0000,  0.0946])\nModel 2 tensor([ 0.0000,  0.3713,  1.9303,  0.0000,  0.0000,  0.3574,  0.0000,\n         1.1273,  1.5818,  0.0946])\n\nEvaluation mode:\nModel 1 tensor([ 0.0000,  0.3713,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000])\nModel 2 tensor([ 0.7520,  0.1857,  0.9651,  0.4281,  0.7883,  0.1787,  0.7975,\n         0.5636,  0.7909,  0.0473])\nPrint summary:\nModel1()\nModel2(\n  (drop_layer): Dropout(p=0.5)\n)\n</code></pre>\n\n<p><strong>So which should I use?</strong></p>\n\n<p>Both are completely equivalent in terms of applying dropout and even though the differences in usage are not that big, there are some reasons to favour the <code>nn.Dropout</code> over <code>nn.functional.dropout</code>:</p>\n\n<p>Dropout is designed to be only applied during training, so when doing predictions or evaluation of the model you want dropout to be turned off.</p>\n\n<p>The dropout module <code>nn.Dropout</code> conveniently handles this and shuts dropout off as soon as your model enters evaluation mode, while the functional dropout does not care about the evaluation / prediction mode. </p>\n\n<p>Even though you <em>can</em> set functional dropout to <code>training=False</code> to turn it off, it is still not such a convenient solution like with <code>nn.Dropout</code>.</p>\n\n<p>Also the drop rate is stored in the module, so you don't have to save it in an extra variable. In larger networks you might want to create different dropout layers with different drop rates - here <code>nn.Dropout</code> may increase readability and can bear also some convenience when using the layers multiple times.</p>\n\n<p>Finally, all modules which are assigned to your model are registered in your model. So you model class keeps track of them, that is why you can just turn off the dropout module by calling <code>eval()</code>. When using the functional dropout your model is not aware of it, thus it won't appear in any summary.</p>\n"}, {"title": "LSTM time sequence generation using PyTorch", "question_body": "<p>For several days now, I am trying to build a simple sine-wave sequence generation using LSTM, without any glimpse of success so far.</p>\n\n<p>I started from the <a href=\"https://github.com/pytorch/examples/tree/master/time_sequence_prediction\" rel=\"noreferrer\">time sequence prediction example</a></p>\n\n<p>All what I wanted to do differently is:</p>\n\n<ul>\n<li>Use different optimizers (e.g RMSprob) than LBFGS</li>\n<li>Try different signals (more sine-wave components)</li>\n</ul>\n\n<p>This is the link to <a href=\"https://github.com/osm3000/sequence_generation_pytorch.git\" rel=\"noreferrer\">my code</a>. \"experiment.py\" is the main file</p>\n\n<p>What I do is:</p>\n\n<ul>\n<li>I generate artificial time-series data (sine waves)</li>\n<li>I cut those time-series data into small sequences</li>\n<li>The input to my model is a sequence of time 0...T, and the output is a sequence of time 1...T+1</li>\n</ul>\n\n<p>What happens is:</p>\n\n<ul>\n<li>The training and the validation losses goes down smoothly</li>\n<li>The test loss is very low</li>\n<li>However, when I try to generate arbitrary-length sequences, starting from a seed (a random sequence from the test data), everything goes wrong. The output always flats out</li>\n</ul>\n\n<p><a href=\"https://i.stack.imgur.com/crO8z.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/crO8z.png\" alt=\"Shape of the generated signal\"></a></p>\n\n<p>I simply don't see what the problem is. I am playing with this for a week now, with no progress in sight.\nI would be very grateful for any help.</p>\n\n<p>Thank you</p>\n", "link": "https://stackoverflow.com/questions/43459013/lstm-time-sequence-generation-using-pytorch", "question_id": 43459013, "accepted_answer_id": null}, {"title": "Adding L1/L2 regularization in PyTorch?", "question_body": "<p>Is there any way, I can add simple L1/L2 regularization in PyTorch? We can probably compute the regularized loss by simply adding the <code>data_loss</code> with the <code>reg_loss</code> but is there any explicit way, any support from PyTorch library to do it more easily without doing it manually?</p>\n", "link": "https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch", "question_id": 42704283, "accepted_answer_id": 42723573, "answer_body": "<p>This is presented in the documentation for PyTorch. Have a look at <a href=\"http://pytorch.org/docs/optim.html#torch.optim.Adagrad\" rel=\"noreferrer\">http://pytorch.org/docs/optim.html#torch.optim.Adagrad</a>. You can add L2 loss using the weight decay parameter to the Optimization function.</p>\n"}]