[{"title": "How to calculate the sentence similarity using word2vec model of gensim with python", "question_body": "<p>According to the <a href=\"http://radimrehurek.com/gensim/models/word2vec.html\" rel=\"noreferrer\">Gensim Word2Vec</a>, I can use the word2vec model in gensim package to calculate the similarity between 2 words.</p>\n\n<p>e.g.</p>\n\n<pre><code>trained_model.similarity('woman', 'man') \n0.73723527\n</code></pre>\n\n<p>However, the word2vec model fails to predict the sentence similarity. I find out the LSI model with sentence similarity in gensim, but, which doesn't seem that can be combined with word2vec model. The length of corpus of each sentence I have is not very long (shorter than 10 words).  So, are there any simple ways to achieve the goal?</p>\n", "link": "https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt", "question_id": 22129943, "accepted_answer_id": 22130100, "answer_body": "<p>This is actually a pretty challenging problem that you are asking. Computing sentence similarity requires building a grammatical model of the sentence, understanding equivalent structures (e.g. \"he walked to the store yesterday\" and \"yesterday, he walked to the store\"), finding similarity not just in the pronouns and verbs but also in the proper nouns, finding statistical co-occurences / relationships in lots of real textual examples, etc.</p>\n\n<p>The simplest thing you could try -- though I don't know how well this would perform and it would certainly not give you the optimal results -- would be to first remove all \"stop\" words (words like \"the\", \"an\", etc. that don't add much meaning to the sentence) and then run word2vec on the words in both sentences, sum up the vectors in the one sentence, sum up the vectors in the other sentence, and then find the difference between the sums. By summing them up instead of doing a word-wise difference, you'll at least not be subject to word order. That being said, this will fail in lots of ways and isn't a good solution by any means (though good solutions to this problem almost always involve some amount of NLP, machine learning, and other cleverness).</p>\n\n<p>So, short answer is, no, there's no easy way to do this (at least not to do it well).</p>\n"}, {"title": "Convert word2vec bin file to text", "question_body": "<p>From the <a href=\"https://code.google.com/p/word2vec/\">word2vec</a> site I can download GoogleNews-vectors-negative300.bin.gz.  The .bin file (about 3.4GB) is a binary format not useful to me.  Tomas Mikolov <a href=\"https://groups.google.com/d/msg/word2vec-toolkit/lxbl_MB29Ic/g4uEz5rNV08J\">assures us</a> that \"It should be fairly straightforward to convert the binary format to text format (though that will take more disk space). Check the code in the distance tool, it's rather trivial to read the binary file.\"  Unfortunately, I don't know enough C to understand <a href=\"http://word2vec.googlecode.com/svn/trunk/distance.c\">http://word2vec.googlecode.com/svn/trunk/distance.c</a>.</p>\n\n<p>Supposedly <a href=\"http://radimrehurek.com/2014/02/word2vec-tutorial/\">gensim</a> can do this also, but all the tutorials I've found seem to be about converting <em>from</em> text, not the other way.</p>\n\n<p>Can someone suggest modifications to the C code or instructions for gensim to emit text?</p>\n", "link": "https://stackoverflow.com/questions/27324292/convert-word2vec-bin-file-to-text", "question_id": 27324292, "accepted_answer_id": 27329142, "answer_body": "<p>On the word2vec-toolkit mailing list Thomas Mensink has provided an <a href=\"https://groups.google.com/forum/#!topic/word2vec-toolkit/5Qh-x2O1lV4\" rel=\"noreferrer\">answer</a> in the form of a small C program that will convert a .bin file to text.  This is a modification of the distance.c file.  I replaced the original distance.c with Thomas's code below and rebuilt word2vec (make clean; make), and renamed the compiled distance to readbin.  Then <code>./readbin vector.bin</code> will create a text version of vector.bin.</p>\n\n<pre><code>//  Copyright 2013 Google Inc. All Rights Reserved.\n//\n//  Licensed under the Apache License, Version 2.0 (the \"License\");\n//  you may not use this file except in compliance with the License.\n//  You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n//  Unless required by applicable law or agreed to in writing, software\n//  distributed under the License is distributed on an \"AS IS\" BASIS,\n//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n//  See the License for the specific language governing permissions and\n//  limitations under the License.\n\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n#include &lt;math.h&gt;\n#include &lt;malloc.h&gt;\n\nconst long long max_size = 2000;         // max length of strings\nconst long long N = 40;                  // number of closest words that will be shown\nconst long long max_w = 50;              // max length of vocabulary entries\n\nint main(int argc, char **argv) {\n  FILE *f;\n  char file_name[max_size];\n  float len;\n  long long words, size, a, b;\n  char ch;\n  float *M;\n  char *vocab;\n  if (argc &lt; 2) {\n    printf(\"Usage: ./distance &lt;FILE&gt;\\nwhere FILE contains word projections in the BINARY FORMAT\\n\");\n    return 0;\n  }\n  strcpy(file_name, argv[1]);\n  f = fopen(file_name, \"rb\");\n  if (f == NULL) {\n    printf(\"Input file not found\\n\");\n    return -1;\n  }\n  fscanf(f, \"%lld\", &amp;words);\n  fscanf(f, \"%lld\", &amp;size);\n  vocab = (char *)malloc((long long)words * max_w * sizeof(char));\n  M = (float *)malloc((long long)words * (long long)size * sizeof(float));\n  if (M == NULL) {\n    printf(\"Cannot allocate memory: %lld MB    %lld  %lld\\n\", (long long)words * size * sizeof(float) / 1048576, words, size);\n    return -1;\n  }\n  for (b = 0; b &lt; words; b++) {\n    fscanf(f, \"%s%c\", &amp;vocab[b * max_w], &amp;ch);\n    for (a = 0; a &lt; size; a++) fread(&amp;M[a + b * size], sizeof(float), 1, f);\n    len = 0;\n    for (a = 0; a &lt; size; a++) len += M[a + b * size] * M[a + b * size];\n    len = sqrt(len);\n    for (a = 0; a &lt; size; a++) M[a + b * size] /= len;\n  }\n  fclose(f);\n  //Code added by Thomas Mensink\n  //output the vectors of the binary format in text\n  printf(\"%lld %lld #File: %s\\n\",words,size,file_name);\n  for (a = 0; a &lt; words; a++){\n    printf(\"%s \",&amp;vocab[a * max_w]);\n    for (b = 0; b&lt; size; b++){ printf(\"%f \",M[a*size + b]); }\n    printf(\"\\b\\b\\n\");\n  }  \n\n  return 0;\n}\n</code></pre>\n\n<p>I removed the \"\\b\\b\" from the <code>printf</code>.  </p>\n\n<p>By the way, the resulting text file still contained the text word and some unnecessary whitespace which I did not want for some numerical calculations.  I removed the initial text column and the trailing blank from each line with bash commands.</p>\n\n<pre><code>cut --complement -d ' ' -f 1 GoogleNews-vectors-negative300.txt &gt; GoogleNews-vectors-negative300_tuples-only.txt\nsed 's/ $//' GoogleNews-vectors-negative300_tuples-only.txt\n</code></pre>\n"}, {"title": "Doc2vec: How to get document vectors", "question_body": "<p>How to get document vectors of two text documents using Doc2vec?\nI am new to this, so it would be helpful if someone could point me in the right direction / help me with some tutorial</p>\n\n<p>I am using gensim.</p>\n\n<pre><code>doc1=[\"This is a sentence\",\"This is another sentence\"]\ndocuments1=[doc.strip().split(\" \") for doc in doc1 ]\nmodel = doc2vec.Doc2Vec(documents1, size = 100, window = 300, min_count = 10, workers=4)\n</code></pre>\n\n<p>I get </p>\n\n<blockquote>\n  <p>AttributeError: 'list' object has no attribute 'words'</p>\n</blockquote>\n\n<p>whenever I run this.</p>\n", "link": "https://stackoverflow.com/questions/31321209/doc2vec-how-to-get-document-vectors", "question_id": 31321209, "accepted_answer_id": null}, {"title": "gensim Doc2Vec vs tensorflow Doc2Vec", "question_body": "<p>I'm trying to compare my implementation of Doc2Vec (via tf) and gensims implementation. It seems atleast visually that the gensim ones are performing better.</p>\n\n<p>I ran the following code to train the gensim model and the one below that for tensorflow model. My questions are as follows:</p>\n\n<ol>\n<li>Is my tf implementation of Doc2Vec correct. Basically is it supposed to be concatenating the word vectors and the document vector to predict the middle word in a certain context?</li>\n<li>Does the <code>window=5</code> parameter in gensim mean that I am using two words on either side to predict the middle one? Or is it 5 on either side. Thing is there are quite a few documents that are smaller than length 10.</li>\n<li>Any insights as to why Gensim is performing better? Is my model any different to how they implement it?</li>\n<li>Considering that this is effectively a matrix factorisation problem, why is the TF model even getting an answer? There are infinite solutions to this since its a rank deficient problem. &lt;- This last question is simply a bonus.</li>\n</ol>\n\n<h3>Gensim</h3>\n\n<pre><code>model = Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=10, hs=0, min_count=2, workers=cores)\nmodel.build_vocab(corpus)\nepochs = 100\nfor i in range(epochs):\n    model.train(corpus)\n</code></pre>\n\n<h3>TF</h3>\n\n<pre><code>batch_size = 512\nembedding_size = 100 # Dimension of the embedding vector.\nnum_sampled = 10 # Number of negative examples to sample.\n\n\ngraph = tf.Graph()\n\nwith graph.as_default(), tf.device('/cpu:0'):\n    # Input data.\n    train_word_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n    train_doc_dataset = tf.placeholder(tf.int32, shape=[batch_size/context_window])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size/context_window, 1])\n\n    # The variables   \n    word_embeddings =  tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n    doc_embeddings = tf.Variable(tf.random_uniform([len_docs,embedding_size],-1.0,1.0))\n    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, (context_window+1)*embedding_size],\n                             stddev=1.0 / np.sqrt(embedding_size)))\n    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n    ###########################\n    # Model.\n    ###########################\n    # Look up embeddings for inputs and stack words side by side\n    embed_words = tf.reshape(tf.nn.embedding_lookup(word_embeddings, train_word_dataset),\n                            shape=[int(batch_size/context_window),-1])\n    embed_docs = tf.nn.embedding_lookup(doc_embeddings, train_doc_dataset)\n    embed = tf.concat(1,[embed_words, embed_docs])\n    # Compute the softmax loss, using a sample of the negative labels each time.\n    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,\n                                   train_labels, num_sampled, vocabulary_size))\n\n    # Optimizer.\n    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n</code></pre>\n\n<h2>Update:</h2>\n\n<p>Check out the jupyter notebook <a href=\"https://github.com/sachinruk/doc2vec_tf\" rel=\"noreferrer\">here</a> (I have both models working and tested in here). It still feels like the gensim model is performing better in this initial analysis.</p>\n", "link": "https://stackoverflow.com/questions/39843584/gensim-doc2vec-vs-tensorflow-doc2vec", "question_id": 39843584, "accepted_answer_id": null}, {"title": "How to create a word cloud from a corpus in Python?", "question_body": "<p>From <a href=\"https://stackoverflow.com/questions/15502802/creating-a-subset-of-words-from-a-corpus-in-r\">Creating a subset of words from a corpus in R</a>, the answerer can easily convert a <code>term-document matrix</code> into a word cloud easily.</p>\n\n<p>Is there a similar function from python libraries that takes either a raw word textfile or <code>NLTK</code> corpus or <code>Gensim</code> Mmcorpus into a word cloud?</p>\n\n<p>The result will look somewhat like this:\n<img src=\"https://i.stack.imgur.com/ieYK2.png\" alt=\"enter image description here\"></p>\n", "link": "https://stackoverflow.com/questions/16645799/how-to-create-a-word-cloud-from-a-corpus-in-python", "question_id": 16645799, "accepted_answer_id": 16739431, "answer_body": "<p>Here's a blog post which does just that: <a href=\"http://peekaboo-vision.blogspot.com/2012/11/a-wordcloud-in-python.html\">http://peekaboo-vision.blogspot.com/2012/11/a-wordcloud-in-python.html</a></p>\n\n<p>The whole code is here: <a href=\"https://github.com/amueller/word_cloud\">https://github.com/amueller/word_cloud</a></p>\n"}, {"title": "How to use Gensim doc2vec with pre-trained word vectors?", "question_body": "<p>I recently came across the doc2vec addition to Gensim. How can I use pre-trained word vectors (e.g. found in word2vec original website) with doc2vec?</p>\n\n<p>Or is doc2vec getting the word vectors from the same sentences it uses for paragraph-vector training?</p>\n\n<p>Thanks.</p>\n", "link": "https://stackoverflow.com/questions/27470670/how-to-use-gensim-doc2vec-with-pre-trained-word-vectors", "question_id": 27470670, "accepted_answer_id": null}, {"title": "Doc2Vec Get most similar documents", "question_body": "<p>I am trying to build a document retrieval model that returns most documents ordered by their relevancy with respect to a query or a search string. For this I trained a doc2vec model using the <code>Doc2Vec</code> model in gensim. My dataset is in the form of a pandas dataset which has each document stored as a string on each line. This is the code I have so far</p>\n\n<pre><code>import gensim, re\nimport pandas as pd\n\n# TOKENIZER\ndef tokenizer(input_string):\n    return re.findall(r\"[\\w']+\", input_string)\n\n# IMPORT DATA\ndata = pd.read_csv('mp_1002_prepd.txt')\ndata.columns = ['merged']\ndata.loc[:, 'tokens'] = data.merged.apply(tokenizer)\nsentences= []\nfor item_no, line in enumerate(data['tokens'].values.tolist()):\n    sentences.append(LabeledSentence(line,[item_no]))\n\n# MODEL PARAMETERS\ndm = 1 # 1 for distributed memory(default); 0 for dbow \ncores = multiprocessing.cpu_count()\nsize = 300\ncontext_window = 50\nseed = 42\nmin_count = 1\nalpha = 0.5\nmax_iter = 200\n\n# BUILD MODEL\nmodel = gensim.models.doc2vec.Doc2Vec(documents = sentences,\ndm = dm,\nalpha = alpha, # initial learning rate\nseed = seed,\nmin_count = min_count, # ignore words with freq less than min_count\nmax_vocab_size = None, # \nwindow = context_window, # the number of words before and after to be used as context\nsize = size, # is the dimensionality of the feature vector\nsample = 1e-4, # ?\nnegative = 5, # ?\nworkers = cores, # number of cores\niter = max_iter # number of iterations (epochs) over the corpus)\n\n# QUERY BASED DOC RANKING ??\n</code></pre>\n\n<p>The part where I am struggling is in finding documents that are most similar/relevant to the query. I used the <code>infer_vector</code> but then realised that it considers the query as a document, updates the model and returns the results. I tried using the <code>most_similar</code> and <code>most_similar_cosmul</code> methods but I get words along with a similarity score(I guess) in return. What I want to do is when I enter a search string(a query), I should get the documents (ids) that are most relevant along with a similarity score(cosine etc). How do I get this part done?</p>\n", "link": "https://stackoverflow.com/questions/42781292/doc2vec-get-most-similar-documents", "question_id": 42781292, "accepted_answer_id": 42817402, "answer_body": "<p>You need to use <code>infer_vector</code> to get a document vector of the new text - which does not alter the underlying model.</p>\n\n<p>Here is how you do it:</p>\n\n<pre><code>tokens = \"a new sentence to match\".split()\n\nnew_vector = model.infer_vector(tokens)\nsims = model.docvecs.most_similar([new_vector]) #gives you top 10 document tags and their cosine similarity\n</code></pre>\n\n<p>Edit: </p>\n\n<p>Here is an example of how the underlying model does not change after <code>infer_vec</code> is called.</p>\n\n<pre><code>import numpy as np\n\nwords = \"king queen man\".split()\n\nlen_before =  len(model.docvecs) #number of docs\n\n#word vectors for king, queen, man\nw_vec0 = model[words[0]]\nw_vec1 = model[words[1]]\nw_vec2 = model[words[2]]\n\nnew_vec = model.infer_vector(words)\n\nlen_after =  len(model.docvecs)\n\nprint np.array_equal(model[words[0]], w_vec0) # True\nprint np.array_equal(model[words[1]], w_vec1) # True\nprint np.array_equal(model[words[2]], w_vec2) # True\n\nprint len_before == len_after #True\n</code></pre>\n"}, {"title": "Update gensim word2vec model", "question_body": "<p>I have a word2vec model in gensim trained over 98892 documents. For any given sentence that is not present in the sentences array (i.e. the set over which I trained the model), I need to update the model with that sentence so that querying it next time gives out some results. I am doing it like this:</p>\n\n<pre><code>new_sentence = ['moscow', 'weather', 'cold']\nmodel.train(new_sentence)\n</code></pre>\n\n<p>and its printing this as logs:</p>\n\n<pre><code>2014-03-01 16:46:58,061 : INFO : training model with 1 workers on 98892 vocabulary and 100 features\n2014-03-01 16:46:58,211 : INFO : reached the end of input; waiting to finish 1 outstanding jobs\n2014-03-01 16:46:58,235 : INFO : training on 10 words took 0.1s, 174 words/s\n</code></pre>\n\n<p>Now, when I query with similar new_sentence for most positives (as <code>model.most_similar(positive=new_sentence)</code>) it gives out error:</p>\n\n<pre><code>Traceback (most recent call last):\n File \"&lt;pyshell#220&gt;\", line 1, in &lt;module&gt;\n model.most_similar(positive=['moscow', 'weather', 'cold'])\n File \"/Library/Python/2.7/site-packages/gensim/models/word2vec.py\", line 405, in most_similar\n raise KeyError(\"word '%s' not in vocabulary\" % word)\n  KeyError: \"word 'cold' not in vocabulary\"\n</code></pre>\n\n<p>Which indicates that the word 'cold' is not part of the vocabulary over which i trained the thing (am I right)?</p>\n\n<p>So the question is: How to update the model so that it gives out all the possible similarities for the given new sentence?</p>\n", "link": "https://stackoverflow.com/questions/22121028/update-gensim-word2vec-model", "question_id": 22121028, "accepted_answer_id": null}, {"title": "How to check if a key exists in a word2vec trained model or not", "question_body": "<p>I have trained a word2vec model using a corpus of documents with Gensim. Once the model is training, I am writing the following piece of code to get the raw feature vector of a word say \"view\".</p>\n\n<pre><code>myModel[\"view\"]\n</code></pre>\n\n<p>However, I get a KeyError for the word which is probably because this doesn't exist as a key in the list of keys indexed by word2vec. How can I check if a key exits in the index before trying to get the raw feature vector?</p>\n", "link": "https://stackoverflow.com/questions/30301922/how-to-check-if-a-key-exists-in-a-word2vec-trained-model-or-not", "question_id": 30301922, "accepted_answer_id": 42886555, "answer_body": "<p>convert the model into vectors with </p>\n\n<pre><code>word_vectors = model.wv\n</code></pre>\n\n<p>then we can use </p>\n\n<pre><code>if 'word' in word_vectors.vocab\n</code></pre>\n"}, {"title": "How to extract phrases from corpus using gensim", "question_body": "<p>For preprocessing the corpus I was planing to extarct common phrases from the corpus, for this I tried using <strong>Phrases</strong> model in gensim, I tried below code but it's not giving me desired output.</p>\n\n<p><strong>My code</strong></p>\n\n<pre><code>from gensim.models import Phrases\ndocuments = [\"the mayor of new york was there\", \"machine learning can be useful sometimes\"]\n\nsentence_stream = [doc.split(\" \") for doc in documents]\nbigram = Phrases(sentence_stream)\nsent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']\nprint(bigram[sent])\n</code></pre>\n\n<p><strong>Output</strong></p>\n\n<pre><code>[u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']\n</code></pre>\n\n<p><strong>But it should come as</strong> </p>\n\n<pre><code>[u'the', u'mayor', u'of', u'new_york', u'was', u'there']\n</code></pre>\n\n<p>But when I tried to print vocab of train data, I can see bigram, but its not working with test data, where I am going wrong?</p>\n\n<pre><code>print bigram.vocab\n\ndefaultdict(&lt;type 'int'&gt;, {'useful': 1, 'was_there': 1, 'learning_can': 1, 'learning': 1, 'of_new': 1, 'can_be': 1, 'mayor': 1, 'there': 1, 'machine': 1, 'new': 1, 'was': 1, 'useful_sometimes': 1, 'be': 1, 'mayor_of': 1, 'york_was': 1, 'york': 1, 'machine_learning': 1, 'the_mayor': 1, 'new_york': 1, 'of': 1, 'sometimes': 1, 'can': 1, 'be_useful': 1, 'the': 1}) \n</code></pre>\n", "link": "https://stackoverflow.com/questions/35716121/how-to-extract-phrases-from-corpus-using-gensim", "question_id": 35716121, "accepted_answer_id": null}, {"title": "gensim word2vec: Find number of words in vocabulary", "question_body": "<p>After training a word2vec model using python <a href=\"http://radimrehurek.com/gensim/models/word2vec.html\" rel=\"noreferrer\">gensim</a>, how do you find the number of words in the model's vocabulary?</p>\n", "link": "https://stackoverflow.com/questions/35596031/gensim-word2vec-find-number-of-words-in-vocabulary", "question_id": 35596031, "accepted_answer_id": 35641434, "answer_body": "<p>The vocabulary is in the <code>vocab</code> field of the Word2Vec model's <code>wv</code> property, as a dictionary, with the keys being each token (word). So it's just the usual Python for getting a dictionary's length:</p>\n\n<pre><code>len(w2v_model.wv.vocab)\n</code></pre>\n\n<p>(In older gensim versions before 0.13, <code>vocab</code> appeared directly on the model. So you would use <code>w2v_model.vocab</code> instead of <code>w2v_model.wv.vocab</code>.)</p>\n"}, {"title": "Understanding LDA implementation using gensim", "question_body": "<p>I am trying to understand how gensim package in Python implements Latent Dirichlet Allocation. I am doing the following:</p>\n\n<p>Define the dataset</p>\n\n<pre><code>documents = [\"Apple is releasing a new product\", \n             \"Amazon sells many things\",\n             \"Microsoft announces Nokia acquisition\"]             \n</code></pre>\n\n<p>After removing stopwords, I create the dictionary and the corpus:</p>\n\n<pre><code>texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\n</code></pre>\n\n<p>Then I define the LDA model.</p>\n\n<pre><code>lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, update_every=1, chunksize=10000, passes=1)\n</code></pre>\n\n<p>Then I print the topics:</p>\n\n<pre><code>&gt;&gt;&gt; lda.print_topics(5)\n['0.181*things + 0.181*amazon + 0.181*many + 0.181*sells + 0.031*nokia + 0.031*microsoft + 0.031*apple + 0.031*announces + 0.031*acquisition + 0.031*product', '0.077*nokia + 0.077*announces + 0.077*acquisition + 0.077*apple + 0.077*many + 0.077*amazon + 0.077*sells + 0.077*microsoft + 0.077*things + 0.077*new', '0.181*microsoft + 0.181*announces + 0.181*acquisition + 0.181*nokia + 0.031*many + 0.031*sells + 0.031*amazon + 0.031*apple + 0.031*new + 0.031*is', '0.077*acquisition + 0.077*announces + 0.077*sells + 0.077*amazon + 0.077*many + 0.077*nokia + 0.077*microsoft + 0.077*releasing + 0.077*apple + 0.077*new', '0.158*releasing + 0.158*is + 0.158*product + 0.158*new + 0.157*apple + 0.027*sells + 0.027*nokia + 0.027*announces + 0.027*acquisition + 0.027*microsoft']\n2013-12-03 13:26:21,878 : INFO : topic #0: 0.181*things + 0.181*amazon + 0.181*many + 0.181*sells + 0.031*nokia + 0.031*microsoft + 0.031*apple + 0.031*announces + 0.031*acquisition + 0.031*product\n2013-12-03 13:26:21,880 : INFO : topic #1: 0.077*nokia + 0.077*announces + 0.077*acquisition + 0.077*apple + 0.077*many + 0.077*amazon + 0.077*sells + 0.077*microsoft + 0.077*things + 0.077*new\n2013-12-03 13:26:21,880 : INFO : topic #2: 0.181*microsoft + 0.181*announces + 0.181*acquisition + 0.181*nokia + 0.031*many + 0.031*sells + 0.031*amazon + 0.031*apple + 0.031*new + 0.031*is\n2013-12-03 13:26:21,881 : INFO : topic #3: 0.077*acquisition + 0.077*announces + 0.077*sells + 0.077*amazon + 0.077*many + 0.077*nokia + 0.077*microsoft + 0.077*releasing + 0.077*apple + 0.077*new\n2013-12-03 13:26:21,881 : INFO : topic #4: 0.158*releasing + 0.158*is + 0.158*product + 0.158*new + 0.157*apple + 0.027*sells + 0.027*nokia + 0.027*announces + 0.027*acquisition + 0.027*microsoft\n&gt;&gt;&gt; \n</code></pre>\n\n<p>I'm not able to understand much out of this result. Is it providing with a probability of the occurrence of each word? Also, what's the meaning of topic #1, topic #2 etc? I was expecting something more or less like the most important keywords.</p>\n\n<p>I already checked the <a href=\"http://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation\" rel=\"noreferrer\">gensim tutorial</a> but it didn't really help much.</p>\n\n<p>Thanks.</p>\n", "link": "https://stackoverflow.com/questions/20349958/understanding-lda-implementation-using-gensim", "question_id": 20349958, "accepted_answer_id": 20350229, "answer_body": "<p>The answer you're looking for is in the <a href=\"http://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation\" rel=\"noreferrer\">gensim tutorial</a>.  <code>lda.printTopics(k)</code> prints the most contributing words for <code>k</code> randomly selected topics.  One can assume that this is (partially) the distribution of words over each of the given topics, meaning the probability of those words appearing in the topic to the left. </p>\n\n<p>Usually, one would run LDA on a large corpus. Running LDA on a ridiculously small sample won't give the best results.</p>\n"}, {"title": "Python Gensim: how to calculate document similarity using the LDA model?", "question_body": "<p>I've got a trained LDA model and I want to calculate the similarity score between two documents from the corpus I trained my model on.\nAfter studying all the Gensim tutorials and functions, I still can't get my head around it. Can somebody give me a hint? Thanks!</p>\n", "link": "https://stackoverflow.com/questions/22433884/python-gensim-how-to-calculate-document-similarity-using-the-lda-model", "question_id": 22433884, "accepted_answer_id": 22561795, "answer_body": "<p>Don't know if this'll help but, I managed to attain successful results on document matching and similarities when using the actual document as a query.</p>\n\n<pre><code>dictionary = corpora.Dictionary.load('dictionary.dict')\ncorpus = corpora.MmCorpus(\"corpus.mm\")\nlda = models.LdaModel.load(\"model.lda\") #result from running online lda (training)\n\nindex = similarities.MatrixSimilarity(lda[corpus])\nindex.save(\"simIndex.index\")\n\ndocname = \"docs/the_doc.txt\"\ndoc = open(docname, 'r').read()\nvec_bow = dictionary.doc2bow(doc.lower().split())\nvec_lda = lda[vec_bow]\n\nsims = index[vec_lda]\nsims = sorted(enumerate(sims), key=lambda item: -item[1])\nprint sims\n</code></pre>\n\n<p>Your similarity score between all documents residing in the corpus and the document that was used as a query will be the second index of every sim for sims.</p>\n"}, {"title": "Topic distribution: How do we see which document belong to which topic after doing LDA in python", "question_body": "<p>I am able to run the LDA code from gensim and got the top 10 topics with their respective keywords.</p>\n\n<p>Now I would like to go a step further to see how accurate the LDA algo is by seeing which document they cluster into each topic. Is this possible in gensim LDA?</p>\n\n<p>Basically i would like to do something like this, but in python and using gensim.</p>\n\n<p><a href=\"https://stackoverflow.com/questions/14875493/lda-with-topicmodels-how-can-i-see-which-topics-different-documents-belong-to\">LDA with topicmodels, how can I see which topics different documents belong to?</a></p>\n", "link": "https://stackoverflow.com/questions/20984841/topic-distribution-how-do-we-see-which-document-belong-to-which-topic-after-doi", "question_id": 20984841, "accepted_answer_id": 20991190, "answer_body": "<p>Using the probabilities of the topics, you can try to set some threshold and use it as a clustering baseline, but i am sure there are better ways to do clustering than this 'hacky' method.</p>\n\n<pre><code>from gensim import corpora, models, similarities\nfrom itertools import chain\n\n\"\"\" DEMO \"\"\"\ndocuments = [\"Human machine interface for lab abc computer applications\",\n             \"A survey of user opinion of computer system response time\",\n             \"The EPS user interface management system\",\n             \"System and human system engineering testing of EPS\",\n             \"Relation of user perceived response time to error measurement\",\n             \"The generation of random binary unordered trees\",\n             \"The intersection graph of paths in trees\",\n             \"Graph minors IV Widths of trees and well quasi ordering\",\n             \"Graph minors A survey\"]\n\n# remove common words and tokenize\nstoplist = set('for a of the and to in'.split())\ntexts = [[word for word in document.lower().split() if word not in stoplist]\n         for document in documents]\n\n# remove words that appear only once\nall_tokens = sum(texts, [])\ntokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\ntexts = [[word for word in text if word not in tokens_once] for text in texts]\n\n# Create Dictionary.\nid2word = corpora.Dictionary(texts)\n# Creates the Bag of Word corpus.\nmm = [id2word.doc2bow(text) for text in texts]\n\n# Trains the LDA models.\nlda = models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=3, \\\n                               update_every=1, chunksize=10000, passes=1)\n\n# Prints the topics.\nfor top in lda.print_topics():\n  print top\nprint\n\n# Assigns the topics to the documents in corpus\nlda_corpus = lda[mm]\n\n# Find the threshold, let's set the threshold to be 1/#clusters,\n# To prove that the threshold is sane, we average the sum of all probabilities:\nscores = list(chain(*[[score for topic_id,score in topic] \\\n                      for topic in [doc for doc in lda_corpus]]))\nthreshold = sum(scores)/len(scores)\nprint threshold\nprint\n\ncluster1 = [j for i,j in zip(lda_corpus,documents) if i[0][1] &gt; threshold]\ncluster2 = [j for i,j in zip(lda_corpus,documents) if i[1][1] &gt; threshold]\ncluster3 = [j for i,j in zip(lda_corpus,documents) if i[2][1] &gt; threshold]\n\nprint cluster1\nprint cluster2\nprint cluster3\n</code></pre>\n\n<p><code>[out]</code>:</p>\n\n<pre><code>0.131*trees + 0.121*graph + 0.119*system + 0.115*user + 0.098*survey + 0.082*interface + 0.080*eps + 0.064*minors + 0.056*response + 0.056*computer\n0.171*time + 0.171*user + 0.170*response + 0.082*survey + 0.080*computer + 0.079*system + 0.050*trees + 0.042*graph + 0.040*minors + 0.040*human\n0.155*system + 0.150*human + 0.110*graph + 0.107*minors + 0.094*trees + 0.090*eps + 0.088*computer + 0.087*interface + 0.040*survey + 0.028*user\n\n0.333333333333\n\n['The EPS user interface management system', 'The generation of random binary unordered trees', 'The intersection graph of paths in trees', 'Graph minors A survey']\n['A survey of user opinion of computer system response time', 'Relation of user perceived response time to error measurement']\n['Human machine interface for lab abc computer applications', 'System and human system engineering testing of EPS', 'Graph minors IV Widths of trees and well quasi ordering']\n</code></pre>\n\n<hr>\n\n<p>Just to make it clearer:</p>\n\n<pre><code># Find the threshold, let's set the threshold to be 1/#clusters,\n# To prove that the threshold is sane, we average the sum of all probabilities:\nscores = []\nfor doc in lda_corpus\n    for topic in doc:\n        for topic_id, score in topic:\n            scores.append(score)\nthreshold = sum(scores)/len(scores)\n</code></pre>\n\n<p>The above code is sum the score of all words and in all topics for all documents.\nThen normalize the sum by the number of scores.</p>\n"}, {"title": "word2vec lemmatization of corpus before training", "question_body": "<p>Word2vec seems to be mostly trained on raw corpus data. However, lemmatization is a standard preprocessing for many semantic similarity tasks. I was wondering if anybody had experience in lemmatizing the corpus before training word2vec and if this is a useful preprocessing step to do.</p>\n", "link": "https://stackoverflow.com/questions/23877375/word2vec-lemmatization-of-corpus-before-training", "question_id": 23877375, "accepted_answer_id": 23885178, "answer_body": "<p>I think it really matters about what you want to solve with this. It depends on the task. </p>\n\n<p>Essentially by lemmatization, you make the input space sparser, which can help if you don't have enough training data. </p>\n\n<p>But since Word2Vec is fairly big, if you have big enough training data, lemmatization shouldn't gain you much. </p>\n\n<p>Something more interesting is, how to do tokenization with respect to the existing diction of words-vectors inside the W2V (or anything else).  Like \"Good muffins cost $3.88\\nin New York.\" needs to be tokenized to ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New York.'] Then you can replace it with its vectors from W2V. The challenge is that some tokenizers my tokenize \"New York\" as ['New' 'York'], which doesn't make much sense. (For example, NLTK is making this mistake <a href=\"https://nltk.googlecode.com/svn/trunk/doc/howto/tokenize.html\" rel=\"nofollow noreferrer\">https://nltk.googlecode.com/svn/trunk/doc/howto/tokenize.html</a>) This is a problem when you have many multi-word phrases.</p>\n"}, {"title": "what does the vector of a word in word2vec represents?", "question_body": "<p><a href=\"https://code.google.com/p/word2vec/\" rel=\"noreferrer\">word2vec</a> is a open source tool by Google: </p>\n\n<ul>\n<li><p>For each word it provides a vector of float values, what exactly do they represent?</p></li>\n<li><p>There is also a paper on <a href=\"http://cs.stanford.edu/~quocle/paragraph_vector.pdf\" rel=\"noreferrer\">paragraph vector</a> can anyone explain how they are using word2vec in order to obtain fixed length vector for a paragraph.</p></li>\n</ul>\n", "link": "https://stackoverflow.com/questions/27032517/what-does-the-vector-of-a-word-in-word2vec-represents", "question_id": 27032517, "accepted_answer_id": 27258823, "answer_body": "<p><strong>TLDR</strong>: Word2Vec is building word projections (<em>embeddings</em>) in a <strong>latent space</strong> of N dimensions, (N being the size of the word vectors obtained). The float values represents the coordinates of the words in this N dimensional space.</p>\n\n<p>The major idea behind latent space projections, putting objects in a different and continuous dimensional space, is that your objects will have a representation (a vector) that has more interesting calculus characteristics than basic objects. </p>\n\n<p>For words, what's useful is that you have a <strong>dense</strong> vector space which encodes <strong>similarity</strong> (i.e tree has a vector which is more similar to wood than from dancing). This opposes to classical <strong>sparse</strong> one-hot or \"bag-of-word\" encoding which treat each word as one dimension making them <strong>orthogonal</strong> by design (i.e tree,wood and dancing all have the same distance between them)</p>\n\n<p>Word2Vec algorithms do this:</p>\n\n<p>Imagine that you have a sentence: </p>\n\n<blockquote>\n  <p>The dog has to go ___ for a walk in the park.</p>\n</blockquote>\n\n<p>You obviously want to fill the blank with the word \"outside\" but you could also have \"out\". The w2v algorithms are inspired by this idea. You'd like all words that fill in the blanks near, because they belong together - This is called the <strong>Distributional Hypothesis</strong> - Therefore the words \"out\" and \"outside\" will be closer together whereas a word like \"carrot\" would be farther away. </p>\n\n<p>This is sort of the \"intuition\" behind word2vec. For a more theorical explanation of what's going on i'd suggest reading:</p>\n\n<ul>\n<li><a href=\"http://nlp.stanford.edu/pubs/glove.pdf\" rel=\"nofollow noreferrer\">GloVe: Global Vectors for Word Representation</a></li>\n<li><a href=\"http://www.cs.bgu.ac.il/~yoavg/publications/conll2014analogies.pdf\" rel=\"nofollow noreferrer\">Linguistic Regularities in Sparse and Explicit Word Representations</a> </li>\n<li><a href=\"http://www.cs.bgu.ac.il/~yoavg/publications/nips2014pmi.pdf\" rel=\"nofollow noreferrer\">Neural Word Embedding as Implicit Matrix Factorization</a></li>\n</ul>\n\n<p>For paragraph vectors, the idea is the same as in w2v. Each paragraph can be represented by its words. Two models are presented in the paper. </p>\n\n<ol>\n<li>In a \"Bag of Word\" way (the pv-dbow model) where one <strong>fixed length</strong> paragraph vector is used to predict its words.</li>\n<li>By adding a <strong>fixed length</strong> paragraph token in word contexts (the pv-dm model). By retropropagating the gradient they get \"a sense\" of what's missing, bringing paragraph with the same words/topic \"missing\" close together.</li>\n</ol>\n\n<p><a href=\"http://cs.stanford.edu/~quocle/paragraph_vector.pdf\" rel=\"nofollow noreferrer\">Bits from the article</a>:</p>\n\n<blockquote>\n  <p>The\n  paragraph vector and word vectors are averaged or concatenated\n  to predict the next word in a context.\n  [...]\n  The paragraph token can be thought of as another word. It\n  acts as a memory that remembers what is missing from the\n  current context \u2013 or the topic of the paragraph</p>\n</blockquote>\n\n<p>For full understanding on how these vectors are built you'll need to learn how neural nets are built and how the backpropagation algorithm works. (i'd suggest starting by <a href=\"http://youtu.be/q0pm3BrIUFo\" rel=\"nofollow noreferrer\">this video</a> and Andrew NG's Coursera class)</p>\n\n<p><strong>NB:</strong> Softmax is just a fancy way of saying classification, each word in w2v algorithms is considered as a class. Hierarchical softmax/negative sampling are tricks to speed up softmax and handle a lot of classes.</p>\n"}, {"title": "Generator is not an iterator?", "question_body": "<p>I have an generator (a function that yields stuff), but when trying to pass it to <code>gensim.Word2Vec</code> I get the following error:</p>\n\n<blockquote>\n  <p>TypeError: You can't pass a generator as the sentences argument. Try an iterator.</p>\n</blockquote>\n\n<p>Isn't a generator a kind of iterator? If not, how do I make an iterator from it?</p>\n\n<p>Looking at the library code, it seems to simply iterate over sentences like <code>for x in enumerate(sentences)</code>, which works just fine with my generator. What is causing the error then?</p>\n", "link": "https://stackoverflow.com/questions/34166369/generator-is-not-an-iterator", "question_id": 34166369, "accepted_answer_id": 34166439, "answer_body": "<p>Generator is <strong>exhausted</strong> after one loop over it. Word2vec simply needs to traverse sentences multiple times (and probably get item for a given index, which is not possible for generators which are just a kind of stacks where you can only pop), thus requiring something more solid, like a list.</p>\n\n<p>In particular in their code they call two different functions, both iterate over sentences (thus if you use generator, the second one would run on an empty set)</p>\n\n<pre><code>self.build_vocab(sentences, trim_rule=trim_rule)\nself.train(sentences)\n</code></pre>\n\n<p>It should work with anything implementing <code>__iter__</code>  which is not <code>GeneratorType</code>. So wrap your function in an iterable interface and make sure that you can traverse it multiple times, meaning that</p>\n\n<pre><code>sentences = your_code\nfor s in sentences:\n  print s\nfor s in sentences:\n  print s\n</code></pre>\n\n<p>prints your collection twice</p>\n"}, {"title": "Using scikit-learn vectorizers and vocabularies with gensim", "question_body": "<p>I am trying to recycle scikit-learn vectorizer objects with gensim topic models. The reasons are simple: first of all, I already have a great deal of vectorized data; second, I prefer the interface and flexibility of scikit-learn vectorizers; third, even though topic modelling with gensim is very fast, computing its dictionaries (<code>Dictionary()</code>) is relatively slow in my experience.</p>\n\n<p>Similar questions have been asked before, <a href=\"https://stackoverflow.com/questions/19504898/use-sikit-tfidf-with-gensim-lda\">especially here</a> and <a href=\"https://stackoverflow.com/questions/15670525/how-do-you-initialize-a-gensim-corpus-variable-with-a-csr-matrix\">here</a>, and the bridging solution is gensim's <code>Sparse2Corpus()</code> function which transforms a Scipy sparse matrix into a gensim corpus object.</p>\n\n<p>However, this conversion does not make use of the <code>vocabulary_</code> attribute of sklearn vectorizers, which holds the mapping between words and feature ids. This mapping is necessary in order to print the discriminant words for each topic (<code>id2word</code> in gensim topic models, described as \"a a mapping from word ids (integers) to words (strings)\").</p>\n\n<p>I am aware of the fact that gensim's <code>Dictionary</code> objects are much more complex (and slower to compute) than scikit's <code>vect.vocabulary_</code> (a simple Python <code>dict</code>)...</p>\n\n<p>Any ideas to use <code>vect.vocabulary_</code> as <code>id2word</code> in gensim models?</p>\n\n<p>Some example code:</p>\n\n<pre><code># our data\ndocuments = [u'Human machine interface for lab abc computer applications',\n        u'A survey of user opinion of computer system response time',\n        u'The EPS user interface management system',\n        u'System and human system engineering testing of EPS',\n        u'Relation of user perceived response time to error measurement',\n        u'The generation of random binary unordered trees',\n        u'The intersection graph of paths in trees',\n        u'Graph minors IV Widths of trees and well quasi ordering',\n        u'Graph minors A survey']\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n# compute vector space with sklearn\nvect = CountVectorizer(min_df=1, ngram_range=(1, 1), max_features=25000)\ncorpus_vect = vect.fit_transform(documents)\n# each doc is a scipy sparse matrix\nprint vect.vocabulary_\n#{u'and': 1, u'minors': 20, u'generation': 9, u'testing': 32, u'iv': 15, u'engineering': 5, u'computer': 4, u'relation': 28, u'human': 11, u'measurement': 19, u'unordered': 37, u'binary': 3, u'abc': 0, u'for': 8, u'ordering': 23, u'graph': 10, u'system': 31, u'machine': 17, u'to': 35, u'quasi': 26, u'time': 34, u'random': 27, u'paths': 24, u'of': 21, u'trees': 36, u'applications': 2, u'management': 18, u'lab': 16, u'interface': 13, u'intersection': 14, u'response': 29, u'perceived': 25, u'in': 12, u'widths': 40, u'well': 39, u'eps': 6, u'survey': 30, u'error': 7, u'opinion': 22, u'the': 33, u'user': 38}\n\nimport gensim\n# transform sparse matrix into gensim corpus\ncorpus_vect_gensim = gensim.matutils.Sparse2Corpus(corpus_vect, documents_columns=False)\nlsi = gensim.models.LsiModel(corpus_vect_gensim, num_topics=4)\n# I instead would like something like this line below\n# lsi = gensim.models.LsiModel(corpus_vect_gensim, id2word=vect.vocabulary_, num_topics=2)\nprint lsi.print_topics(2)\n#['0.622*\"21\" + 0.359*\"31\" + 0.256*\"38\" + 0.206*\"29\" + 0.206*\"34\" + 0.197*\"36\" + 0.170*\"33\" + 0.168*\"1\" + 0.158*\"10\" + 0.147*\"4\"', '0.399*\"36\" + 0.364*\"10\" + -0.295*\"31\" + 0.245*\"20\" + -0.226*\"38\" + 0.194*\"26\" + 0.194*\"15\" + 0.194*\"39\" + 0.194*\"23\" + 0.194*\"40\"']\n</code></pre>\n", "link": "https://stackoverflow.com/questions/21552518/using-scikit-learn-vectorizers-and-vocabularies-with-gensim", "question_id": 21552518, "accepted_answer_id": 21553171, "answer_body": "<p>Gensim doesn't require <code>Dictionary</code> objects. You can use your plain <code>dict</code> as input to <code>id2word</code> directly, as long as it maps ids (integers) to words (strings).</p>\n\n<p>In fact anything dict-like will do (including <code>dict</code>, <code>Dictionary</code>, <code>SqliteDict</code>...).</p>\n\n<p>(Btw gensim's <code>Dictionary</code> is a simple Python <code>dict</code> underneath.\nNot sure where your remarks on <code>Dictionary</code> performance come from, you can't get a mapping much faster than a plain <code>dict</code> in Python. Maybe you're confusing it with text preprocessing (not part of gensim), which can indeed be slow.)</p>\n"}, {"title": "What is the simplest way to get tfidf with pandas dataframe?", "question_body": "<p>I want to calculate tf-idf from the documents below. I'm using python and pandas.</p>\n\n<pre><code>import pandas as pd\ndf = pd.DataFrame({'docId': [1,2,3], \n               'sent': ['This is the first sentence','This is the second sentence', 'This is the third sentence']})\n</code></pre>\n\n<p>First, I thought I would need to get word_count for each row. So I wrote a simple function:</p>\n\n<pre><code>def word_count(sent):\n    word2cnt = dict()\n    for word in sent.split():\n        if word in word2cnt: word2cnt[word] += 1\n        else: word2cnt[word] = 1\nreturn word2cnt\n</code></pre>\n\n<p>And then, I applied it to each row.</p>\n\n<pre><code>df['word_count'] = df['sent'].apply(word_count)\n</code></pre>\n\n<p>But now I'm lost. I know there's an easy method to calculate tf-idf if I use Graphlab, but I want to stick with an open source option. Both Sklearn and gensim look overwhelming. What's the simplest solution to get tf-idf?</p>\n", "link": "https://stackoverflow.com/questions/37593293/what-is-the-simplest-way-to-get-tfidf-with-pandas-dataframe", "question_id": 37593293, "accepted_answer_id": null}, {"title": "PyTorch / Gensim - How to load pre-trained word embeddings", "question_body": "<p>I want to load a pre-trained word2vec embedding with gensim into a PyTorch embedding layer.</p>\n\n<p>So my question is, how do I get the embedding weights loaded by gensim into the PyTorch embedding layer.</p>\n\n<p>Thanks in Advance!</p>\n", "link": "https://stackoverflow.com/questions/49710537/pytorch-gensim-how-to-load-pre-trained-word-embeddings", "question_id": 49710537, "accepted_answer_id": 49802495, "answer_body": "<p>I just wanted to report my findings about loading a gensim embedding with PyTorch.</p>\n\n<hr>\n\n<ul>\n<li><h2>Solution for PyTorch <code>0.4.0</code> and newer:</h2></li>\n</ul>\n\n<p>From <code>v0.4.0</code> there is a new function <a href=\"https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding.from_pretrained\" rel=\"noreferrer\"><code>from_pretrained()</code></a> which makes loading an embedding very comfortable.\nHere is an example from the documentation.</p>\n\n<pre><code>&gt;&gt; # FloatTensor containing pretrained weights\n&gt;&gt; weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n&gt;&gt; embedding = nn.Embedding.from_pretrained(weight)\n&gt;&gt; # Get embeddings for index 1\n&gt;&gt; input = torch.LongTensor([1])\n&gt;&gt; embedding(input)\n</code></pre>\n\n<p>The weights from <a href=\"https://radimrehurek.com/gensim/\" rel=\"noreferrer\"><em>gensim</em></a> can easily be obtained by:</p>\n\n<pre><code>import gensim\nmodel = gensim.models.KeyedVectors.load_word2vec_format('path/to/file')\nweights = torch.FloatTensor(model.vectors) # formerly syn0, which is soon deprecated\n</code></pre>\n\n<hr>\n\n<ul>\n<li><h2>Solution for PyTorch version <code>0.3.1</code> and older:</h2></li>\n</ul>\n\n<p>I'm using version <code>0.3.1</code> and <a href=\"https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding.from_pretrained\" rel=\"noreferrer\"><code>from_pretrained()</code></a> isn't available in this version.</p>\n\n<p>Therefore I created my own <code>from_pretrained</code> so I can also use it with <code>0.3.1</code>.</p>\n\n<p><em>Code for <code>from_pretrained</code> for PyTorch versions <code>0.3.1</code> or lower:</em></p>\n\n<pre><code>def from_pretrained(embeddings, freeze=True):\n    assert embeddings.dim() == 2, \\\n         'Embeddings parameter is expected to be 2-dimensional'\n    rows, cols = embeddings.shape\n    embedding = torch.nn.Embedding(num_embeddings=rows, embedding_dim=cols)\n    embedding.weight = torch.nn.Parameter(embeddings)\n    embedding.weight.requires_grad = not freeze\n    return embedding\n</code></pre>\n\n<p>The embedding can be loaded then just like this:</p>\n\n<pre><code>embedding = from_pretrained(weights)\n</code></pre>\n\n<p>I hope this is helpful for someone.</p>\n"}, {"title": "Load PreComputed Vectors Gensim", "question_body": "<p>I am using the Gensim Python package to learn a neural language model, and I know that you can provide a training corpus to learn the model. However, there already exist many precomputed word vectors available in text format (e.g. <a href=\"http://www-nlp.stanford.edu/projects/glove/\" rel=\"noreferrer\">http://www-nlp.stanford.edu/projects/glove/</a>). Is there some way to initialize a Gensim Word2Vec model that just makes use of some precomputed vectors, rather than having to learn the vectors from scratch?</p>\n\n<p>Thanks! </p>\n", "link": "https://stackoverflow.com/questions/27139908/load-precomputed-vectors-gensim", "question_id": 27139908, "accepted_answer_id": 27462798, "answer_body": "<p>You can download pre-trained word vectors from here (get the file 'GoogleNews-vectors-negative300.bin'):\n<a href=\"https://code.google.com/p/word2vec/\" rel=\"noreferrer\">word2vec</a></p>\n\n<p>Extract the file and then you can load it in python like:</p>\n\n<pre><code>model = gensim.models.word2vec.Word2Vec.load_word2vec_format(os.path.join(os.path.dirname(__file__), 'GoogleNews-vectors-negative300.bin'), binary=True)\n\nmodel.most_similar('dog')\n</code></pre>\n\n<p>EDIT (May 2017):\nAs the above code is now deprecated, this is how you'd load the vectors now:</p>\n\n<pre><code>model = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(os.path.dirname(__file__), 'GoogleNews-vectors-negative300.bin'), binary=True)\n</code></pre>\n"}, {"title": "What meaning does the length of a Word2vec vector have?", "question_body": "<p>I am using Word2vec through <a href=\"https://radimrehurek.com/gensim/\" rel=\"noreferrer\"><em>gensim</em></a> with Google's pretrained vectors trained on Google News. I have noticed that the word vectors I can access by doing direct index lookups on the <code>Word2Vec</code> object are not unit vectors:</p>\n\n<pre><code>&gt;&gt;&gt; import numpy\n&gt;&gt;&gt; from gensim.models import Word2Vec\n&gt;&gt;&gt; w2v = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n&gt;&gt;&gt; king_vector = w2v['king']\n&gt;&gt;&gt; numpy.linalg.norm(king_vector)\n2.9022589\n</code></pre>\n\n<p>However, in the <a href=\"https://github.com/piskvorky/gensim/blob/0.12.4/gensim/models/word2vec.py#L1153-L1213\" rel=\"noreferrer\"><code>most_similar</code></a> method, these non-unit vectors are not used; instead, normalised versions are used from the undocumented <code>.syn0norm</code> property, which contains only unit vectors:</p>\n\n<pre><code>&gt;&gt;&gt; w2v.init_sims()\n&gt;&gt;&gt; unit_king_vector = w2v.syn0norm[w2v.vocab['king'].index]\n&gt;&gt;&gt; numpy.linalg.norm(unit_king_vector)\n0.99999994\n</code></pre>\n\n<p>The larger vector is just a scaled up version of the unit vector:</p>\n\n<pre><code>&gt;&gt;&gt; king_vector - numpy.linalg.norm(king_vector) * unit_king_vector\narray([  0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,\n         0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,\n        -7.45058060e-09,   0.00000000e+00,   3.72529030e-09,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n        ... (some lines omitted) ...\n        -1.86264515e-09,  -3.72529030e-09,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32)\n</code></pre>\n\n<p>Given that word similarity comparisons in Word2Vec are done by <a href=\"https://en.wikipedia.org/wiki/Cosine_similarity\" rel=\"noreferrer\">cosine similarity</a>, it's not obvious to me what the lengths of the non-normalised vectors mean - although I assume they mean <em>something</em>, since gensim exposes them to me rather than only exposing the unit vectors in <code>.syn0norm</code>.</p>\n\n<p>How are the lengths of these non-normalised Word2vec vectors generated, and what is their meaning? For what calculations does it make sense to use the normalised vectors, and when should I use the non-normalised ones?</p>\n", "link": "https://stackoverflow.com/questions/36034454/what-meaning-does-the-length-of-a-word2vec-vector-have", "question_id": 36034454, "accepted_answer_id": 50550236, "answer_body": "<p>I think the answer you are looking for is described in the 2015 paper <a href=\"https://arxiv.org/pdf/1508.02297.pdf\" rel=\"noreferrer\">Measuring Word Significance\nusing\nDistributed Representations of Words</a> by Adriaan Schakel and Benjamin Wilson. The key points:</p>\n\n<blockquote>\n  <p>When a word appears\n  in different contexts, its vector gets moved in\n  different directions during updates. The final vector\n  then represents some sort of weighted average\n  over the various contexts. Averaging over vectors\n  that point in different directions typically results in\n  a vector that gets shorter with increasing number\n  of different contexts in which the word appears.\n  For words to be used in many different contexts,\n  they must carry little meaning. Prime examples of\n  such insignificant words are high-frequency stop\n  words, which are indeed represented by short vectors\n  despite their high term frequencies ...</p>\n</blockquote>\n\n<hr>\n\n<blockquote>\n  <p>For given term frequency,\n  the vector length is seen to take values only in a\n  narrow interval. That interval initially shifts upwards\n  with increasing frequency. Around a frequency\n  of about 30, that trend reverses and the interval\n  shifts downwards.</p>\n  \n  <p>...</p>\n  \n  <p>Both forces determining the length of a word\n  vector are seen at work here. Small-frequency\n  words tend to be used consistently, so that the\n  more frequently such words appear, the longer\n  their vectors. This tendency is reflected by the upwards\n  trend in Fig. 3 at low frequencies. High-frequency\n  words, on the other hand, tend to be\n  used in many different contexts, the more so, the\n  more frequently they occur. The averaging over\n  an increasing number of different contexts shortens\n  the vectors representing such words. This tendency\n  is clearly reflected by the downwards trend\n  in Fig. 3 at high frequencies, culminating in punctuation\n  marks and stop words with short vectors at\n  the very end.</p>\n  \n  <p>...</p>\n  \n  <p><a href=\"https://i.stack.imgur.com/NI9je.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/NI9je.png\" alt=\"Graph showing the trend described in the previous excerpt\"></a></p>\n  \n  <p>Figure 3: Word vector length <em>v</em> versus term frequency\n  <em>tf</em> of all words in the hep-th vocabulary.\n  Note the logarithmic scale used on the frequency\n  axis. The dark symbols denote bin means with the\n  <i>k</i>th bin containing the frequencies in the interval\n  [2<sup><i>k\u22121</i></sup>, 2<sup><i>k</i></sup> \u2212 1] with <em>k</em> = 1, 2, 3, . . .. These means\n  are included as a guide to the eye. The horizontal\n  line indicates the length <em>v</em> = 1.37 of the mean\n  vector</p>\n</blockquote>\n\n<hr>\n\n<blockquote>\n  <h3>4 Discussion</h3>\n  \n  <p>Most applications of distributed representations of\n  words obtained through word2vec so far centered\n  around semantics. A host of experiments have\n  demonstrated the extent to which the direction of\n  word vectors captures semantics. In this brief report,\n  it was pointed out that not only the direction,\n  but also the length of word vectors carries important\n  information. Specifically, it was shown that\n  word vector length furnishes, in combination with\n  term frequency, a useful measure of word significance. </p>\n</blockquote>\n"}, {"title": "Get most similar words, given the vector of the word (not the word itself)", "question_body": "<p>Using the <code>gensim.models.Word2Vec</code> library, you have the possibility to provide a model and a \"word\" for which you want to find the list of most similar words:</p>\n\n<pre><code>model = gensim.models.Word2Vec.load_word2vec_format(model_file, binary=True)\nmodel.most_similar(positive=[WORD], topn=N)\n</code></pre>\n\n<p>I wonder if there is a possibility to give the system as input the model and a \"vector\", and ask the system to return the top similar words (which their vectors is very close to the given vector). Something similar to:</p>\n\n<pre><code>model.most_similar(positive=[VECTOR], topn=N)\n</code></pre>\n\n<p>I need this functionality for a bilingual setting, in which I have 2 models (English and German), as well as some English words for which I need to find their most similar German candidates.\nWhat I want to do is to get the vector of each English word from the English model:</p>\n\n<pre><code>model_EN = gensim.models.Word2Vec.load_word2vec_format(model_file_EN, binary=True)\nvector_w_en=model_EN[WORD_EN]\n</code></pre>\n\n<p>and then query the German model with these vectors.</p>\n\n<pre><code>model_DE = gensim.models.Word2Vec.load_word2vec_format(model_file_DE, binary=True)\nmodel_DE.most_similar(positive=[vector_w_en], topn=N)\n</code></pre>\n\n<p>I have implemented this in C using the original distance function in the word2vec package. But, now I need it to be in python, in order to be able to integrate it with my other scripts.</p>\n\n<p>Do you know if there is already a method in <code>gensim.models.Word2Vec</code> library or other similar libraries which does this? Do I need to implement it by myself?</p>\n", "link": "https://stackoverflow.com/questions/37818426/get-most-similar-words-given-the-vector-of-the-word-not-the-word-itself", "question_id": 37818426, "accepted_answer_id": null}, {"title": "Matching words and vectors in gensim Word2Vec model", "question_body": "<p>I have had the <a href=\"https://radimrehurek.com/gensim/\" rel=\"noreferrer\">gensim</a> <a href=\"https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\" rel=\"noreferrer\">Word2Vec</a> implementation compute some word embeddings for me. Everything went quite fantastically as far as I can tell; now I am clustering the word vectors created, hoping to get some semantic groupings. </p>\n\n<p>As a next step, I would like to look at the words (rather than the vectors) contained in each cluster. I.e. if I have the vector of embeddings <code>[x, y, z]</code>, I would like to find out which actual word this vector represents. I can get the words/Vocab items by calling <code>model.vocab</code> and the word vectors through <code>model.syn0</code>. But I could not find a location where these are explicitly matched.  </p>\n\n<p>This was more complicated than I expected and I feel I might be missing the obvious way of doing it. Any help is appreciated!</p>\n\n<h3>Problem:</h3>\n\n<p>Match words to embedding vectors created by <code>Word2Vec ()</code> -- how do I do it? </p>\n\n<h3>My approach:</h3>\n\n<p>After creating the model (code below*), I would now like to match the indexes assigned to each word (during the <code>build_vocab()</code> phase) to the vector matrix outputted as <code>model.syn0</code>.\nThus</p>\n\n<pre><code>for i in range (0, newmod.syn0.shape[0]): #iterate over all words in model\n    print i\n    word= [k for k in newmod.vocab if newmod.vocab[k].__dict__['index']==i] #get the word out of the internal dicationary by its index\n    wordvector= newmod.syn0[i] #get the vector with the corresponding index\n    print wordvector == newmod[word] #testing: compare result of looking up the word in the model -- this prints True\n</code></pre>\n\n<ul>\n<li><p>Is there a better way of doing this, e.g. by feeding the vector into the model to match the word?</p></li>\n<li><p>Does this even get me correct results?</p></li>\n</ul>\n\n<p>*My code to create the word vectors:</p>\n\n<pre><code>model = Word2Vec(size=1000, min_count=5, workers=4, sg=1)\n\nmodel.build_vocab(sentencefeeder(folderlist)) #sentencefeeder puts out sentences as lists of strings\n\nmodel.save(\"newmodel\")\n</code></pre>\n\n<p>I found <a href=\"https://stackoverflow.com/questions/35914287/word2vec-how-to-get-words-from-vectors\">this question</a> which is similar but has not really been answered. </p>\n", "link": "https://stackoverflow.com/questions/38665556/matching-words-and-vectors-in-gensim-word2vec-model", "question_id": 38665556, "accepted_answer_id": 44579542, "answer_body": "<p>So I found an easy way to do this, where <code>nmodel</code> is the name of your model. </p>\n\n<pre><code>#zip the two lists containing vectors and words\nzipped = zip(nmodel.wv.index2word, nmodel.wv.syn0)\n\n#the resulting list contains `(word, wordvector)` tuples. We can extract the entry for any `word` or `vector` (replace with the word/vector you're looking for) using a list comprehension:\nwordresult = [i for i in zipped if i[0] == word]\nvecresult = [i for i in zipped if i[1] == vector]\n</code></pre>\n\n<p>This is based on the <a href=\"https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py\" rel=\"noreferrer\">gensim code</a>. For older versions of gensim, you might need to drop the <code>wv</code> after the model.  </p>\n"}, {"title": "Interpreting the sum of TF-IDF scores of words across documents", "question_body": "<p>First let's extract the TF-IDF scores per term per document:</p>\n\n<pre><code>from gensim import corpora, models, similarities\ndocuments = [\"Human machine interface for lab abc computer applications\",\n              \"A survey of user opinion of computer system response time\",\n              \"The EPS user interface management system\",\n              \"System and human system engineering testing of EPS\",\n              \"Relation of user perceived response time to error measurement\",\n              \"The generation of random binary unordered trees\",\n              \"The intersection graph of paths in trees\",\n              \"Graph minors IV Widths of trees and well quasi ordering\",\n              \"Graph minors A survey\"]\nstoplist = set('for a of the and to in'.split())\ntexts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\ntfidf = models.TfidfModel(corpus)\ncorpus_tfidf = tfidf[corpus]\n</code></pre>\n\n<p>Printing it out:</p>\n\n<pre><code>for doc in corpus_tfidf:\n    print doc\n</code></pre>\n\n<p>[out]:</p>\n\n<pre><code>[(0, 0.4301019571350565), (1, 0.4301019571350565), (2, 0.4301019571350565), (3, 0.4301019571350565), (4, 0.2944198962221451), (5, 0.2944198962221451), (6, 0.2944198962221451)]\n[(4, 0.3726494271826947), (7, 0.27219160459794917), (8, 0.3726494271826947), (9, 0.27219160459794917), (10, 0.3726494271826947), (11, 0.5443832091958983), (12, 0.3726494271826947)]\n[(6, 0.438482464916089), (7, 0.32027755044706185), (9, 0.32027755044706185), (13, 0.6405551008941237), (14, 0.438482464916089)]\n[(5, 0.3449874408519962), (7, 0.5039733231394895), (14, 0.3449874408519962), (15, 0.5039733231394895), (16, 0.5039733231394895)]\n[(9, 0.21953536176370683), (10, 0.30055933182961736), (12, 0.30055933182961736), (17, 0.43907072352741366), (18, 0.43907072352741366), (19, 0.43907072352741366), (20, 0.43907072352741366)]\n[(21, 0.48507125007266594), (22, 0.48507125007266594), (23, 0.48507125007266594), (24, 0.48507125007266594), (25, 0.24253562503633297)]\n[(25, 0.31622776601683794), (26, 0.31622776601683794), (27, 0.6324555320336759), (28, 0.6324555320336759)]\n[(25, 0.20466057569885868), (26, 0.20466057569885868), (29, 0.2801947048062438), (30, 0.40932115139771735), (31, 0.40932115139771735), (32, 0.40932115139771735), (33, 0.40932115139771735), (34, 0.40932115139771735)]\n[(8, 0.6282580468670046), (26, 0.45889394536615247), (29, 0.6282580468670046)]\n</code></pre>\n\n<p>If we want to find the \"saliency\" or \"importance\" of the words within this corpus, <strong>can we simple do the sum of the tf-idf scores across all documents and divide it by the number of documents?</strong> I.e. </p>\n\n<pre><code>&gt;&gt;&gt; tfidf_saliency = Counter()\n&gt;&gt;&gt; for doc in corpus_tfidf:\n...     for word, score in doc:\n...         tfidf_saliency[word] += score / len(corpus_tfidf)\n... \n&gt;&gt;&gt; tfidf_saliency\nCounter({7: 0.12182694202050007, 8: 0.11121194156107769, 26: 0.10886469856464989, 29: 0.10093919463036093, 9: 0.09022272408985754, 14: 0.08705221175200946, 25: 0.08482488519466996, 6: 0.08143359568202602, 10: 0.07480097322359022, 12: 0.07480097322359022, 4: 0.07411881371164887, 13: 0.07117278898823597, 5: 0.07104525967490458, 27: 0.07027283689263066, 28: 0.07027283689263066, 11: 0.060487023243988705, 15: 0.055997035904387725, 16: 0.055997035904387725, 21: 0.05389680556362955, 22: 0.05389680556362955, 23: 0.05389680556362955, 24: 0.05389680556362955, 17: 0.048785635947490406, 18: 0.048785635947490406, 19: 0.048785635947490406, 20: 0.048785635947490406, 0: 0.04778910634833961, 1: 0.04778910634833961, 2: 0.04778910634833961, 3: 0.04778910634833961, 30: 0.045480127933079706, 31: 0.045480127933079706, 32: 0.045480127933079706, 33: 0.045480127933079706, 34: 0.045480127933079706})\n</code></pre>\n\n<p>Looking at the output, could we assume that the most \"prominent\" word in the corpus is:</p>\n\n<pre><code>&gt;&gt;&gt; dictionary[7]\nu'system'\n&gt;&gt;&gt; dictionary[8]\nu'survey'\n&gt;&gt;&gt; dictionary[26]\nu'graph'\n</code></pre>\n\n<p>If so, <strong>what is the mathematical interpretation of the sum of TF-IDF scores of words across documents?</strong></p>\n", "link": "https://stackoverflow.com/questions/42269313/interpreting-the-sum-of-tf-idf-scores-of-words-across-documents", "question_id": 42269313, "accepted_answer_id": null}, {"title": "How to print the LDA topics models from gensim? Python", "question_body": "<p>Using <code>gensim</code> I was able to extract topics from a set of documents in LSA but how do I access the topics generated from the LDA models?</p>\n\n<p>When printing the <code>lda.print_topics(10)</code> the code gave the following error because <code>print_topics()</code> return a <code>NoneType</code>:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"/home/alvas/workspace/XLINGTOP/xlingtop.py\", line 93, in &lt;module&gt;\n    for top in lda.print_topics(2):\nTypeError: 'NoneType' object is not iterable\n</code></pre>\n\n<p>The code:</p>\n\n<pre><code>from gensim import corpora, models, similarities\nfrom gensim.models import hdpmodel, ldamodel\nfrom itertools import izip\n\ndocuments = [\"Human machine interface for lab abc computer applications\",\n              \"A survey of user opinion of computer system response time\",\n              \"The EPS user interface management system\",\n              \"System and human system engineering testing of EPS\",\n              \"Relation of user perceived response time to error measurement\",\n              \"The generation of random binary unordered trees\",\n              \"The intersection graph of paths in trees\",\n              \"Graph minors IV Widths of trees and well quasi ordering\",\n              \"Graph minors A survey\"]\n\n# remove common words and tokenize\nstoplist = set('for a of the and to in'.split())\ntexts = [[word for word in document.lower().split() if word not in stoplist]\n         for document in documents]\n\n# remove words that appear only once\nall_tokens = sum(texts, [])\ntokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\ntexts = [[word for word in text if word not in tokens_once]\n         for text in texts]\n\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\n\n# I can print out the topics for LSA\nlsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)\ncorpus_lsi = lsi[corpus]\n\nfor l,t in izip(corpus_lsi,corpus):\n  print l,\"#\",t\nprint\nfor top in lsi.print_topics(2):\n  print top\n\n# I can print out the documents and which is the most probable topics for each doc.\nlda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=50)\ncorpus_lda = lda[corpus]\n\nfor l,t in izip(corpus_lda,corpus):\n  print l,\"#\",t\nprint\n\n# But I am unable to print out the topics, how should i do it?\nfor top in lda.print_topics(10):\n  print top\n</code></pre>\n", "link": "https://stackoverflow.com/questions/15016025/how-to-print-the-lda-topics-models-from-gensim-python", "question_id": 15016025, "accepted_answer_id": null}, {"title": "Document topical distribution in Gensim LDA", "question_body": "<p>I've derived a LDA topic model using a toy corpus as follows:</p>\n\n<pre><code>documents = ['Human machine interface for lab abc computer applications',\n             'A survey of user opinion of computer system response time',\n             'The EPS user interface management system',\n             'System and human system engineering testing of EPS',\n             'Relation of user perceived response time to error measurement',\n             'The generation of random binary unordered trees',\n             'The intersection graph of paths in trees',\n             'Graph minors IV Widths of trees and well quasi ordering',\n             'Graph minors A survey']\n\ntexts = [[word for word in document.lower().split()] for document in documents]\ndictionary = corpora.Dictionary(texts)\n\nid2word = {}\nfor word in dictionary.token2id:    \n    id2word[dictionary.token2id[word]] = word\n</code></pre>\n\n<p>I found that when I use a small number of topics to derive the model, Gensim yields a full report of topical distribution over all potential topics for a test document. E.g.:</p>\n\n<pre><code>test_lda = LdaModel(corpus,num_topics=5, id2word=id2word)\ntest_lda[dictionary.doc2bow('human system')]\n\nOut[314]: [(0, 0.59751626959781134),\n(1, 0.10001902477790173),\n(2, 0.10001375856907335),\n(3, 0.10005453508763221),\n(4, 0.10239641196758137)]\n</code></pre>\n\n<p>However when I use a large number of topics, the report is no longer complete:</p>\n\n<pre><code>test_lda = LdaModel(corpus,num_topics=100, id2word=id2word)\n\ntest_lda[dictionary.doc2bow('human system')]\nOut[315]: [(73, 0.50499999999997613)]\n</code></pre>\n\n<p>It seems to me that topics with a probability less than some threshold (I observed 0.01 to be more specific) are omitted form the output.</p>\n\n<p>I'm wondering if this behaviour is due to some aesthetic considerations? And how can I get the distribution of the probability mass residual over all other topics?</p>\n\n<p>Thank you for your kind answer! </p>\n", "link": "https://stackoverflow.com/questions/17310933/document-topical-distribution-in-gensim-lda", "question_id": 17310933, "accepted_answer_id": 19530686, "answer_body": "<p>Read the <a href=\"https://github.com/piskvorky/gensim/blob/develop/gensim/models/ldamodel.py\" rel=\"noreferrer\">source</a> and it turns out that topics with probabilities smaller than a threshold are ignored. This threshold is with a default value of 0.01.</p>\n"}, {"title": "Python: gensim: RuntimeError: you must first build vocabulary before training the model", "question_body": "<p>I know that this question has been asked already, but I was still not able to find a solution for it. </p>\n\n<p>I would like to use gensim's <code>word2vec</code> on a custom data set, but now I'm still figuring out in what format the dataset has to be. I had a look at <a href=\"http://streamhacker.com/2014/12/29/word2vec-nltk/\">this post</a> where the input is basically a list of lists (one big list containing other lists that are tokenized sentences from the NLTK Brown corpus). So I thought that this is the input format I have to use for the command <code>word2vec.Word2Vec()</code>. However, it won't work with my little test set and I don't understand why.</p>\n\n<p>What I have tried:</p>\n\n<p><strong>This worked</strong>:</p>\n\n<pre><code>from gensim.models import word2vec\nfrom nltk.corpus import brown\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\nbrown_vecs = word2vec.Word2Vec(brown.sents())\n</code></pre>\n\n<p><strong>This didn't work</strong>:</p>\n\n<pre><code>sentences = [ \"the quick brown fox jumps over the lazy dogs\",\"yoyoyo you go home now to sleep\"]\nvocab = [s.encode('utf-8').split() for s in sentences]\nvoc_vec = word2vec.Word2Vec(vocab)\n</code></pre>\n\n<p>I don't understand why it doesn't work with the \"mock\" data, even though it has the same data structure as the sentences from the Brown corpus:</p>\n\n<p><strong>vocab</strong>:</p>\n\n<pre><code>[['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dogs'], ['yoyoyo', 'you', 'go', 'home', 'now', 'to', 'sleep']]\n</code></pre>\n\n<p><strong>brown.sents()</strong>: (the beginning of it)</p>\n\n<pre><code>[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n</code></pre>\n\n<p>Can anyone please tell me what I'm doing wrong? </p>\n", "link": "https://stackoverflow.com/questions/33989826/python-gensim-runtimeerror-you-must-first-build-vocabulary-before-training-th", "question_id": 33989826, "accepted_answer_id": 33991111, "answer_body": "<p>Default <code>min_count</code> in gensim's Word2Vec is set to 5. If there is no word in your vocab with frequency greater than 4, your vocab will be empty and hence the error. Try</p>\n\n<pre><code>voc_vec = word2vec.Word2Vec(vocab, min_count=1)\n</code></pre>\n"}, {"title": "gensim error : no module named gensim", "question_body": "<p>I trying to import gensim.</p>\n\n<p>I have the following code</p>\n\n<pre><code>import gensim\nmodel = gensim.models.Word2Vec.load_word2vec_format('./model/GoogleNews-  \nvectors-negative300.bin', binary=True)  \n</code></pre>\n\n<p>I got the following error.</p>\n\n<pre><code>ImportError                               Traceback (most recent call  \nlast)\n&lt;ipython-input-5-50007be813d4&gt; in &lt;module&gt;()\n----&gt; 1 import gensim\n  2 model = gensim.models.Word2Vec.load_word2vec_format('./model  \n/GoogleNews-vectors-negative300.bin', binary=True)\n\nImportError: No module named 'gensim'\n</code></pre>\n\n<p>I installed gensim in python. I use genssim for word2vec.</p>\n", "link": "https://stackoverflow.com/questions/46168600/gensim-error-no-module-named-gensim", "question_id": 46168600, "accepted_answer_id": null}, {"title": "Gensim train word2vec on wikipedia - preprocessing and parameters", "question_body": "<p>I am trying to train the word2vec model from <code>gensim</code> using the Italian wikipedia\n\"<a href=\"http://dumps.wikimedia.org/itwiki/latest/itwiki-latest-pages-articles.xml.bz2\" rel=\"nofollow noreferrer\">http://dumps.wikimedia.org/itwiki/latest/itwiki-latest-pages-articles.xml.bz2</a>\"</p>\n\n<p>However, I am not sure what is the best preprocessing for this corpus.</p>\n\n<p><code>gensim</code> model accepts a list of tokenized sentences.\nMy first try is to just use the standard <code>WikipediaCorpus</code> preprocessor from <code>gensim</code>. This extract each article, remove punctuation and split words on spaces. With this tool each sentence would correspond to an entire model, and I am not sure of the impact of this fact on the model.</p>\n\n<p>After this I train the model with default parameters. Unfortunately after training it seems that I do not manage to obtain very meaningful similarities.</p>\n\n<p>What is the most appropriate preprocessing on the Wikipedia corpus for this task? (if this questions are too broad please help me by pointing to a relevant tutorial / article )</p>\n\n<p>This the code of my first trial:</p>\n\n<pre><code>from gensim.corpora import WikiCorpus\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\ncorpus = WikiCorpus('itwiki-latest-pages-articles.xml.bz2',dictionary=False)\nmax_sentence = -1\n\ndef generate_lines():\n    for index, text in enumerate(corpus.get_texts()):\n        if index &lt; max_sentence or max_sentence==-1:\n            yield text\n        else:\n            break\n\nfrom gensim.models.word2vec import BrownCorpus, Word2Vec\nmodel = Word2Vec() \nmodel.build_vocab(generate_lines()) #This strangely builds a vocab of \"only\" 747904 words which is &lt;&lt; than those reported in the literature 10M words\nmodel.train(generate_lines(),chunksize=500)\n</code></pre>\n", "link": "https://stackoverflow.com/questions/23735576/gensim-train-word2vec-on-wikipedia-preprocessing-and-parameters", "question_id": 23735576, "accepted_answer_id": 23959741, "answer_body": "<p>Your approach is fine.</p>\n\n<pre><code>model.build_vocab(generate_lines()) #This strangely builds a vocab of \"only\" 747904 words which is &lt;&lt; than those reported in the literature 10M words\n</code></pre>\n\n<p>This could be because of pruning infrequent words (the default is <code>min_count=5</code>).</p>\n\n<p>To speed up computation, you can consider \"caching\" the preprocessed articles as a plain <code>.txt.gz</code> file, one sentence (document) per line, and then simply using <a href=\"http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence\">word2vec.LineSentence</a> corpus. This saves parsing the bzipped wiki XML on every iteration.</p>\n\n<p>Why word2vec doesn't produce \"meaningful similarities\" for Italian wiki, I don't know. English wiki seems to work fine. See also <a href=\"https://groups.google.com/forum/#!topic/gensim/MJWrDw_IvXw\">here</a>.</p>\n"}]