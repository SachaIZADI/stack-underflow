[{"title": "Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2", "question_body": "\n\n<p>I am new to TensorFlow. I have recently installed it (Windows CPU version) and received the following message:</p>\n\n<blockquote>\n  <p>Successfully installed tensorflow-1.4.0 tensorflow-tensorboard-0.4.0rc2</p>\n</blockquote>\n\n<p>Then when I tried to run</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import tensorflow as tf\nhello = tf.constant('Hello, TensorFlow!')\nsess = tf.Session()\nsess.run(hello)\n'Hello, TensorFlow!'\na = tf.constant(10)\nb = tf.constant(32)\nsess.run(a + b)\n42\nsess.close()\n</code></pre>\n\n<p>(which I found through <a href=\"https://github.com/tensorflow/tensorflow\" rel=\"noreferrer\">https://github.com/tensorflow/tensorflow</a>)</p>\n\n<p>I received the following message:</p>\n\n<blockquote>\n  <p>2017-11-02 01:56:21.698935: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2</p>\n</blockquote>\n\n<p>But when I ran </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import tensorflow as tf\nhello = tf.constant('Hello, TensorFlow!')\nsess = tf.Session()\nprint(sess.run(hello))\n</code></pre>\n\n<p>it ran as it should and output <code>Hello, TensorFlow!</code>, which indicates that the installation was successful indeed but there is something else that is wrong.</p>\n\n<p>Do you know what the problem is and how to fix it? Thanks.</p>\n", "link": "https://stackoverflow.com/questions/47068709/your-cpu-supports-instructions-that-this-tensorflow-binary-was-not-compiled-to-u", "question_id": 47068709, "accepted_answer_id": 47227886, "answer_body": "<h2>What is this warning about?</h2>\n\n<p>Modern CPUs provide a lot of low-level instructions, besides the usual arithmetic and logic, known as extensions, e.g. SSE2, SSE4, AVX, etc. From the <a href=\"https://en.wikipedia.org/wiki/Advanced_Vector_Extensions\" rel=\"noreferrer\">Wikipedia</a>:</p>\n\n<blockquote>\n  <p><strong>Advanced Vector Extensions</strong> (<strong>AVX</strong>) are extensions to the x86 instruction\n  set architecture for microprocessors from Intel and AMD proposed by\n  Intel in March 2008 and first supported by Intel with the Sandy\n  Bridge processor shipping in Q1 2011 and later on by AMD with the\n  Bulldozer processor shipping in Q3 2011. AVX provides new features,\n  new instructions and a new coding scheme.</p>\n</blockquote>\n\n<p>In particular, AVX introduces <a href=\"https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation#Fused_multiply.E2.80.93add\" rel=\"noreferrer\">fused multiply-accumulate</a> (FMA) operations, which speed up linear algebra computation, namely dot-product, matrix multiply, convolution, etc. Almost every machine-learning training involves a great deal of these operations, hence will be faster on a CPU that supports AVX and FMA (up to 300%). The warning states that your CPU does support AVX (hooray!).</p>\n\n<p>I'd like to stress here: it's all about <strong>CPU only</strong>.</p>\n\n<h2>Why isn't it used then?</h2>\n\n<p>Because tensorflow default distribution is built <a href=\"https://github.com/tensorflow/tensorflow/issues/7778\" rel=\"noreferrer\">without CPU extensions</a>, such as SSE4.1, SSE4.2, AVX, AVX2, FMA, etc. The default builds (ones from <code>pip install tensorflow</code>) are intended to be compatible with as many CPUs as possible. Another argument is that even with these extensions CPU is a lot slower than a GPU, and it's expected for medium- and large-scale machine-learning training to be performed on a GPU.</p>\n\n<h2>What should you do?</h2>\n\n<p><strong>If you have a GPU</strong>, you shouldn't care about AVX support, because most expensive ops will be dispatched on a GPU device (unless explicitly set not to). In this case, you can simply ignore this warning by</p>\n\n<pre><code># Just disables the warning, doesn't enable AVX/FMA\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n</code></pre>\n\n<p>... or by setting <code>export TF_CPP_MIN_LOG_LEVEL=2</code> if you're on Unix. Tensorflow is working fine anyway, but you won't see these annoying warnings.</p>\n\n<hr>\n\n<p><strong>If you don't have a GPU</strong> and want to utilize CPU as much as possible, <strong>you should build tensorflow from the source optimized for <em>your</em> CPU</strong> with AVX, AVX2, and FMA enabled if your CPU supports them. It's been discussed in <a href=\"https://stackoverflow.com/q/41293077/712995\">this question</a> and also <a href=\"https://github.com/tensorflow/tensorflow/issues/8037\" rel=\"noreferrer\">this GitHub issue</a>. Tensorflow uses an ad-hoc build system called <a href=\"https://bazel.build/\" rel=\"noreferrer\">bazel</a> and building it is not that trivial, but is certainly doable. After this, not only will the warning disappear, tensorflow performance should also improve.</p>\n"}, {"title": "Tensorflow: how to save/restore a model?", "question_body": "<p>After you train a model in Tensorflow: </p>\n\n<ol>\n<li>How do you save the trained model?</li>\n<li>How do you later restore this saved model?</li>\n</ol>\n", "link": "https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model", "question_id": 33759623, "accepted_answer_id": 50852627, "answer_body": "<h1>Docs</h1>\n\n<p>They built an exhaustive and useful tutorial -> <a href=\"https://www.tensorflow.org/guide/saved_model\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/guide/saved_model</a></p>\n\n<p>From the docs:</p>\n\n<h3>Save</h3>\n\n<pre><code># Create some variables.\nv1 = tf.get_variable(\"v1\", shape=[3], initializer = tf.zeros_initializer)\nv2 = tf.get_variable(\"v2\", shape=[5], initializer = tf.zeros_initializer)\n\ninc_v1 = v1.assign(v1+1)\ndec_v2 = v2.assign(v2-1)\n\n# Add an op to initialize the variables.\ninit_op = tf.global_variables_initializer()\n\n# Add ops to save and restore all the variables.\nsaver = tf.train.Saver()\n\n# Later, launch the model, initialize the variables, do some work, and save the\n# variables to disk.\nwith tf.Session() as sess:\n  sess.run(init_op)\n  # Do some work with the model.\n  inc_v1.op.run()\n  dec_v2.op.run()\n  # Save the variables to disk.\n  save_path = saver.save(sess, \"/tmp/model.ckpt\")\n  print(\"Model saved in path: %s\" % save_path)\n</code></pre>\n\n<h3>Restore</h3>\n\n<pre><code>tf.reset_default_graph()\n\n# Create some variables.\nv1 = tf.get_variable(\"v1\", shape=[3])\nv2 = tf.get_variable(\"v2\", shape=[5])\n\n# Add ops to save and restore all the variables.\nsaver = tf.train.Saver()\n\n# Later, launch the model, use the saver to restore variables from disk, and\n# do some work with the model.\nwith tf.Session() as sess:\n  # Restore variables from disk.\n  saver.restore(sess, \"/tmp/model.ckpt\")\n  print(\"Model restored.\")\n  # Check the values of the variables\n  print(\"v1 : %s\" % v1.eval())\n  print(\"v2 : %s\" % v2.eval())\n</code></pre>\n\n<h1>Tensorflow 2</h1>\n\n<p>This is still beta so I'd advise against for now. If you still want to go down that road here is the <a href=\"https://www.tensorflow.org/beta/guide/saved_model\" rel=\"nofollow noreferrer\"><code>tf.saved_model</code> usage guide</a></p>\n\n<h1>Tensorflow &lt; 2</h1>\n\n<h2><code>simple_save</code></h2>\n\n<p>Many good answer, for completeness I'll add my 2 cents: <strong><a href=\"https://www.tensorflow.org/programmers_guide/saved_model\" rel=\"nofollow noreferrer\">simple_save</a></strong>. Also a standalone code example using the <code>tf.data.Dataset</code> API.</p>\n\n<p>Python 3 ; Tensorflow <strong>1.14</strong></p>\n\n<pre><code>import tensorflow as tf\nfrom tensorflow.saved_model import tag_constants\n\nwith tf.Graph().as_default():\n    with tf.Session() as sess:\n        ...\n\n        # Saving\n        inputs = {\n            \"batch_size_placeholder\": batch_size_placeholder,\n            \"features_placeholder\": features_placeholder,\n            \"labels_placeholder\": labels_placeholder,\n        }\n        outputs = {\"prediction\": model_output}\n        tf.saved_model.simple_save(\n            sess, 'path/to/your/location/', inputs, outputs\n        )\n</code></pre>\n\n<p>Restoring:</p>\n\n<pre><code>graph = tf.Graph()\nwith restored_graph.as_default():\n    with tf.Session() as sess:\n        tf.saved_model.loader.load(\n            sess,\n            [tag_constants.SERVING],\n            'path/to/your/location/',\n        )\n        batch_size_placeholder = graph.get_tensor_by_name('batch_size_placeholder:0')\n        features_placeholder = graph.get_tensor_by_name('features_placeholder:0')\n        labels_placeholder = graph.get_tensor_by_name('labels_placeholder:0')\n        prediction = restored_graph.get_tensor_by_name('dense/BiasAdd:0')\n\n        sess.run(prediction, feed_dict={\n            batch_size_placeholder: some_value,\n            features_placeholder: some_other_value,\n            labels_placeholder: another_value\n        })\n</code></pre>\n\n<h1>Standalone example</h1>\n\n<p><strong><a href=\"http://vict0rsch.github.io/2018/05/17/restore-tf-model-dataset/\" rel=\"nofollow noreferrer\">Original blog post</a></strong></p>\n\n<p>The following code generates random data for the sake of the demonstration.</p>\n\n<ol>\n<li>We start by creating the placeholders. They will hold the data at runtime. From them, we create the <code>Dataset</code> and then its <code>Iterator</code>. We get the iterator's generated tensor, called <code>input_tensor</code> which will serve as input to our model.</li>\n<li>The model itself is built from <code>input_tensor</code>: a GRU-based bidirectional RNN followed by a dense classifier. Because why not.</li>\n<li>The loss is a <code>softmax_cross_entropy_with_logits</code>, optimized with <code>Adam</code>. After 2 epochs (of 2 batches each), we save the \"trained\" model with <code>tf.saved_model.simple_save</code>. If you run the code as is, then the model will be saved in a folder called <code>simple/</code> in your current working directory.</li>\n<li>In a new graph, we then restore the saved model with <code>tf.saved_model.loader.load</code>. We grab the placeholders and logits with <code>graph.get_tensor_by_name</code> and the <code>Iterator</code> initializing operation with <code>graph.get_operation_by_name</code>.</li>\n<li>Lastly we run an inference for both batches in the dataset, and check that the saved and restored model both yield the same values. They do!</li>\n</ol>\n\n<p>Code:</p>\n\n<pre><code>import os\nimport shutil\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.saved_model import tag_constants\n\n\ndef model(graph, input_tensor):\n    \"\"\"Create the model which consists of\n    a bidirectional rnn (GRU(10)) followed by a dense classifier\n\n    Args:\n        graph (tf.Graph): Tensors' graph\n        input_tensor (tf.Tensor): Tensor fed as input to the model\n\n    Returns:\n        tf.Tensor: the model's output layer Tensor\n    \"\"\"\n    cell = tf.nn.rnn_cell.GRUCell(10)\n    with graph.as_default():\n        ((fw_outputs, bw_outputs), (fw_state, bw_state)) = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw=cell,\n            cell_bw=cell,\n            inputs=input_tensor,\n            sequence_length=[10] * 32,\n            dtype=tf.float32,\n            swap_memory=True,\n            scope=None)\n        outputs = tf.concat((fw_outputs, bw_outputs), 2)\n        mean = tf.reduce_mean(outputs, axis=1)\n        dense = tf.layers.dense(mean, 5, activation=None)\n\n        return dense\n\n\ndef get_opt_op(graph, logits, labels_tensor):\n    \"\"\"Create optimization operation from model's logits and labels\n\n    Args:\n        graph (tf.Graph): Tensors' graph\n        logits (tf.Tensor): The model's output without activation\n        labels_tensor (tf.Tensor): Target labels\n\n    Returns:\n        tf.Operation: the operation performing a stem of Adam optimizer\n    \"\"\"\n    with graph.as_default():\n        with tf.variable_scope('loss'):\n            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n                    logits=logits, labels=labels_tensor, name='xent'),\n                    name=\"mean-xent\"\n                    )\n        with tf.variable_scope('optimizer'):\n            opt_op = tf.train.AdamOptimizer(1e-2).minimize(loss)\n        return opt_op\n\n\nif __name__ == '__main__':\n    # Set random seed for reproducibility\n    # and create synthetic data\n    np.random.seed(0)\n    features = np.random.randn(64, 10, 30)\n    labels = np.eye(5)[np.random.randint(0, 5, (64,))]\n\n    graph1 = tf.Graph()\n    with graph1.as_default():\n        # Random seed for reproducibility\n        tf.set_random_seed(0)\n        # Placeholders\n        batch_size_ph = tf.placeholder(tf.int64, name='batch_size_ph')\n        features_data_ph = tf.placeholder(tf.float32, [None, None, 30], 'features_data_ph')\n        labels_data_ph = tf.placeholder(tf.int32, [None, 5], 'labels_data_ph')\n        # Dataset\n        dataset = tf.data.Dataset.from_tensor_slices((features_data_ph, labels_data_ph))\n        dataset = dataset.batch(batch_size_ph)\n        iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\n        dataset_init_op = iterator.make_initializer(dataset, name='dataset_init')\n        input_tensor, labels_tensor = iterator.get_next()\n\n        # Model\n        logits = model(graph1, input_tensor)\n        # Optimization\n        opt_op = get_opt_op(graph1, logits, labels_tensor)\n\n        with tf.Session(graph=graph1) as sess:\n            # Initialize variables\n            tf.global_variables_initializer().run(session=sess)\n            for epoch in range(3):\n                batch = 0\n                # Initialize dataset (could feed epochs in Dataset.repeat(epochs))\n                sess.run(\n                    dataset_init_op,\n                    feed_dict={\n                        features_data_ph: features,\n                        labels_data_ph: labels,\n                        batch_size_ph: 32\n                    })\n                values = []\n                while True:\n                    try:\n                        if epoch &lt; 2:\n                            # Training\n                            _, value = sess.run([opt_op, logits])\n                            print('Epoch {}, batch {} | Sample value: {}'.format(epoch, batch, value[0]))\n                            batch += 1\n                        else:\n                            # Final inference\n                            values.append(sess.run(logits))\n                            print('Epoch {}, batch {} | Final inference | Sample value: {}'.format(epoch, batch, values[-1][0]))\n                            batch += 1\n                    except tf.errors.OutOfRangeError:\n                        break\n            # Save model state\n            print('\\nSaving...')\n            cwd = os.getcwd()\n            path = os.path.join(cwd, 'simple')\n            shutil.rmtree(path, ignore_errors=True)\n            inputs_dict = {\n                \"batch_size_ph\": batch_size_ph,\n                \"features_data_ph\": features_data_ph,\n                \"labels_data_ph\": labels_data_ph\n            }\n            outputs_dict = {\n                \"logits\": logits\n            }\n            tf.saved_model.simple_save(\n                sess, path, inputs_dict, outputs_dict\n            )\n            print('Ok')\n    # Restoring\n    graph2 = tf.Graph()\n    with graph2.as_default():\n        with tf.Session(graph=graph2) as sess:\n            # Restore saved values\n            print('\\nRestoring...')\n            tf.saved_model.loader.load(\n                sess,\n                [tag_constants.SERVING],\n                path\n            )\n            print('Ok')\n            # Get restored placeholders\n            labels_data_ph = graph2.get_tensor_by_name('labels_data_ph:0')\n            features_data_ph = graph2.get_tensor_by_name('features_data_ph:0')\n            batch_size_ph = graph2.get_tensor_by_name('batch_size_ph:0')\n            # Get restored model output\n            restored_logits = graph2.get_tensor_by_name('dense/BiasAdd:0')\n            # Get dataset initializing operation\n            dataset_init_op = graph2.get_operation_by_name('dataset_init')\n\n            # Initialize restored dataset\n            sess.run(\n                dataset_init_op,\n                feed_dict={\n                    features_data_ph: features,\n                    labels_data_ph: labels,\n                    batch_size_ph: 32\n                }\n\n            )\n            # Compute inference for both batches in dataset\n            restored_values = []\n            for i in range(2):\n                restored_values.append(sess.run(restored_logits))\n                print('Restored values: ', restored_values[i][0])\n\n    # Check if original inference and restored inference are equal\n    valid = all((v == rv).all() for v, rv in zip(values, restored_values))\n    print('\\nInferences match: ', valid)\n</code></pre>\n\n<p>This will print:</p>\n\n<pre><code>$ python3 save_and_restore.py\n\nEpoch 0, batch 0 | Sample value: [-0.13851789 -0.3087595   0.12804556  0.20013677 -0.08229901]\nEpoch 0, batch 1 | Sample value: [-0.00555491 -0.04339041 -0.05111827 -0.2480045  -0.00107776]\nEpoch 1, batch 0 | Sample value: [-0.19321944 -0.2104792  -0.00602257  0.07465433  0.11674127]\nEpoch 1, batch 1 | Sample value: [-0.05275984  0.05981954 -0.15913513 -0.3244143   0.10673307]\nEpoch 2, batch 0 | Final inference | Sample value: [-0.26331693 -0.13013336 -0.12553    -0.04276478  0.2933622 ]\nEpoch 2, batch 1 | Final inference | Sample value: [-0.07730117  0.11119192 -0.20817074 -0.35660955  0.16990358]\n\nSaving...\nINFO:tensorflow:Assets added to graph.\nINFO:tensorflow:No assets to write.\nINFO:tensorflow:SavedModel written to: b'/some/path/simple/saved_model.pb'\nOk\n\nRestoring...\nINFO:tensorflow:Restoring parameters from b'/some/path/simple/variables/variables'\nOk\nRestored values:  [-0.26331693 -0.13013336 -0.12553    -0.04276478  0.2933622 ]\nRestored values:  [-0.07730117  0.11119192 -0.20817074 -0.35660955  0.16990358]\n\nInferences match:  True\n</code></pre>\n"}, {"title": "TensorFlow not found using pip", "question_body": "<p>I'm trying to intstall TensorFlow using pip:</p>\n\n <pre class=\"lang-none prettyprint-override\"><code>$ pip install tensorflow --user\nCollecting tensorflow\nCould not find a version that satisfies the requirement tensorflow (from versions: )\nNo matching distribution found for tensorflow\n</code></pre>\n\n<p>What am I doing wrong? So far I've used Python and pip with no issues.</p>\n", "link": "https://stackoverflow.com/questions/38896424/tensorflow-not-found-using-pip", "question_id": 38896424, "accepted_answer_id": null}, {"title": "What is logits, softmax and softmax_cross_entropy_with_logits?", "question_body": "<p>I was going through the tensorflow API docs <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/nn.html#softmax\" rel=\"noreferrer\">here</a>. In the tensorflow documentation, they used a keyword called <code>logits</code>. What is it? In a lot of methods in the API docs it is written like</p>\n\n<pre><code>tf.nn.softmax(logits, name=None)\n</code></pre>\n\n<p>If what is written is those <code>logits</code> are only <code>Tensors</code>, why keeping a different name like <code>logits</code>? </p>\n\n<p>Another thing is that there are two methods I could not differentiate. They were</p>\n\n<pre><code>tf.nn.softmax(logits, name=None)\ntf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)\n</code></pre>\n\n<p>What are the differences between them? The docs are not clear to me. I know what <code>tf.nn.softmax</code> does. But not the other. An example will be really helpful.</p>\n", "link": "https://stackoverflow.com/questions/34240703/what-is-logits-softmax-and-softmax-cross-entropy-with-logits", "question_id": 34240703, "accepted_answer_id": 34243720, "answer_body": "<p>Logits simply means that the function operates on the unscaled output of earlier layers and that the relative scale to understand the units is linear.  It means, in particular, the sum of the inputs may not equal 1, that the values are <em>not</em> probabilities (you might have an input of 5).</p>\n\n<p><code>tf.nn.softmax</code> produces just the result of applying the <a href=\"https://en.wikipedia.org/wiki/Softmax_function\" rel=\"noreferrer\">softmax function</a> to an input tensor.  The softmax \"squishes\" the inputs so that <code>sum(input) = 1</code>:  it's a way of normalizing.  The shape of output of a softmax is the same as the input: it just normalizes the values.  The outputs of softmax <em>can</em> be interpreted as probabilities.</p>\n\n<pre><code>a = tf.constant(np.array([[.1, .3, .5, .9]]))\nprint s.run(tf.nn.softmax(a))\n[[ 0.16838508  0.205666    0.25120102  0.37474789]]\n</code></pre>\n\n<p>In contrast, <code>tf.nn.softmax_cross_entropy_with_logits</code> computes the cross entropy of the result after applying the softmax function (but it does it all together in a more mathematically careful way).  It's similar to the result of:</p>\n\n<pre><code>sm = tf.nn.softmax(x)\nce = cross_entropy(sm)\n</code></pre>\n\n<p>The cross entropy is a summary metric: it sums across the elements.  The output of <code>tf.nn.softmax_cross_entropy_with_logits</code> on a shape <code>[2,5]</code> tensor is of shape <code>[2,1]</code> (the first dimension is treated as the batch).</p>\n\n<p>If you want to do optimization to minimize the cross entropy <strong>AND</strong> you're softmaxing after your last layer, you should use <code>tf.nn.softmax_cross_entropy_with_logits</code> instead of doing it yourself, because it covers numerically unstable corner cases in the mathematically right way.  Otherwise, you'll end up hacking it by adding little epsilons here and there.</p>\n\n<p><strong>Edited 2016-02-07:</strong> \nIf you have single-class labels, where an object can only belong to one class, you might now  consider using <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> so that you don't have to convert your labels to a dense one-hot array.  This function was added after release 0.6.0.</p>\n"}, {"title": "How to compile Tensorflow with SSE4.2 and AVX instructions?", "question_body": "<p>This is the message received from running a script to check if Tensorflow is working:</p>\n\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally\nW tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n</code></pre>\n\n<p>I noticed that it has mentioned SSE4.2 and AVX,</p>\n\n<ol>\n<li>What are SSE4.2 and AVX?</li>\n<li>How do these SSE4.2 and AVX improve CPU computations for Tensorflow tasks.</li>\n<li>How to make Tensorflow compile using the two libraries?</li>\n</ol>\n", "link": "https://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions", "question_id": 41293077, "accepted_answer_id": null}, {"title": "What is the difference between &#39;SAME&#39; and &#39;VALID&#39; padding in tf.nn.max_pool of tensorflow?", "question_body": "<p>What is the difference between 'SAME' and 'VALID' padding in <code>tf.nn.max_pool</code> of <code>tensorflow</code>?</p>\n\n<p>In my opinion, 'VALID' means there will be no zero padding outside the edges when we do max pool. </p>\n\n<p>According to <a href=\"https://arxiv.org/pdf/1603.07285v1.pdf\">A guide to convolution arithmetic for deep learning</a>, it says that there will be no padding in pool operator, i.e. just use 'VALID' of <code>tensorflow</code>.\nBut what is 'SAME' padding of max pool in <code>tensorflow</code>?</p>\n", "link": "https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t", "question_id": 37674306, "accepted_answer_id": 37675359, "answer_body": "<p>I'll give an example to make it clearer:</p>\n\n<ul>\n<li><code>x</code>: input image of shape [2, 3], 1 channel</li>\n<li><code>valid_pad</code>: max pool with 2x2 kernel, stride 2 and VALID padding.</li>\n<li><code>same_pad</code>: max pool with 2x2 kernel, stride 2 and SAME padding (this is the <strong>classic</strong> way to go)</li>\n</ul>\n\n<p>The output shapes are:</p>\n\n<ul>\n<li><code>valid_pad</code>: here, no padding so the output shape is [1, 1]</li>\n<li><code>same_pad</code>: here, we pad the image to the shape [2, 4] (with <code>-inf</code> and then apply max pool), so the output shape is [1, 2]</li>\n</ul>\n\n<hr>\n\n<pre class=\"lang-py prettyprint-override\"><code>x = tf.constant([[1., 2., 3.],\n                 [4., 5., 6.]])\n\nx = tf.reshape(x, [1, 2, 3, 1])  # give a shape accepted by tf.nn.max_pool\n\nvalid_pad = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\nsame_pad = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n\nvalid_pad.get_shape() == [1, 1, 1, 1]  # valid_pad is [5.]\nsame_pad.get_shape() == [1, 1, 2, 1]   # same_pad is  [5., 6.]\n</code></pre>\n\n<hr>\n"}, {"title": "What&#39;s the difference between tf.placeholder and tf.Variable?", "question_body": "<p>I'm a newbie to TensorFlow. I'm confused about the difference between <code>tf.placeholder</code> and <code>tf.Variable</code>. In my view, <code>tf.placeholder</code> is used for input data, and <code>tf.Variable</code> is used to store the state of data. This is all what I know. </p>\n\n<p>Could someone explain to me more in detail about their differences? In particular, when to use <code>tf.Variable</code> and when to use <code>tf.placeholder</code>? </p>\n", "link": "https://stackoverflow.com/questions/36693740/whats-the-difference-between-tf-placeholder-and-tf-variable", "question_id": 36693740, "accepted_answer_id": null}, {"title": "What&#39;s the difference of name scope and a variable scope in tensorflow?", "question_body": "<p>What's the differences between these functions?</p>\n\n<blockquote>\n  <p><code>tf.variable_op_scope(values, name, default_name, initializer=None)</code></p>\n  \n  <p>Returns a context manager for defining an op that creates variables.\n  This context manager validates that the given values are from the same graph, ensures that that graph is the default graph, and pushes a name scope and a variable scope.</p>\n</blockquote>\n\n<hr>\n\n<blockquote>\n  <p><code>tf.op_scope(values, name, default_name=None)</code></p>\n  \n  <p>Returns a context manager for use when defining a Python op.\n  This context manager validates that the given values are from the same graph, ensures that that graph is the default graph, and pushes a name scope.            </p>\n</blockquote>\n\n<hr>\n\n<blockquote>\n  <p><code>tf.name_scope(name)</code></p>\n  \n  <p>Wrapper for <code>Graph.name_scope()</code> using the default graph.\n  See <code>Graph.name_scope()</code> for more details.</p>\n</blockquote>\n\n<hr>\n\n<blockquote>\n  <p><code>tf.variable_scope(name_or_scope, reuse=None, initializer=None)</code></p>\n  \n  <p>Returns a context for variable scope.\n  Variable scope allows to create new variables and to share already created ones while providing checks to not create or share by accident. For details, see the Variable Scope How To, here we present only a few basic examples.  </p>\n</blockquote>\n", "link": "https://stackoverflow.com/questions/35919020/whats-the-difference-of-name-scope-and-a-variable-scope-in-tensorflow", "question_id": 35919020, "accepted_answer_id": null}, {"title": "How to prevent tensorflow from allocating the totality of a GPU memory?", "question_body": "<p>I work in an environment in which computational resources are shared, i.e., we have a few server machines equipped with a few Nvidia Titan X GPUs each.</p>\n\n<p>For small to moderate size models, the 12GB of the Titan X are usually enough for 2-3 people to run training concurrently on the same GPU. If the models are small enough that a single model does not take full advantage of all the computational units of the Titan X, this can actually result in a speedup compared with running one training process after the other. Even in cases where the concurrent access to the GPU does slow down the individual training time, it is still nice to have the flexibility of having several users running things on the GPUs at once.</p>\n\n<p>The problem with TensorFlow is that, by default, it allocates the full amount of available memory on the GPU when it is launched. Even for a small 2-layer Neural Network, I see that the 12 GB of the Titan X are used up.</p>\n\n<p>Is there a way to make TensorFlow only allocate, say, 4GB of GPU memory, if one knows that that amount is enough for a given model?</p>\n", "link": "https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory", "question_id": 34199233, "accepted_answer_id": 34200194, "answer_body": "<p>You can set the fraction of GPU memory to be allocated when you construct a <a href=\"https://www.tensorflow.org/api_docs/python/tf/Session\" rel=\"noreferrer\"><code>tf.Session</code></a> by passing a <a href=\"https://github.com/tensorflow/tensorflow/blob/08ed32dbb9e8f67eec9efce3807b5bdb3933eb2f/tensorflow/core/protobuf/config.proto\" rel=\"noreferrer\"><code>tf.GPUOptions</code></a> as part of the optional <code>config</code> argument:</p>\n\n<pre><code># Assume that you have 12GB of GPU memory and want to allocate ~4GB:\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n</code></pre>\n\n<p>The <code>per_process_gpu_memory_fraction</code> acts as a hard upper bound on the amount of GPU memory that will be used by the process on each GPU on the same machine. Currently, this fraction is applied uniformly to all of the GPUs on the same machine; there is no way to set this on a per-GPU basis.</p>\n"}, {"title": "How to print the value of a Tensor object in TensorFlow?", "question_body": "<p>I have been using the introductory example of matrix multiplication in TensorFlow.</p>\n\n<pre><code>matrix1 = tf.constant([[3., 3.]])\nmatrix2 = tf.constant([[2.],[2.]])\nproduct = tf.matmul(matrix1, matrix2)\n</code></pre>\n\n<p>When I print the product, it is displaying it as a <code>Tensor</code> object:</p>\n\n<pre><code>&lt;tensorflow.python.framework.ops.Tensor object at 0x10470fcd0&gt;\n</code></pre>\n\n<p>But how do I know the value of <code>product</code>?</p>\n\n<p>The following doesn't help:</p>\n\n<pre><code>print product\nTensor(\"MatMul:0\", shape=TensorShape([Dimension(1), Dimension(1)]), dtype=float32)\n</code></pre>\n\n<p>I know that graphs run on <code>Sessions</code>, but isn't there any way I can check the output of a <code>Tensor</code> object without running the graph in a <code>session</code>?</p>\n", "link": "https://stackoverflow.com/questions/33633370/how-to-print-the-value-of-a-tensor-object-in-tensorflow", "question_id": 33633370, "accepted_answer_id": 33633839, "answer_body": "<p>The easiest<sup>[A]</sup> way to evaluate the actual value of a <code>Tensor</code> object is to pass it to the <code>Session.run()</code> method, or call <code>Tensor.eval()</code> when you have a default session (i.e. in a <code>with tf.Session():</code> block, or see below). In general<sup>[B]</sup>, you cannot print the value of a tensor without running some code in a session.</p>\n\n<p>If you are experimenting with the programming model, and want an easy way to evaluate tensors, the <a href=\"https://www.tensorflow.org/api_docs/python/tf/InteractiveSession\" rel=\"nofollow noreferrer\"><code>tf.InteractiveSession</code></a> lets you open a session at the start of your program, and then use that session for all <code>Tensor.eval()</code> (and <code>Operation.run()</code>) calls. This can be easier in an interactive setting, such as the shell or an IPython notebook, when it's tedious to pass around a <code>Session</code> object everywhere. For example, the following works in a Jupyter notebook:</p>\n\n<pre><code>with tf.Session() as sess:  print(product.eval()) \n</code></pre>\n\n<p>This might seem silly for such a small expression, but one of the key ideas in Tensorflow is <em>deferred execution</em>: it's very cheap to build a large and complex expression, and when you want to evaluate it, the back-end (to which you connect with a <code>Session</code>) is able to schedule its execution more efficiently (e.g. executing independent parts in parallel and using GPUs).</p>\n\n<hr>\n\n<p>[A]: To print the value of a tensor without returning it to your Python program, you can use the <a href=\"https://www.tensorflow.org/api_docs/python/tf/Print\" rel=\"nofollow noreferrer\"><code>tf.Print()</code></a> operator, as <a href=\"https://stackoverflow.com/a/36296783/3574081\">Andrzej suggests in another answer</a>. Note that you still need to run part of the graph to see the output of this op, which is printed to standard output. If you're running distributed TensorFlow, <code>tf.Print()</code> will print its output to the standard output of the task where that op runs. This means that if you use <a href=\"https://colab.research.google.com\" rel=\"nofollow noreferrer\">https://colab.research.google.com</a> for example, or any other Jupyter Notebook, then you will <em>not see</em> the output of <a href=\"https://www.tensorflow.org/api_docs/python/tf/Print\" rel=\"nofollow noreferrer\"><code>tf.Print()</code></a> in the notebook; in that case refer to <a href=\"https://stackoverflow.com/questions/49969193/no-result-of-tf-print-in-kerass-model-fit/49969365#49969365\">this answer</a> on how to get it to print still.</p>\n\n<p>[B]: You <em>might</em> be able to use the experimental <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/util/constant_value\" rel=\"nofollow noreferrer\"><code>tf.contrib.util.constant_value()</code></a> function to get the value of a constant tensor, but it isn't intended for general use, and it isn't defined for many operators.</p>\n"}, {"title": "How to tell if tensorflow is using gpu acceleration from inside python shell?", "question_body": "<p>I have installed tensorflow in my ubuntu 16.04 using the second answer <a href=\"https://devtalk.nvidia.com/default/topic/936429/-solved-tensorflow-with-gpu-in-anaconda-env-ubuntu-16-04-cuda-7-5-cudnn-/\" rel=\"noreferrer\">here</a> with ubuntu's builtin apt cuda installation.</p>\n\n<p>Now my question is how can I test if tensorflow is really using gpu? I have a gtx 960m gpu. When I <code>import tensorflow</code> this is the output</p>\n\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n</code></pre>\n\n<p>Is this output enough to check if tensorflow is using gpu ? </p>\n", "link": "https://stackoverflow.com/questions/38009682/how-to-tell-if-tensorflow-is-using-gpu-acceleration-from-inside-python-shell", "question_id": 38009682, "accepted_answer_id": 38019608, "answer_body": "<p>No, I don't think \"open CUDA library\" is enough to tell, because different nodes of the graph may be on different devices.</p>\n\n<p>To find out which device is used, you can enable log device placement like this:</p>\n\n<pre><code>sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n</code></pre>\n\n<p>Check your console for this type of output.</p>\n\n<p><a href=\"https://i.stack.imgur.com/RtRiB.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/RtRiB.png\" alt=\"\"></a></p>\n"}, {"title": "In TensorFlow, what is the difference between Session.run() and Tensor.eval()?", "question_body": "<p>TensorFlow has two ways to evaluate part of graph: <code>Session.run</code> on a list of variables and <code>Tensor.eval</code>.  Is there a difference between these two?</p>\n", "link": "https://stackoverflow.com/questions/33610685/in-tensorflow-what-is-the-difference-between-session-run-and-tensor-eval", "question_id": 33610685, "accepted_answer_id": 33610914, "answer_body": "<p>If you have a <code>Tensor</code> t, calling <a href=\"https://www.tensorflow.org/api_docs/python/tf/Tensor#eval\" rel=\"noreferrer\"><code>t.eval()</code></a> is equivalent to calling <code>tf.get_default_session().run(t)</code>.</p>\n\n<p>You can make a session the default as follows:</p>\n\n<pre><code>t = tf.constant(42.0)\nsess = tf.Session()\nwith sess.as_default():   # or `with sess:` to close on exit\n    assert sess is tf.get_default_session()\n    assert t.eval() == sess.run(t)\n</code></pre>\n\n<p>The most important difference is that you can use <code>sess.run()</code> to fetch the values of many tensors in the same step:</p>\n\n<pre><code>t = tf.constant(42.0)\nu = tf.constant(37.0)\ntu = tf.mul(t, u)\nut = tf.mul(u, t)\nwith sess.as_default():\n   tu.eval()  # runs one step\n   ut.eval()  # runs one step\n   sess.run([tu, ut])  # evaluates both tensors in a single step\n</code></pre>\n\n<p>Note that each call to <code>eval</code> and <code>run</code> will execute the whole graph from scratch. To cache the result of a computation, assign it to a <a href=\"https://www.tensorflow.org/how_tos/variables/\" rel=\"noreferrer\"><code>tf.Variable</code></a>.</p>\n"}, {"title": "How to find which version of TensorFlow is installed in my system?", "question_body": "<p>The title says it all. I'm using Ubuntu 16.04 Long Term Support.  </p>\n", "link": "https://stackoverflow.com/questions/38549253/how-to-find-which-version-of-tensorflow-is-installed-in-my-system", "question_id": 38549253, "accepted_answer_id": 38549357, "answer_body": "<p>This depends on how you installed TensorFlow. I am going to use the same headings used by <a href=\"https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#download-and-setup\" rel=\"noreferrer\">TensorFlow's installation instructions</a> to structure this answer.</p>\n\n<hr>\n\n<h2>Pip installation</h2>\n\n<p>Run:</p>\n\n<pre><code>python -c 'import tensorflow as tf; print(tf.__version__)'  # for Python 2\npython3 -c 'import tensorflow as tf; print(tf.__version__)'  # for Python 3\n</code></pre>\n\n<p>Note that <code>python</code> is symlinked to <code>/usr/bin/python3</code> in some Linux distributions, so use <code>python</code> instead of <code>python3</code> in these cases.</p>\n\n<p><code>pip list | grep tensorflow</code> for Python 2 or <code>pip3 list | grep tensorflow</code> for Python 3 will also show the version of Tensorflow installed. </p>\n\n<hr>\n\n<h2>Virtualenv installation</h2>\n\n<p>Run:</p>\n\n<pre><code>python -c 'import tensorflow as tf; print(tf.__version__)'  # for both Python 2 and Python 3\n</code></pre>\n\n<p><code>pip list | grep tensorflow</code> will also show the version of Tensorflow installed. </p>\n\n<p>For example, I have installed TensorFlow 0.9.0 in a <code>virtualenv</code> for Python 3. So, I get:</p>\n\n<pre><code>$ python -c 'import tensorflow as tf; print(tf.__version__)'\n0.9.0\n\n$ pip list | grep tensorflow\ntensorflow (0.9.0)\n</code></pre>\n"}, {"title": "What is the meaning of the word logits in TensorFlow?", "question_body": "<p>In the following TensorFlow function, we must feed the activation of artificial neurons in the final layer. That I understand. But I don't understand why it is called logits? Isn't that a mathematical function? </p>\n\n<pre><code>loss_function = tf.nn.softmax_cross_entropy_with_logits(\n     logits = last_layer,\n     labels = target_output\n)\n</code></pre>\n", "link": "https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow", "question_id": 41455101, "accepted_answer_id": null}, {"title": "What does tf.nn.embedding_lookup function do?", "question_body": "<pre><code>tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None)\n</code></pre>\n\n<p>I cannot understand the duty of this function. Is it like a lookup table? Which means to return the parameters corresponding to each id (in ids)?</p>\n\n<p>For instance, in the <code>skip-gram</code> model if we use <code>tf.nn.embedding_lookup(embeddings, train_inputs)</code>, then for each <code>train_input</code> it finds the correspond embedding?</p>\n", "link": "https://stackoverflow.com/questions/34870614/what-does-tf-nn-embedding-lookup-function-do", "question_id": 34870614, "accepted_answer_id": 34877590, "answer_body": "<p><code>embedding_lookup</code> function retrieves rows of the <code>params</code> tensor. The behavior is similar to using indexing with arrays in numpy. E.g.</p>\n\n<pre><code>matrix = np.random.random([1024, 64])  # 64-dimensional embeddings\nids = np.array([0, 5, 17, 33])\nprint matrix[ids]  # prints a matrix of shape [4, 64] \n</code></pre>\n\n<p><code>params</code> argument can be also a list of tensors in which case the <code>ids</code> will be distributed among the tensors. For example, given a list of 3 tensors <code>[2, 64]</code>, the default behavior is that they will represent <code>ids</code>: <code>[0, 3]</code>, <code>[1, 4]</code>, <code>[2, 5]</code>. </p>\n\n<p><code>partition_strategy</code> controls the way how the <code>ids</code> are distributed among the list. The partitioning is useful for larger scale problems when the matrix might be too large to keep in one piece.</p>\n"}, {"title": "How to build and use Google TensorFlow C++ api", "question_body": "<p>I'm really eager to start using Google's new Tensorflow library in C++. The website and docs are just really unclear in terms of how to build the project's C++ API and I don't know where to start. </p>\n\n<p>Can someone with more experience help by discovering and sharing a guide to using tensorflow's C++ API?  </p>\n", "link": "https://stackoverflow.com/questions/33620794/how-to-build-and-use-google-tensorflow-c-api", "question_id": 33620794, "accepted_answer_id": 33622489, "answer_body": "<p>To get started, you should download the source code from Github, by <a href=\"http://tensorflow.org/get_started/os_setup.md#installing_from_sources\">following the instructions here</a> (you'll need <a href=\"http://bazel.io\">Bazel</a> and a recent version of GCC).</p>\n\n<p>The C++ API (and the backend of the system) is in <code>tensorflow/core</code>. Right now, only the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/session.h\">C++ Session interface</a>, and the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/tensor_c_api.h\">C API</a> are being supported. You can use either of these to execute TensorFlow graphs that have been built using the Python API and serialized to a <code>GraphDef</code> protocol buffer. There is also an experimental feature for building graphs in C++, but this is currently not quite as full-featured as the Python API (e.g. no support for auto-differentiation at present). You can see an example program that <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/tutorials/example_trainer.cc\">builds a small graph in C++ here</a>.</p>\n\n<p>The second part of the C++ API is the API for adding a new <code>OpKernel</code>, which is the class containing implementations of numerical kernels for CPU and GPU. There are numerous examples of how to build these in <code>tensorflow/core/kernels</code>, as well as a <a href=\"http://tensorflow.org/how_tos/adding_an_op/index.md\">tutorial for adding a new op in C++</a>.</p>\n"}, {"title": "How can I convert a tensor into a numpy array in TensorFlow?", "question_body": "<p>How to convert a tensor into a numpy array when using Tensorflow with Python bindings?</p>\n", "link": "https://stackoverflow.com/questions/34097281/how-can-i-convert-a-tensor-into-a-numpy-array-in-tensorflow", "question_id": 34097281, "accepted_answer_id": 37912938, "answer_body": "<p>Any tensor returned by <code>Session.run</code> or <code>eval</code> is a NumPy array.</p>\n\n<pre><code>&gt;&gt;&gt; print(type(tf.Session().run(tf.constant([1,2,3]))))\n&lt;class 'numpy.ndarray'&gt;\n</code></pre>\n\n<p>Or:</p>\n\n<pre><code>&gt;&gt;&gt; sess = tf.InteractiveSession()\n&gt;&gt;&gt; print(type(tf.constant([1,2,3]).eval()))\n&lt;class 'numpy.ndarray'&gt;\n</code></pre>\n\n<p>Or, equivalently:</p>\n\n<pre><code>&gt;&gt;&gt; sess = tf.Session()\n&gt;&gt;&gt; with sess.as_default():\n&gt;&gt;&gt;    print(type(tf.constant([1,2,3]).eval()))\n&lt;class 'numpy.ndarray'&gt;\n</code></pre>\n\n<p><strong>EDIT:</strong> Not <em>any</em> tensor returned by <code>Session.run</code> or <code>eval()</code> is a NumPy array. Sparse Tensors for example are returned as SparseTensorValue:</p>\n\n<pre><code>&gt;&gt;&gt; print(type(tf.Session().run(tf.SparseTensor([[0, 0]],[1],[1,2]))))\n&lt;class 'tensorflow.python.framework.sparse_tensor.SparseTensorValue'&gt;\n</code></pre>\n"}, {"title": "How does tf.app.run() work?", "question_body": "<p>How does <code>tf.app.run()</code> work in Tensorflow translate demo? </p>\n\n<p>In <code>tensorflow/models/rnn/translate/translate.py</code>, there is a call to <code>tf.app.run()</code>. How is it being handled?</p>\n\n<pre><code>if __name__ == \"__main__\":\n    tf.app.run() \n</code></pre>\n", "link": "https://stackoverflow.com/questions/33703624/how-does-tf-app-run-work", "question_id": 33703624, "accepted_answer_id": null}, {"title": "TensorFlow, why was python the chosen language?", "question_body": "<p>I recently started studying deep learning and other ML techniques, and I started searching for frameworks that simplify the process of build a net and training it, then I found TensorFlow, having little experience in the field, for me, it seems that speed is a big factor for making a big ML system even more if working with deep learning, so why python was chosen by Google to make TensorFlow? Wouldn't it be better to make it over an language that can be compiled and not interpreted?</p>\n\n<p>What are the advantages of using Python over a language like C++ for machine learning?</p>\n", "link": "https://stackoverflow.com/questions/35677724/tensorflow-why-was-python-the-chosen-language", "question_id": 35677724, "accepted_answer_id": 35678837, "answer_body": "<p>The most important thing to realize about TensorFlow is that, for the most part, <em>the core is not written in Python</em>:  It's written in a combination of highly-optimized C++ and CUDA (Nvidia's language for programming GPUs).  Much of that happens, in turn, by using <a href=\"http://eigen.tuxfamily.org/index.php?title=Main_Page\" rel=\"noreferrer\">Eigen</a> (a high-performance C++ and CUDA numerical library) and <a href=\"https://developer.nvidia.com/cudnn\" rel=\"noreferrer\">NVidia's cuDNN</a> (a very optimized DNN library for <a href=\"https://developer.nvidia.com/cuda-gpus\" rel=\"noreferrer\">NVidia GPUs</a>, for functions such as <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" rel=\"noreferrer\">convolutions</a>).</p>\n\n<p>The model for TensorFlow is that the programmer uses \"some language\" (most likely Python!) to express the model.  This model, written in the TensorFlow constructs such as:</p>\n\n<pre><code>h1 = tf.nn.relu(tf.matmul(l1, W1) + b1)\nh2 = ...\n</code></pre>\n\n<p>is not actually executed when the Python is run.  Instead, what's actually created is a <a href=\"https://www.tensorflow.org/get_started/graph_viz\" rel=\"noreferrer\">dataflow graph</a> that says to take particular inputs, apply particular operations, supply the results as the inputs to other operations, and so on.  <em>This model is executed by fast C++ code, and for the most part, the data going between operations is never copied back to the Python code</em>.</p>\n\n<p>Then the programmer \"drives\" the execution of this model by pulling on nodes -- for training, usually in Python, and for serving, sometimes in Python and sometimes in raw C++:</p>\n\n<pre><code>sess.run(eval_results)\n</code></pre>\n\n<p>This one Python (or C++ function call) uses either an in-process call to C++ or an <a href=\"https://en.wikipedia.org/wiki/Remote_procedure_call\" rel=\"noreferrer\">RPC</a> for the distributed version to call into the C++ TensorFlow server to tell it to execute, and then copies back the results.</p>\n\n<p><strong>So, with that said, let's re-phrase the question:  Why did TensorFlow choose  Python as the first well-supported language for expressing and controlling the training of models?</strong></p>\n\n<p>The answer to that is simple:  Python is probably <em>the</em> most comfortable language for a large range of data scientists and machine learning experts that's also that easy to integrate and have control a C++ backend, while also being general, widely-used both inside and outside of Google, and open source.  Given that with the basic model of TensorFlow, the performance of Python isn't that important, it was a natural fit.  It's also a huge plus that <a href=\"http://www.numpy.org/\" rel=\"noreferrer\">NumPy</a> makes it easy to do pre-processing in Python -- also with high performance -- before feeding it in to TensorFlow for the truly CPU-heavy things.</p>\n\n<p>There's also a bunch of complexity in expressing the model that isn't used when executing it -- shape inference (e.g., if you do matmul(A, B), what is the shape of the resulting data?) and automatic <a href=\"https://en.wikipedia.org/wiki/Gradient\" rel=\"noreferrer\">gradient</a> computation.  It turns out to have been nice to be able to express those in Python, though I think in the long term they'll probably move to the C++ backend to make adding other languages easier.</p>\n\n<p>(The hope, of course, is to support other languages in the future for creating and expressing models.  It's already quite straightforward to run inference using several other languages -- C++ works now, someone from Facebook contributed <a href=\"https://golang.org/\" rel=\"noreferrer\">Go</a> bindings that we're reviewing now, etc.)</p>\n"}, {"title": "How to get current available GPUs in tensorflow?", "question_body": "<p>I have a plan to use distributed TensorFlow, and I saw TensorFlow can use GPUs for training and testing. In a cluster environment, each machine could have 0 or 1 or more GPUs, and I want to run my TensorFlow graph into GPUs on as many machines as possible.</p>\n\n<p>I found that when running <code>tf.Session()</code> TensorFlow gives information about GPU in the log messages like below:</p>\n\n<pre><code>I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\n</code></pre>\n\n<p>My question is how do I get information about current available GPU from TensorFlow? I can get loaded GPU information from the log, but I want to do it in a more sophisticated, programmatic way.\nI also could restrict GPUs intentionally using the CUDA_VISIBLE_DEVICES environment variable, so I don't want to know a way of getting GPU information from OS kernel.</p>\n\n<p>In short, I want a function like <code>tf.get_available_gpus()</code> that will return <code>['/gpu:0', '/gpu:1']</code> if there are two GPUs available in the machine. How can I implement this?</p>\n", "link": "https://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow", "question_id": 38559755, "accepted_answer_id": 38580201, "answer_body": "<p>There is an undocumented method called <a href=\"https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/python/client/device_lib.py#L27\" rel=\"noreferrer\"><code>device_lib.list_local_devices()</code></a> that enables you to list the devices available in the local process. (<strong>N.B.</strong> As an undocumented method, this is subject to backwards incompatible changes.) The function returns a list of <a href=\"https://github.com/tensorflow/tensorflow/blob/8a4f6abb395b3f1bca732797068021c786c1ec76/tensorflow/core/framework/device_attributes.proto\" rel=\"noreferrer\"><code>DeviceAttributes</code> protocol buffer</a> objects. You can extract a list of string device names for the GPU devices as follows:</p>\n\n<pre><code>from tensorflow.python.client import device_lib\n\ndef get_available_gpus():\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n</code></pre>\n\n<p>Note that (at least up to TensorFlow 1.4), calling <code>device_lib.list_local_devices()</code> will run some initialization code that, by default, will allocate all of the GPU memory on all of the devices (<a href=\"https://github.com/tensorflow/tensorflow/issues/9374\" rel=\"noreferrer\">GitHub issue</a>). To avoid this, first create a session with an explicitly small <code>per_process_gpu_fraction</code>, or <code>allow_growth=True</code>, to prevent all of the memory being allocated. See <a href=\"https://stackoverflow.com/q/34199233/3574081\">this question</a> for more details.</p>\n"}, {"title": "Disable Tensorflow debugging information", "question_body": "<p>By debugging information I mean what TensorFlow shows in my terminal about loaded libraries and found devices etc. not Python errors.</p>\n\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Graphics Device\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.0885\npciBusID 0000:04:00.0\nTotal memory: 12.00GiB\nFree memory: 11.83GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Graphics Device, pci bus id: 0000:04:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\n...\n</code></pre>\n", "link": "https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information", "question_id": 35911252, "accepted_answer_id": null}, {"title": "What does tf.nn.conv2d do in tensorflow?", "question_body": "<p>I was looking at the docs of tensorflow about <code>tf.nn.conv2d</code> <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/conv2d\" rel=\"noreferrer\">here</a>. But I can't understand what it does or what it is trying to achieve. It says on the docs, </p>\n\n<blockquote>\n  <p>#1 : Flattens the filter to a 2-D matrix with shape </p>\n  \n  <p><code>[filter_height * filter_width * in_channels, output_channels]</code>.</p>\n</blockquote>\n\n<p>Now what does that do? Is that element-wise multiplication or just plain matrix multiplication? I also could not understand the other two points mentioned in the docs. I have written them below :</p>\n\n<blockquote>\n  <p># 2: Extracts image patches from the the input tensor to form a virtual tensor of shape </p>\n  \n  <p><code>[batch, out_height, out_width, filter_height * filter_width * in_channels]</code>.</p>\n  \n  <p># 3: For each patch, right-multiplies the filter matrix and the image patch vector.</p>\n</blockquote>\n\n<p>It would be really helpful if anyone could give an example, a piece of code (extremely helpful) maybe and explain what is going on there and why the operation is like this.</p>\n\n<p>I've tried coding a small portion and printing out the shape of the operation. Still, I can't understand. </p>\n\n<p>I tried something like this:</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>op = tf.shape(tf.nn.conv2d(tf.random_normal([1,10,10,10]), \n              tf.random_normal([2,10,10,10]), \n              strides=[1, 2, 2, 1], padding='SAME'))\n\nwith tf.Session() as sess:\n    result = sess.run(op)\n    print(result)\n</code></pre>\n\n<p>I understand bits and pieces of convolutional neural networks. I studied them <a href=\"http://cs231n.github.io/convolutional-networks/\" rel=\"noreferrer\">here</a>. But the implementation on tensorflow is not what I expected. So it raised the question.</p>\n\n<p><strong>EDIT</strong>:\nSo, I implemented a much simpler code. But I can't figure out what's going on. I mean how the results are like this. It would be extremely helpful if anyone could tell me what process yields this output.</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>input = tf.Variable(tf.random_normal([1,2,2,1]))\nfilter = tf.Variable(tf.random_normal([1,1,1,1]))\n\nop = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME')\ninit = tf.initialize_all_variables()\nwith tf.Session() as sess:\n    sess.run(init)\n\n    print(\"input\")\n    print(input.eval())\n    print(\"filter\")\n    print(filter.eval())\n    print(\"result\")\n    result = sess.run(op)\n    print(result)\n</code></pre>\n\n<p>output</p>\n\n<pre><code>input\n[[[[ 1.60314465]\n   [-0.55022103]]\n\n  [[ 0.00595062]\n   [-0.69889867]]]]\nfilter\n[[[[-0.59594476]]]]\nresult\n[[[[-0.95538563]\n   [ 0.32790133]]\n\n  [[-0.00354624]\n   [ 0.41650501]]]]\n</code></pre>\n", "link": "https://stackoverflow.com/questions/34619177/what-does-tf-nn-conv2d-do-in-tensorflow", "question_id": 34619177, "accepted_answer_id": 44103248, "answer_body": "<p>2D convolution is computed in a similar way one would calculate <a href=\"http://www.riptutorial.com/tensorflow/example/30750/math-behind-1d-convolution-with-advanced-examples-in-tf\" rel=\"noreferrer\">1D convolution</a>: you slide your kernel over the input, calculate the element-wise multiplications and sum them up. But instead of your kernel/input being an array, here they are matrices.</p>\n\n<hr>\n\n<p>In the most basic example there is no padding and stride=1. Let's assume your <code>input</code> and <code>kernel</code> are:\n<a href=\"https://i.stack.imgur.com/yTCl8.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/yTCl8.png\" alt=\"enter image description here\"></a></p>\n\n<p>When you use your kernel you will receive the following output: <a href=\"https://i.stack.imgur.com/TPhBi.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/TPhBi.png\" alt=\"enter image description here\"></a>, which is calculated in the following way:</p>\n\n<ul>\n<li>14 = 4 * 1 + 3 * 0 + 1 * 1 + 2 * 2 + 1 * 1 + 0 * 0 + 1 * 0 + 2 * 0 + 4 * 1</li>\n<li>6  = 3 * 1 + 1 * 0 + 0 * 1 + 1 * 2 + 0 * 1 + 1 * 0 + 2 * 0 + 4 * 0 + 1 * 1</li>\n<li>6  = 2 * 1 + 1 * 0 + 0 * 1 + 1 * 2 + 2 * 1 + 4 * 0 + 3 * 0 + 1 * 0 + 0 * 1</li>\n<li>12 = 1 * 1 + 0 * 0 + 1 * 1 + 2 * 2 + 4 * 1 + 1 * 0 + 1 * 0 + 0 * 0 + 2 * 1</li>\n</ul>\n\n<p>TF's <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/conv2d\" rel=\"noreferrer\">conv2d</a> function calculates convolutions in batches and uses a slightly different format. For an input it is <code>[batch, in_height, in_width, in_channels]</code> for the kernel it is <code>[filter_height, filter_width, in_channels, out_channels]</code>. So we need to provide the data in the correct format:</p>\n\n<pre><code>import tensorflow as tf\nk = tf.constant([\n    [1, 0, 1],\n    [2, 1, 0],\n    [0, 0, 1]\n], dtype=tf.float32, name='k')\ni = tf.constant([\n    [4, 3, 1, 0],\n    [2, 1, 0, 1],\n    [1, 2, 4, 1],\n    [3, 1, 0, 2]\n], dtype=tf.float32, name='i')\nkernel = tf.reshape(k, [3, 3, 1, 1], name='kernel')\nimage  = tf.reshape(i, [1, 4, 4, 1], name='image')\n</code></pre>\n\n<p>Afterwards the convolution is computed with:</p>\n\n<pre><code>res = tf.squeeze(tf.nn.conv2d(image, kernel, [1, 1, 1, 1], \"VALID\"))\n# VALID means no padding\nwith tf.Session() as sess:\n   print sess.run(res)\n</code></pre>\n\n<p>And will be equivalent to the one we calculated by hand. </p>\n\n<hr>\n\n<p>For <a href=\"http://www.riptutorial.com/tensorflow/example/30755/some-padding--strides-1\" rel=\"noreferrer\">examples with padding/strides, take a look here</a>.</p>\n"}, {"title": "Difference between Variable and get_variable in TensorFlow", "question_body": "<p>As far as I know, <code>Variable</code> is the default operation for making a variable, and <code>get_variable</code> is mainly used for weight sharing.</p>\n\n<p>On the one hand, there are some people suggesting using <code>get_variable</code> instead of the primitive <code>Variable</code> operation whenever you need a variable. On the other hand, I merely see any use of <code>get_variable</code> in TensorFlow's official documents and demos.</p>\n\n<p>Thus I want to know some rules of thumb on how to correctly use these two mechanisms. Are there any \"standard\" principles?</p>\n", "link": "https://stackoverflow.com/questions/37098546/difference-between-variable-and-get-variable-in-tensorflow", "question_id": 37098546, "accepted_answer_id": 37102908, "answer_body": "<p>I'd recommend to always use <code>tf.get_variable(...)</code> -- it will make it way easier to refactor your code if you need to share variables at any time, e.g. in a multi-gpu setting (see the multi-gpu CIFAR example). There is no downside to it. </p>\n\n<p>Pure <code>tf.Variable</code> is lower-level; at some point <code>tf.get_variable()</code> did not exist so some code still uses the low-level way.</p>\n"}, {"title": "Tensorflow Strides Argument", "question_body": "<p>I am trying to understand the <strong>strides</strong> argument in tf.nn.avg_pool, tf.nn.max_pool, tf.nn.conv2d. </p>\n\n<p>The <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/nn.html#max_pool\" rel=\"noreferrer\">documentation</a> repeatedly says </p>\n\n<blockquote>\n  <p>strides: A list of ints that has length >= 4. The stride of the sliding window for each dimension of the input tensor.</p>\n</blockquote>\n\n<p>My questions are:</p>\n\n<ol>\n<li>What do each of the 4+ integers represent?</li>\n<li>Why must they have strides[0] = strides[3] = 1 for convnets?</li>\n<li>In <a href=\"https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3%20-%20Neural%20Networks/convolutional_network.ipynb\" rel=\"noreferrer\">this example</a> we see <code>tf.reshape(_X,shape=[-1, 28, 28, 1])</code>. Why -1?</li>\n</ol>\n\n<p>Sadly the examples in the docs for reshape using -1 don't translate too well to this scenario.</p>\n", "link": "https://stackoverflow.com/questions/34642595/tensorflow-strides-argument", "question_id": 34642595, "accepted_answer_id": 34643081, "answer_body": "<p>The pooling and convolutional ops slide a \"window\" across the input tensor.  Using <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/nn.html#conv2d\"><code>tf.nn.conv2d</code></a> as an example: If the input tensor has 4 dimensions:  <code>[batch, height, width, channels]</code>, then the convolution operates on a 2D window on the <code>height, width</code> dimensions.</p>\n\n<p><code>strides</code> determines how much the window shifts by in each of the dimensions.  The typical use sets the first (the batch) and last (the depth) stride to 1.</p>\n\n<p>Let's use a very concrete example:  Running a 2-d convolution over a 32x32 greyscale input image.  I say greyscale because then the input image has depth=1, which helps keep it simple.  Let that image look like this:</p>\n\n<pre><code>00 01 02 03 04 ...\n10 11 12 13 14 ...\n20 21 22 23 24 ...\n30 31 32 33 34 ...\n...\n</code></pre>\n\n<p>Let's run a 2x2 convolution window over a single example (batch size = 1).  We'll give the convolution an output channel depth of 8.</p>\n\n<p>The input to the convolution has <code>shape=[1, 32, 32, 1]</code>.</p>\n\n<p>If you specify <code>strides=[1,1,1,1]</code> with <code>padding=SAME</code>, then the output of the filter will be [1, 32, 32, 8].</p>\n\n<p>The filter will first create an output for:</p>\n\n<pre><code>F(00 01\n  10 11)\n</code></pre>\n\n<p>And then for:</p>\n\n<pre><code>F(01 02\n  11 12)\n</code></pre>\n\n<p>and so on.  Then it will move to the second row, calculating:</p>\n\n<pre><code>F(10, 11\n  20, 21)\n</code></pre>\n\n<p>then</p>\n\n<pre><code>F(11, 12\n  21, 22)\n</code></pre>\n\n<p>If you specify a stride of [1, 2, 2, 1] it won't do overlapping windows.  It will compute:</p>\n\n<pre><code>F(00, 01\n  10, 11)\n</code></pre>\n\n<p>and then</p>\n\n<pre><code>F(02, 03\n  12, 13)\n</code></pre>\n\n<p>The stride operates similarly for the pooling operators.</p>\n\n<p><strong>Question 2:  Why strides [1, x, y, 1] for convnets</strong></p>\n\n<p>The first 1 is the batch:  You don't usually want to skip over examples in your batch, or you shouldn't have included them in the first place. :)</p>\n\n<p>The last 1 is the depth of the convolution:  You don't usually want to skip inputs, for the same reason.</p>\n\n<p>The conv2d operator is more general, so you <em>could</em> create convolutions that slide the window along other dimensions, but that's not a typical use in convnets.  The typical use is to use them spatially.</p>\n\n<p><strong>Why reshape to -1</strong>  -1 is a placeholder that says \"adjust as necessary to match the size needed for the full tensor.\"  It's a way of making the code be independent of the input batch size, so that you can change your pipeline and not have to adjust the batch size everywhere in the code.</p>\n"}, {"title": "Keras, How to get the output of each layer?", "question_body": "<p>I have trained a binary classification model with CNN, and here is my code</p>\n\n<pre><code>model = Sequential()\nmodel.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n                        border_mode='valid',\n                        input_shape=input_shape))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=pool_size))\n# (16, 16, 32)\nmodel.add(Convolution2D(nb_filters*2, kernel_size[0], kernel_size[1]))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(nb_filters*2, kernel_size[0], kernel_size[1]))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=pool_size))\n# (8, 8, 64) = (2048)\nmodel.add(Flatten())\nmodel.add(Dense(1024))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2))  # define a binary classification problem\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adadelta',\n              metrics=['accuracy'])\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          nb_epoch=nb_epoch,\n          verbose=1,\n          validation_data=(x_test, y_test))\n</code></pre>\n\n<p>And here, I wanna get the output of each layer just like TensorFlow, how can I do that?</p>\n", "link": "https://stackoverflow.com/questions/41711190/keras-how-to-get-the-output-of-each-layer", "question_id": 41711190, "accepted_answer_id": 41712013, "answer_body": "<p>You can easily get the outputs of any layer by using: <code>model.layers[index].output</code></p>\n\n<p>For all layers use this:</p>\n\n<pre><code>from keras import backend as K\n\ninp = model.input                                           # input placeholder\noutputs = [layer.output for layer in model.layers]          # all layer outputs\nfunctors = [K.function([inp, K.learning_phase()], [out]) for out in outputs]    # evaluation functions\n\n# Testing\ntest = np.random.random(input_shape)[np.newaxis,...]\nlayer_outs = [func([test, 1.]) for func in functors]\nprint layer_outs\n</code></pre>\n\n<p>Note: To simulate Dropout use <code>learning_phase</code> as <code>1.</code> in <code>layer_outs</code> otherwise use <code>0.</code></p>\n\n<p><strong>Edit:</strong> (based on comments)</p>\n\n<p><code>K.function</code> creates theano/tensorflow tensor functions which is later used to get the output from the symbolic graph given the input. </p>\n\n<p>Now <code>K.learning_phase()</code> is required as an input as many Keras layers like Dropout/Batchnomalization depend on it to change behavior during training and test time. </p>\n\n<p>So if you remove the dropout layer in your code you can simply use:</p>\n\n<pre><code>from keras import backend as K\n\ninp = model.input                                           # input placeholder\noutputs = [layer.output for layer in model.layers]          # all layer outputs\nfunctors = [K.function([inp], [out]) for out in outputs]    # evaluation functions\n\n# Testing\ntest = np.random.random(input_shape)[np.newaxis,...]\nlayer_outs = [func([test]) for func in functors]\nprint layer_outs\n</code></pre>\n\n<p><strong>Edit 2: More optimized</strong></p>\n\n<p>I just realized that the previous answer is not that optimized as for each function evaluation the data will be transferred CPU->GPU memory and also the tensor calculations needs to be done for the lower layers over-n-over. </p>\n\n<p>Instead this is a much better way as you don't need multiple functions but a single function giving you the list of all outputs:</p>\n\n<pre><code>from keras import backend as K\n\ninp = model.input                                           # input placeholder\noutputs = [layer.output for layer in model.layers]          # all layer outputs\nfunctor = K.function([inp, K.learning_phase()], outputs )   # evaluation function\n\n# Testing\ntest = np.random.random(input_shape)[np.newaxis,...]\nlayer_outs = functor([test, 1.])\nprint layer_outs\n</code></pre>\n"}, {"title": "What is the difference between steps and epochs in TensorFlow?", "question_body": "<p>In most of the models, there is a <em>steps</em> parameter indicating the <em>number of steps to run over data</em>. But yet I see in most practical usage, we also execute the fit function N <em>epochs</em>. </p>\n\n<p>What is the difference between running 1000 steps with 1 epoch and running 100 steps with 10 epoch? Which one is better in practice? Any logic changes between consecutive epochs? Data shuffling?</p>\n", "link": "https://stackoverflow.com/questions/38340311/what-is-the-difference-between-steps-and-epochs-in-tensorflow", "question_id": 38340311, "accepted_answer_id": null}, {"title": "What&#39;s the purpose of tf.app.flags in TensorFlow?", "question_body": "<p>I am reading some example codes in Tensorflow, I found following code </p>\n\n<pre><code>flags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\nflags.DEFINE_integer('max_steps', 2000, 'Number of steps to run trainer.')\nflags.DEFINE_integer('hidden1', 128, 'Number of units in hidden layer 1.')\nflags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\nflags.DEFINE_integer('batch_size', 100, 'Batch size.  '\n                 'Must divide evenly into the dataset sizes.')\nflags.DEFINE_string('train_dir', 'data', 'Directory to put the training data.')\nflags.DEFINE_boolean('fake_data', False, 'If true, uses fake data '\n                 'for unit testing.')\n</code></pre>\n\n<p>in <code>tensorflow/tensorflow/g3doc/tutorials/mnist/fully_connected_feed.py</code></p>\n\n<p>But I can't find any docs about this usage of <code>tf.app.flags</code>. </p>\n\n<p>And I found the implementation of this flags is in the \n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/default/_flags.py\"><code>tensorflow/tensorflow/python/platform/default/_flags.py</code></a></p>\n\n<p>Obviously, this <code>tf.app.flags</code> is somehow used to configure a network, so why  is it not in the API docs? Can anyone explain what is going on here? </p>\n", "link": "https://stackoverflow.com/questions/33932901/whats-the-purpose-of-tf-app-flags-in-tensorflow", "question_id": 33932901, "accepted_answer_id": null}, {"title": "TensorFlow, why there are 3 files after saving the model?", "question_body": "<p>Having read the <a href=\"https://www.tensorflow.org/how_tos/variables/#saving_and_restoring\" rel=\"noreferrer\">docs</a>, I saved a model in <code>TensorFlow</code>, here is my demo code:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Create some variables.\nv1 = tf.Variable(..., name=\"v1\")\nv2 = tf.Variable(..., name=\"v2\")\n...\n# Add an op to initialize the variables.\ninit_op = tf.global_variables_initializer()\n\n# Add ops to save and restore all the variables.\nsaver = tf.train.Saver()\n\n# Later, launch the model, initialize the variables, do some work, save the\n# variables to disk.\nwith tf.Session() as sess:\n  sess.run(init_op)\n  # Do some work with the model.\n  ..\n  # Save the variables to disk.\n  save_path = saver.save(sess, \"/tmp/model.ckpt\")\n  print(\"Model saved in file: %s\" % save_path)\n</code></pre>\n\n<p>but after that, I found there are 3 files</p>\n\n<pre><code>model.ckpt.data-00000-of-00001\nmodel.ckpt.index\nmodel.ckpt.meta\n</code></pre>\n\n<p>And I can't restore the model by restore the <code>model.ckpt</code> file, since there is no such file. Here is my code</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>with tf.Session() as sess:\n  # Restore variables from disk.\n  saver.restore(sess, \"/tmp/model.ckpt\")\n</code></pre>\n\n<p>So, why there are 3 files?</p>\n", "link": "https://stackoverflow.com/questions/41265035/tensorflow-why-there-are-3-files-after-saving-the-model", "question_id": 41265035, "accepted_answer_id": 41273348, "answer_body": "<p>Try this:</p>\n\n<pre><code>with tf.Session() as sess:\n    saver = tf.train.import_meta_graph('/tmp/model.ckpt.meta')\n    saver.restore(sess, \"/tmp/model.ckpt\")\n</code></pre>\n\n<p>The TensorFlow save method saves three kinds of files because it stores the <b>graph structure</b> separately from the <b>variable values</b>. The <code>.meta</code> file describes the saved graph structure, so you need to import it before restoring the checkpoint (otherwise it doesn't know what variables the saved checkpoint values correspond to).</p>\n\n<p>Alternatively, you could do this:</p>\n\n<pre><code># Recreate the EXACT SAME variables\nv1 = tf.Variable(..., name=\"v1\")\nv2 = tf.Variable(..., name=\"v2\")\n\n...\n\n# Now load the checkpoint variable values\nwith tf.Session() as sess:\n    saver = tf.train.Saver()\n    saver.restore(sess, \"/tmp/model.ckpt\")\n</code></pre>\n\n<p>Even though there is no file named <code>model.ckpt</code>, you still refer to the saved checkpoint by that name when restoring it. From the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py\" rel=\"noreferrer\"><code>saver.py</code> source code</a>: </p>\n\n<blockquote>\n  <p>Users only need to interact with the user-specified prefix... instead\n  of any physical pathname.</p>\n</blockquote>\n"}, {"title": "What&#39;s the difference between sparse_softmax_cross_entropy_with_logits and softmax_cross_entropy_with_logits?", "question_body": "<p>I recently came across <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\" rel=\"noreferrer\">tf.nn.sparse_softmax_cross_entropy_with_logits</a> and I can not figure out what the difference is compared to <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\" rel=\"noreferrer\">tf.nn.softmax_cross_entropy_with_logits</a>.</p>\n\n<p>Is the only difference that training vectors <code>y</code> have to be <a href=\"http://www.cs.toronto.edu/~guerzhoy/321/lec/W04/onehot.pdf\" rel=\"noreferrer\">one-hot encoded</a> when using <code>sparse_softmax_cross_entropy_with_logits</code>?</p>\n\n<p>Reading the API, I was unable to find any other difference compared to <code>softmax_cross_entropy_with_logits</code>. But why do we need the extra function then?</p>\n\n<p>Shouldn't <code>softmax_cross_entropy_with_logits</code> produce the same results as <code>sparse_softmax_cross_entropy_with_logits</code>, if it is supplied with one-hot encoded training data/vectors?  </p>\n", "link": "https://stackoverflow.com/questions/37312421/whats-the-difference-between-sparse-softmax-cross-entropy-with-logits-and-softm", "question_id": 37312421, "accepted_answer_id": 37317322, "answer_body": "<p>Having two different functions is a <strong>convenience</strong>, as they produce the same result.  </p>\n\n<p>The difference is simple:</p>\n\n<ul>\n<li>For <code>sparse_softmax_cross_entropy_with_logits</code>, labels must have the shape [batch_size] and the dtype int32 or int64. Each label is an int in range <code>[0, num_classes-1]</code>.</li>\n<li>For <code>softmax_cross_entropy_with_logits</code>, labels must have the shape [batch_size, num_classes] and dtype float32 or float64.</li>\n</ul>\n\n<p>Labels used in <code>softmax_cross_entropy_with_logits</code> are the <strong>one hot version</strong> of labels used in <code>sparse_softmax_cross_entropy_with_logits</code>.</p>\n\n<p>Another tiny difference is that with <code>sparse_softmax_cross_entropy_with_logits</code>, you can give -1 as a label to have loss <code>0</code> on this label.</p>\n"}, {"title": "In Tensorflow, get the names of all the Tensors in a graph", "question_body": "<p>I am creating neural nets with <code>Tensorflow</code> and <code>skflow</code>; for some reason I want to get the values of some inner tensors for a given input, so I am using <code>myClassifier.get_layer_value(input, \"tensorName\")</code>, <code>myClassifier</code> being a <code>skflow.estimators.TensorFlowEstimator</code>. </p>\n\n<p>However, I find it difficult to find the correct syntax of the tensor name, even knowing its name (and I'm getting confused between operation and tensors), so I'm using tensorboard to plot the graph and look for the name.</p>\n\n<p>Is there a way to enumerate all the tensors in a graph without using tensorboard?</p>\n", "link": "https://stackoverflow.com/questions/36883949/in-tensorflow-get-the-names-of-all-the-tensors-in-a-graph", "question_id": 36883949, "accepted_answer_id": 36893840, "answer_body": "<p>You can do</p>\n\n<pre><code>[n.name for n in tf.get_default_graph().as_graph_def().node]\n</code></pre>\n\n<p>Also, if you are prototyping in an IPython notebook, you can show the graph directly in notebook, see <code>show_graph</code> function in Alexander's Deep Dream <a href=\"http://nbviewer.jupyter.org/github/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb\" rel=\"noreferrer\">notebook</a></p>\n"}]