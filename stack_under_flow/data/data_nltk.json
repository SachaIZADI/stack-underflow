[{"title": "What is &quot;entropy and information gain&quot;?", "question_body": "<p>I am reading this book (<a href=\"http://www.nltk.org/book\" rel=\"noreferrer\">NLTK</a>) and it is confusing.  <strong>Entropy</strong> is <a href=\"http://www.nltk.org/book/ch06.html#entropy-and-information-gain\" rel=\"noreferrer\">defined as</a>:</p>\n\n<blockquote>\n  <p>Entropy is the sum of the probability of each label\n  times the log probability of that same label</p>\n</blockquote>\n\n<p>How can I apply <em>entropy</em> and <em>maximum entropy</em> in terms of text mining?  Can someone give me a easy, simple example (visual)?</p>\n", "link": "https://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain", "question_id": 1859554, "accepted_answer_id": 1859910, "answer_body": "<p>I assume entropy was mentioned in the context of building <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning\" rel=\"noreferrer\"><strong>decision trees</strong></a>.</p>\n\n<p>To illustrate, imagine the task of <a href=\"https://en.wikipedia.org/wiki/Supervised_learning\" rel=\"noreferrer\">learning</a> to <a href=\"https://en.wikipedia.org/wiki/Statistical_classification\" rel=\"noreferrer\">classify</a> first-names into male/female groups. That is given a list of names each labeled with either <code>m</code> or <code>f</code>, we want to learn a <a href=\"https://en.wikipedia.org/wiki/Predictive_modelling\" rel=\"noreferrer\">model</a> that fits the data and can be used to predict the gender of a new unseen first-name.</p>\n\n<pre><code>name       gender\n-----------------        Now we want to predict \nAshley        f              the gender of \"Amro\" (my name)\nBrian         m\nCaroline      f\nDavid         m\n</code></pre>\n\n<p>First step is <a href=\"https://en.wikipedia.org/wiki/Feature_selection\" rel=\"noreferrer\">deciding</a> what <a href=\"https://en.wikipedia.org/wiki/Feature_%28machine_learning%29\" rel=\"noreferrer\"><strong>features</strong></a> of the data are relevant to the target class we want to predict. Some example features include: first/last letter, length, number of vowels, does it end with a vowel, etc.. So after feature extraction, our data looks like:</p>\n\n<pre><code># name    ends-vowel  num-vowels   length   gender\n# ------------------------------------------------\nAshley        1         3           6        f\nBrian         0         2           5        m\nCaroline      1         4           8        f\nDavid         0         2           5        m\n</code></pre>\n\n<p>The goal is to build a <a href=\"https://en.wikipedia.org/wiki/Decision_tree\" rel=\"noreferrer\">decision tree</a>. An example of a <a href=\"https://en.wikipedia.org/wiki/Tree_%28data_structure%29\" rel=\"noreferrer\">tree</a> would be:</p>\n\n<pre><code>length&lt;7\n|   num-vowels&lt;3: male\n|   num-vowels&gt;=3\n|   |   ends-vowel=1: female\n|   |   ends-vowel=0: male\nlength&gt;=7\n|   length=5: male\n</code></pre>\n\n<p>basically each node represent a test performed on a single attribute, and we go left or right depending on the result of the test. We keep traversing the tree until we reach a leaf node which contains the class prediction (<code>m</code> or <code>f</code>)</p>\n\n<p>So if we run the name <em>Amro</em> down this tree, we start by testing \"<em>is the length&lt;7?</em>\" and the answer is <em>yes</em>, so we go down that branch. Following the branch, the next test \"<em>is the number of vowels&lt;3?</em>\" again evaluates to <em>true</em>. This leads to a leaf node labeled <code>m</code>, and thus the prediction is <em>male</em> (which I happen to be, so the tree predicted the outcome <a href=\"https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers\" rel=\"noreferrer\">correctly</a>).</p>\n\n<p>The decision tree is <a href=\"https://en.wikipedia.org/wiki/ID3_algorithm\" rel=\"noreferrer\">built in a top-down fashion</a>, but the question is how do you choose which attribute to split at each node? The answer is find the feature that best splits the target class into the purest possible children nodes (ie: nodes that don't contain a mix of both male and female, rather pure nodes with only one class).</p>\n\n<p>This measure of <em>purity</em> is called the <a href=\"https://en.wikipedia.org/wiki/Information_theory\" rel=\"noreferrer\"><strong>information</strong></a>. It represents the <a href=\"https://en.wikipedia.org/wiki/Expected_value\" rel=\"noreferrer\">expected</a> amount of <a href=\"https://en.wikipedia.org/wiki/Self-information\" rel=\"noreferrer\">information</a> that would be needed to specify whether a new instance (first-name) should be classified male or female, given the example that reached the node. We calculate it\nbased on the number of male and female classes at the node.</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Information_entropy\" rel=\"noreferrer\"><strong>Entropy</strong></a> on the other hand is a measure of <em>impurity</em> (the opposite). It is defined for a <a href=\"https://en.wikipedia.org/wiki/Binary_classification\" rel=\"noreferrer\">binary class</a> with values <code>a</code>/<code>b</code> as:</p>\n\n<pre><code>Entropy = - p(a)*log(p(a)) - p(b)*log(p(b))\n</code></pre>\n\n<p>This <a href=\"https://en.wikipedia.org/wiki/Binary_entropy_function\" rel=\"noreferrer\">binary entropy function</a> is depicted in the figure below (random variable can take one of two values). It reaches its maximum when the probability is <code>p=1/2</code>, meaning that <code>p(X=a)=0.5</code> or similarly<code>p(X=b)=0.5</code> having a 50%/50% chance of being either <code>a</code> or <code>b</code> (uncertainty is at a maximum). The entropy function is at zero minimum when probability is <code>p=1</code> or <code>p=0</code> with complete certainty (<code>p(X=a)=1</code> or <code>p(X=a)=0</code> respectively, latter implies <code>p(X=b)=1</code>).</p>\n\n<p><img src=\"https://i.stack.imgur.com/OUgcx.png\" alt=\"https://en.wikipedia.org/wiki/File:Binary_entropy_plot.svg\"></p>\n\n<p>Of course the definition of entropy can be generalized for a discrete random variable X with N outcomes (not just two):</p>\n\n<p><img src=\"https://i.stack.imgur.com/vIFD7.png\" alt=\"entropy\"></p>\n\n<p><em>(the <code>log</code> in the formula is usually taken as the <a href=\"https://en.wikipedia.org/wiki/Binary_logarithm\" rel=\"noreferrer\">logarithm to the base 2</a>)</em></p>\n\n<hr>\n\n<p>Back to our task of name classification, lets look at an example. Imagine at some point during the process of constructing the tree, we were considering the following split:</p>\n\n<pre><code>     ends-vowel\n      [9m,5f]          &lt;--- the [..,..] notation represents the class\n    /          \\            distribution of instances that reached a node\n   =1          =0\n -------     -------\n [3m,4f]     [6m,1f]\n</code></pre>\n\n<p>As you can see, before the split we had 9 males and 5 females, i.e. <code>P(m)=9/14</code> and <code>P(f)=5/14</code>. According to the definition of entropy:</p>\n\n<pre><code>Entropy_before = - (5/14)*log2(5/14) - (9/14)*log2(9/14) = 0.9403\n</code></pre>\n\n<p>Next we compare it with the entropy computed after considering the split by looking at two child branches. In the left branch of <code>ends-vowel=1</code>, we have:</p>\n\n<pre><code>Entropy_left = - (3/7)*log2(3/7) - (4/7)*log2(4/7) = 0.9852\n</code></pre>\n\n<p>and the right branch of <code>ends-vowel=0</code>, we have:</p>\n\n<pre><code>Entropy_right = - (6/7)*log2(6/7) - (1/7)*log2(1/7) = 0.5917\n</code></pre>\n\n<p>We combine the left/right entropies using the number of instances down each branch as <a href=\"https://en.wikipedia.org/wiki/Weighted_arithmetic_mean\" rel=\"noreferrer\">weight factor</a> (7 instances went left, and 7 instances went right), and get the final entropy after the split:</p>\n\n<pre><code>Entropy_after = 7/14*Entropy_left + 7/14*Entropy_right = 0.7885\n</code></pre>\n\n<p>Now by comparing the entropy before and after the split, we obtain a measure of <a href=\"https://en.wikipedia.org/wiki/Information_gain_in_decision_trees\" rel=\"noreferrer\"><strong>information gain</strong></a>, or how much information we gained by doing the split using that particular feature:</p>\n\n<pre><code>Information_Gain = Entropy_before - Entropy_after = 0.1518\n</code></pre>\n\n<p><em>You can interpret the above calculation as following: by doing the split with the <code>end-vowels</code> feature, we were able to reduce uncertainty in the sub-tree prediction outcome by a small amount of 0.1518 (measured in <a href=\"https://en.wikipedia.org/wiki/Bit\" rel=\"noreferrer\">bits</a> as <a href=\"https://en.wikipedia.org/wiki/Units_of_information\" rel=\"noreferrer\">units of information</a>).</em></p>\n\n<p>At each node of the tree, this calculation is performed for every feature, and the feature with the <em>largest information gain</em> is chosen for the split in a <a href=\"https://en.wikipedia.org/wiki/Greedy_algorithm\" rel=\"noreferrer\">greedy</a> manner (thus favoring features that produce <em>pure</em> splits with low uncertainty/entropy). This process is applied recursively from the root-node down, and stops when a leaf node contains instances all having the same class (no need to split it further).</p>\n\n<p>Note that I skipped over some <a href=\"https://en.wikipedia.org/wiki/C4.5_algorithm\" rel=\"noreferrer\">details</a> which are beyond the scope of this post, including how to handle <a href=\"https://en.wikipedia.org/wiki/Discretization_of_continuous_features\" rel=\"noreferrer\">numeric features</a>, <a href=\"https://en.wikipedia.org/wiki/Missing_data\" rel=\"noreferrer\">missing values</a>, <a href=\"https://en.wikipedia.org/wiki/Overfitting\" rel=\"noreferrer\">overfitting</a> and <a href=\"https://en.wikipedia.org/wiki/Pruning_%28decision_trees%29\" rel=\"noreferrer\">pruning</a> trees, etc..</p>\n"}, {"title": "Failed loading english.pickle with nltk.data.load", "question_body": "<p>When trying to load the <code>punkt</code> tokenizer...</p>\n\n<pre><code>import nltk.data\ntokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n</code></pre>\n\n<p>...a <code>LookupError</code> was raised:</p>\n\n<pre><code>&gt; LookupError: \n&gt;     *********************************************************************   \n&gt; Resource 'tokenizers/punkt/english.pickle' not found.  Please use the NLTK Downloader to obtain the resource: nltk.download().   Searched in:\n&gt;         - 'C:\\\\Users\\\\Martinos/nltk_data'\n&gt;         - 'C:\\\\nltk_data'\n&gt;         - 'D:\\\\nltk_data'\n&gt;         - 'E:\\\\nltk_data'\n&gt;         - 'E:\\\\Python26\\\\nltk_data'\n&gt;         - 'E:\\\\Python26\\\\lib\\\\nltk_data'\n&gt;         - 'C:\\\\Users\\\\Martinos\\\\AppData\\\\Roaming\\\\nltk_data'\n&gt;     **********************************************************************\n</code></pre>\n", "link": "https://stackoverflow.com/questions/4867197/failed-loading-english-pickle-with-nltk-data-load", "question_id": 4867197, "accepted_answer_id": null}, {"title": "What are all possible pos tags of NLTK?", "question_body": "<p>How do I find a list with all possible pos tags used by the Natural Language Toolkit (nltk)?</p>\n", "link": "https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk", "question_id": 15388831, "accepted_answer_id": 15389153, "answer_body": "<p><a href=\"http://nltk.org/book/ch05.html\" rel=\"noreferrer\">The book</a> has a note how to find help on tag sets, e.g.:</p>\n\n<pre><code>nltk.help.upenn_tagset()\n</code></pre>\n\n<p>Others are probably similar. (Note: Maybe you first have to download <code>tagsets</code> from the download helper's <em>Models</em> section for this)</p>\n"}, {"title": "How to check if a word is an English word with Python?", "question_body": "<p>I want to check in a Python program if a word is in the English dictionary.</p>\n\n<p>I believe nltk wordnet interface might be the way to go but I have no clue how to use it for such a simple task.</p>\n\n<pre><code>def is_english_word(word):\n    pass # how to I implement is_english_word?\n\nis_english_word(token.lower())\n</code></pre>\n\n<p>In the future, I might want to check if the singular form of a word is in the dictionary (e.g., properties -> property -> english word). How would I achieve that?</p>\n", "link": "https://stackoverflow.com/questions/3788870/how-to-check-if-a-word-is-an-english-word-with-python", "question_id": 3788870, "accepted_answer_id": 3789057, "answer_body": "<p>For (much) more power and flexibility, use a dedicated spellchecking library like <a href=\"https://pypi.org/project/pyenchant/\" rel=\"noreferrer\"><code>PyEnchant</code></a>. There's a <a href=\"http://pythonhosted.org/pyenchant/tutorial.html\" rel=\"noreferrer\">tutorial</a>, or you could just dive straight in:</p>\n\n<pre><code>&gt;&gt;&gt; import enchant\n&gt;&gt;&gt; d = enchant.Dict(\"en_US\")\n&gt;&gt;&gt; d.check(\"Hello\")\nTrue\n&gt;&gt;&gt; d.check(\"Helo\")\nFalse\n&gt;&gt;&gt; d.suggest(\"Helo\")\n['He lo', 'He-lo', 'Hello', 'Helot', 'Help', 'Halo', 'Hell', 'Held', 'Helm', 'Hero', \"He'll\"]\n&gt;&gt;&gt;\n</code></pre>\n\n<p><code>PyEnchant</code> comes with a few dictionaries (en_GB, en_US, de_DE, fr_FR), but can use any of the <a href=\"http://wiki.services.openoffice.org/wiki/Dictionaries\" rel=\"noreferrer\">OpenOffice ones</a> if you want more languages.</p>\n\n<p>There appears to be a pluralisation library called <a href=\"http://pypi.python.org/pypi/inflect\" rel=\"noreferrer\"><code>inflect</code></a>, but I've no idea whether it's any good.</p>\n"}, {"title": "n-grams in python, four, five, six grams?", "question_body": "<p>I'm looking for a way to split a text into n-grams.\nNormally I would do something like:</p>\n\n<pre><code>import nltk\nfrom nltk import bigrams\nstring = \"I really like python, it's pretty awesome.\"\nstring_bigrams = bigrams(string)\nprint string_bigrams\n</code></pre>\n\n<p>I am aware that nltk only offers bigrams and trigrams, but is there a way to split my text in four-grams, five-grams or even hundred-grams?</p>\n\n<p>Thanks!</p>\n", "link": "https://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams", "question_id": 17531684, "accepted_answer_id": 17547860, "answer_body": "<p>Great native python based answers given by other users. But here's the <code>nltk</code> approach (just in case, the OP gets penalized for reinventing what's already existing in the <code>nltk</code> library). </p>\n\n<p>There is an <a href=\"http://www.nltk.org/_modules/nltk/model/ngram.html\" rel=\"noreferrer\">ngram module</a> that people seldom use in <code>nltk</code>. It's not because it's hard to read ngrams, but training a model base on ngrams where n > 3 will result in much data sparsity.</p>\n\n<pre><code>from nltk import ngrams\n\nsentence = 'this is a foo bar sentences and i want to ngramize it'\n\nn = 6\nsixgrams = ngrams(sentence.split(), n)\n\nfor grams in sixgrams:\n  print grams\n</code></pre>\n"}, {"title": "what is the true difference between lemmatization vs stemming?", "question_body": "<p>When do I use each ?</p>\n\n<p>Also...is the NLTK lemmatization dependent upon Parts of Speech?\nWouldn't it be more accurate if it was?</p>\n", "link": "https://stackoverflow.com/questions/1787110/what-is-the-true-difference-between-lemmatization-vs-stemming", "question_id": 1787110, "accepted_answer_id": 1787121, "answer_body": "<p>Short and dense: <a href=\"http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\">http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html</a></p>\n\n<blockquote>\n  <p>The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.</p>\n  \n  <p>However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .</p>\n</blockquote>\n\n<p>From the NLTK docs:</p>\n\n<blockquote>\n  <p>Lemmatization and stemming are special cases of normalization. They identify a canonical representative for a set of related word forms.</p>\n</blockquote>\n"}, {"title": "How to get rid of punctuation using NLTK tokenizer?", "question_body": "<p>I'm just starting to use NLTK and I don't quite understand how to get a list of words from text. If I use <code>nltk.word_tokenize()</code>, I get a list of words and punctuation. I need only the words instead. How can I get rid of punctuation? Also <code>word_tokenize</code> doesn't work with multiple sentences: dots are added to the last word.</p>\n", "link": "https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer", "question_id": 15547409, "accepted_answer_id": null}, {"title": "How to remove stop words using nltk or python", "question_body": "<p>So I have a dataset that I would like to remove stop words from using </p>\n\n<pre><code>stopwords.words('english')\n</code></pre>\n\n<p>I'm struggling how to use this within my code to just simply take out these words. I have a list of the words from this dataset already, the part i'm struggling with is comparing to this list and removing the stop words.\nAny help is appreciated.</p>\n", "link": "https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python", "question_id": 5486337, "accepted_answer_id": null}, {"title": "pip issue installing almost any library", "question_body": "<p>I have a difficult time using pip to install almost anything. I'm new to coding, so I thought maybe this is something I've been doing wrong and have opted out to easy_install to get most of what I needed done, which has generally worked. However, now I'm trying to download the nltk library, and neither is getting the job done.</p>\n\n<p>I tried entering</p>\n\n<pre><code>sudo pip install nltk\n</code></pre>\n\n<p>but got the following response:</p>\n\n<pre><code>/Library/Frameworks/Python.framework/Versions/2.7/bin/pip run on Sat May  4 00:15:38 2013\nDownloading/unpacking nltk\n\n  Getting page https://pypi.python.org/simple/nltk/\n  Could not fetch URL [need more reputation to post link]: There was a problem confirming the ssl certificate: &lt;urlopen error [Errno 1] _ssl.c:504: error:0D0890A1:asn1 encoding routines:ASN1_verify:unknown message digest algorithm&gt;\n\n  Will skip URL [need more reputation to post link]/simple/nltk/ when looking for download links for nltk\n\n  Getting page [need more reputation to post link]/simple/\n  Could not fetch URL https://pypi.python. org/simple/: There was a problem confirming the ssl certificate: &lt;urlopen error [Errno 1] _ssl.c:504: error:0D0890A1:asn1 encoding routines:ASN1_verify:unknown message digest algorithm&gt;\n\n  Will skip URL [need more reputation to post link] when looking for download links for nltk\n\n  Cannot fetch index base URL [need more reputation to post link]\n\n  URLs to search for versions for nltk:\n  * [need more reputation to post link]\n  Getting page [need more reputation to post link]\n  Could not fetch URL [need more reputation to post link]: There was a problem confirming the ssl certificate: &lt;urlopen error [Errno 1] _ssl.c:504: error:0D0890A1:asn1 encoding routines:ASN1_verify:unknown message digest algorithm&gt;\n\n  Will skip URL [need more reputation to post link] when looking for download links for nltk\n\n  Could not find any downloads that satisfy the requirement nltk\n\nNo distributions at all found for nltk\n\nException information:\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pip-1.3.1-py2.7.egg/pip/basecommand.py\", line 139, in main\n    status = self.run(options, args)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pip-1.3.1-py2.7.egg/pip/commands/install.py\", line 266, in run\n    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pip-1.3.1-py2.7.egg/pip/req.py\", line 1026, in prepare_files\n    url = finder.find_requirement(req_to_install, upgrade=self.upgrade)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pip-1.3.1-py2.7.egg/pip/index.py\", line 171, in find_requirement\n    raise DistributionNotFound('No distributions at all found for %s' % req)\nDistributionNotFound: No distributions at all found for nltk\n\n--easy_install installed fragments of the library and the code ran into trouble very quickly upon trying to run it.\n</code></pre>\n\n<p>Any thoughts on this issue? I'd really appreciate some feedback on how I can either get pip working or something to get around the issue in the meantime.</p>\n", "link": "https://stackoverflow.com/questions/16370583/pip-issue-installing-almost-any-library", "question_id": 16370583, "accepted_answer_id": null}, {"title": "Resource u&#39;tokenizers/punkt/english.pickle&#39; not found", "question_body": "<p>My Code:</p>\n\n<pre><code>import nltk.data\ntokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n</code></pre>\n\n<p>ERROR Message:</p>\n\n<pre><code>[ec2-user@ip-172-31-31-31 sentiment]$ python mapper_local_v1.0.py\nTraceback (most recent call last):\nFile \"mapper_local_v1.0.py\", line 16, in &lt;module&gt;\n\n    tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n\nFile \"/usr/lib/python2.6/site-packages/nltk/data.py\", line 774, in load\n\n    opened_resource = _open(resource_url)\n\nFile \"/usr/lib/python2.6/site-packages/nltk/data.py\", line 888, in _open\n\n    return find(path_, path + ['']).open()\n\nFile \"/usr/lib/python2.6/site-packages/nltk/data.py\", line 618, in find\n\n    raise LookupError(resource_not_found)\n\nLookupError:\n\nResource u'tokenizers/punkt/english.pickle' not found.  Please\nuse the NLTK Downloader to obtain the resource:\n\n    &gt;&gt;&gt;nltk.download()\n\nSearched in:\n- '/home/ec2-user/nltk_data'\n- '/usr/share/nltk_data'\n- '/usr/local/share/nltk_data'\n- '/usr/lib/nltk_data'\n- '/usr/local/lib/nltk_data'\n- u''\n</code></pre>\n\n<p>I'm trying to run this program in Unix machine:</p>\n\n<p>As per the error message, I logged into python shell from my unix machine then I used the below commands:</p>\n\n<pre><code>import nltk\nnltk.download()\n</code></pre>\n\n<p>and then I downloaded all the available things using d- down loader and l- list options but still the problem persists.</p>\n\n<p>I tried my best to find the solution in internet but I got the same solution what I did as I mentioned in my above steps.</p>\n", "link": "https://stackoverflow.com/questions/26570944/resource-utokenizers-punkt-english-pickle-not-found", "question_id": 26570944, "accepted_answer_id": 26575754, "answer_body": "<p>I got the solution:</p>\n\n<pre><code>import nltk\nnltk.download()\n</code></pre>\n\n<h2>once the NLTK Downloader starts</h2>\n\n<h2>    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit</h2>\n\n<p>Downloader> d</p>\n\n<p>Download which package (l=list; x=cancel)?\n  Identifier> punkt</p>\n"}, {"title": "Stanford Parser and NLTK", "question_body": "<p>Is it possible to use Stanford Parser in NLTK? (I am not talking about Stanford POS.)</p>\n", "link": "https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk", "question_id": 13883277, "accepted_answer_id": null}, {"title": "Python: tf-idf-cosine: to find document similarity", "question_body": "<p>I was following a tutorial which was available at <a href=\"http://blog.christianperone.com/?p=1589\" rel=\"noreferrer\">Part 1</a> &amp; <a href=\"http://blog.christianperone.com/?p=1747\" rel=\"noreferrer\">Part 2</a>. Unfortunately the author didn't have the time for the final section which involved using cosine similarity to actually find the distance between two documents. I followed the examples in the article with the help of the following link from <a href=\"https://stackoverflow.com/questions/11911469/tfidf-for-search-queries\">stackoverflow</a>, included is the code mentioned in the above link (just so as to make life easier)</p>\n\n<pre><code>from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport numpy.linalg as LA\n\ntrain_set = [\"The sky is blue.\", \"The sun is bright.\"]  # Documents\ntest_set = [\"The sun in the sky is bright.\"]  # Query\nstopWords = stopwords.words('english')\n\nvectorizer = CountVectorizer(stop_words = stopWords)\n#print vectorizer\ntransformer = TfidfTransformer()\n#print transformer\n\ntrainVectorizerArray = vectorizer.fit_transform(train_set).toarray()\ntestVectorizerArray = vectorizer.transform(test_set).toarray()\nprint 'Fit Vectorizer to train set', trainVectorizerArray\nprint 'Transform Vectorizer to test set', testVectorizerArray\n\ntransformer.fit(trainVectorizerArray)\nprint\nprint transformer.transform(trainVectorizerArray).toarray()\n\ntransformer.fit(testVectorizerArray)\nprint \ntfidf = transformer.transform(testVectorizerArray)\nprint tfidf.todense()\n</code></pre>\n\n<p>as a result of the above code I have the following matrix</p>\n\n<pre><code>Fit Vectorizer to train set [[1 0 1 0]\n [0 1 0 1]]\nTransform Vectorizer to test set [[0 1 1 1]]\n\n[[ 0.70710678  0.          0.70710678  0.        ]\n [ 0.          0.70710678  0.          0.70710678]]\n\n[[ 0.          0.57735027  0.57735027  0.57735027]]\n</code></pre>\n\n<p>I am not sure how to use this output in order to calculate cosine similarity, I know how to implement cosine similarity with respect to two vectors of similar length but here I am not sure how to identify the two vectors.</p>\n", "link": "https://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity", "question_id": 12118720, "accepted_answer_id": 12124981, "answer_body": "<p>WIth the Help of @excray's comment, I manage to figure it out the answer, What we need to do is actually write a simple for loop to iterate over the two arrays that represent the train data and test data. </p>\n\n<p>First implement a simple lambda function to hold formula for the cosine calculation:</p>\n\n<pre><code>cosine_function = lambda a, b : round(np.inner(a, b)/(LA.norm(a)*LA.norm(b)), 3)\n</code></pre>\n\n<p>And then just write a simple for loop to iterate over the to vector, logic is for every \"For each vector in trainVectorizerArray, you have to find the cosine similarity with the vector in testVectorizerArray.\"</p>\n\n<pre><code>from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport numpy.linalg as LA\n\ntrain_set = [\"The sky is blue.\", \"The sun is bright.\"] #Documents\ntest_set = [\"The sun in the sky is bright.\"] #Query\nstopWords = stopwords.words('english')\n\nvectorizer = CountVectorizer(stop_words = stopWords)\n#print vectorizer\ntransformer = TfidfTransformer()\n#print transformer\n\ntrainVectorizerArray = vectorizer.fit_transform(train_set).toarray()\ntestVectorizerArray = vectorizer.transform(test_set).toarray()\nprint 'Fit Vectorizer to train set', trainVectorizerArray\nprint 'Transform Vectorizer to test set', testVectorizerArray\ncx = lambda a, b : round(np.inner(a, b)/(LA.norm(a)*LA.norm(b)), 3)\n\nfor vector in trainVectorizerArray:\n    print vector\n    for testV in testVectorizerArray:\n        print testV\n        cosine = cx(vector, testV)\n        print cosine\n\ntransformer.fit(trainVectorizerArray)\nprint\nprint transformer.transform(trainVectorizerArray).toarray()\n\ntransformer.fit(testVectorizerArray)\nprint \ntfidf = transformer.transform(testVectorizerArray)\nprint tfidf.todense()\n</code></pre>\n\n<p>Here is the output:</p>\n\n<pre><code>Fit Vectorizer to train set [[1 0 1 0]\n [0 1 0 1]]\nTransform Vectorizer to test set [[0 1 1 1]]\n[1 0 1 0]\n[0 1 1 1]\n0.408\n[0 1 0 1]\n[0 1 1 1]\n0.816\n\n[[ 0.70710678  0.          0.70710678  0.        ]\n [ 0.          0.70710678  0.          0.70710678]]\n\n[[ 0.          0.57735027  0.57735027  0.57735027]]\n</code></pre>\n"}, {"title": "Creating a new corpus with NLTK", "question_body": "<p>I reckoned that often the answer to my title is to go and read the documentations, but I ran through the <a href=\"http://www.nltk.org/book\" rel=\"nofollow noreferrer\">NLTK book</a> but it doesn't give the answer. I'm kind of new to Python.</p>\n\n<p>I have a bunch of <code>.txt</code> files and I want to be able to use the corpus functions that NLTK provides for the corpus <code>nltk_data</code>. </p>\n\n<p>I've tried <code>PlaintextCorpusReader</code> but I couldn't get further than:</p>\n\n<pre><code>&gt;&gt;&gt;import nltk\n&gt;&gt;&gt;from nltk.corpus import PlaintextCorpusReader\n&gt;&gt;&gt;corpus_root = './'\n&gt;&gt;&gt;newcorpus = PlaintextCorpusReader(corpus_root, '.*')\n&gt;&gt;&gt;newcorpus.words()\n</code></pre>\n\n<p>How do I segment the <code>newcorpus</code> sentences using punkt? I tried using the punkt functions but the punkt functions couldn't read <code>PlaintextCorpusReader</code> class?</p>\n\n<p>Can you also lead me to how I can write the segmented data into text files?</p>\n", "link": "https://stackoverflow.com/questions/4951751/creating-a-new-corpus-with-nltk", "question_id": 4951751, "accepted_answer_id": 4952238, "answer_body": "<p>I think the <code>PlaintextCorpusReader</code> already segments the input with a punkt tokenizer, at least if your input language is english.</p>\n\n<p><a href=\"http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.plaintext.PlaintextCorpusReader\" rel=\"nofollow noreferrer\">PlainTextCorpusReader's constructor</a></p>\n\n<pre><code>def __init__(self, root, fileids,\n             word_tokenizer=WordPunctTokenizer(),\n             sent_tokenizer=nltk.data.LazyLoader(\n                 'tokenizers/punkt/english.pickle'),\n             para_block_reader=read_blankline_block,\n             encoding='utf8'):\n</code></pre>\n\n<p>You can pass the reader a word and sentence tokenizer, but for the latter the default already is <code>nltk.data.LazyLoader('tokenizers/punkt/english.pickle')</code>.</p>\n\n<p>For a single string, a tokenizer would be used as follows (explained <a href=\"https://www.nltk.org/api/nltk.tokenize.html\" rel=\"nofollow noreferrer\">here</a>, see section 5 for punkt tokenizer).</p>\n\n<pre><code>&gt;&gt;&gt; import nltk.data\n&gt;&gt;&gt; text = \"\"\"\n... Punkt knows that the periods in Mr. Smith and Johann S. Bach\n... do not mark sentence boundaries.  And sometimes sentences\n... can start with non-capitalized words.  i is a good variable\n... name.\n... \"\"\"\n&gt;&gt;&gt; tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n&gt;&gt;&gt; tokenizer.tokenize(text.strip())\n</code></pre>\n"}, {"title": "Practical examples of NLTK use", "question_body": "<p>I'm playing around with the <a href=\"http://www.nltk.org/\" rel=\"noreferrer\">Natural Language Toolkit</a> (NLTK).</p>\n\n<p>Its documentation (<a href=\"http://www.nltk.org/book\" rel=\"noreferrer\">Book</a> and <a href=\"http://nltk.googlecode.com/svn/trunk/doc/howto/index.html\" rel=\"noreferrer\">HOWTO</a>) are quite bulky and the examples are sometimes slightly advanced. </p>\n\n<p>Are there any good but basic examples of uses/applications of NLTK? I'm thinking of things like the <a href=\"http://streamhacker.wordpress.com/tag/nltk/\" rel=\"noreferrer\">NTLK articles</a> on the <em>Stream Hacker</em> blog. </p>\n", "link": "https://stackoverflow.com/questions/526469/practical-examples-of-nltk-use", "question_id": 526469, "accepted_answer_id": null}, {"title": "how to check which version of nltk, scikit learn installed?", "question_body": "<p>In shell script I am checking whether this packages are installed or not, if not installed then install it. So withing shell script:</p>\n\n<pre><code>import nltk\necho nltk.__version__\n</code></pre>\n\n<p>but it stops shell script at <code>import</code> line</p>\n\n<p>in linux terminal tried to see in this manner:</p>\n\n<pre><code>which nltk\n</code></pre>\n\n<p>which gives nothing thought it is installed.</p>\n\n<p>Is there any other way to verify this package installation in shell script, if not installed, also install it.</p>\n", "link": "https://stackoverflow.com/questions/28501072/how-to-check-which-version-of-nltk-scikit-learn-installed", "question_id": 28501072, "accepted_answer_id": 28501150, "answer_body": "<p><code>import nltk</code> is Python syntax, and as such won't work in a shell script.</p>\n\n<p>To test the version of <code>nltk</code> and <code>scikit_learn</code>, you can write a <strong>Python script</strong> and run it. Such a script may look like</p>\n\n<pre><code>import nltk\nimport sklearn\n\nprint('The nltk version is {}.'.format(nltk.__version__))\nprint('The scikit-learn version is {}.'.format(sklearn.__version__))\n\n# The nltk version is 3.0.0.\n# The scikit-learn version is 0.15.2.\n</code></pre>\n\n<p>Note that not all Python packages are guaranteed to have a <code>__version__</code> attribute, so for some others it may fail, but for nltk and scikit-learn at least it will work.</p>\n"}, {"title": "How to config nltk data directory from code?", "question_body": "<p>How to config nltk data directory from code?</p>\n", "link": "https://stackoverflow.com/questions/3522372/how-to-config-nltk-data-directory-from-code", "question_id": 3522372, "accepted_answer_id": null}, {"title": "Stopword removal with NLTK", "question_body": "<p>I am trying to process a user entered text by removing stopwords using nltk toolkit, but with stopword-removal the words like 'and', 'or', 'not' gets removed. I want these words to be present after stopword removal process as they are operators which are required for later processing text as query. I don't know which are the words which can be operators in text query, and I also want to remove unnecessary words from my text.</p>\n", "link": "https://stackoverflow.com/questions/19130512/stopword-removal-with-nltk", "question_id": 19130512, "accepted_answer_id": 24106778, "answer_body": "<p>I suggest you create your own list of operator words that you take out of the stopword list. Sets can be conveniently subtracted, so:</p>\n\n<pre><code>operators = set(('and', 'or', 'not'))\nstop = set(stopwords...) - operators\n</code></pre>\n\n<p>Then you can simply test if a word is <code>in</code> or <code>not in</code> the set without relying on whether your operators are part of the stopword list. You can then later switch to another stopword list or add an operator.</p>\n\n<pre><code>if word.lower() not in stop:\n    # use word\n</code></pre>\n"}, {"title": "SSL error downloading NLTK data", "question_body": "<p>I am trying to download NLTK 3.0 for use with Python 3.6 on Mac OS X 10.7.5, but am getting an SSL error:</p>\n\n<pre><code>import nltk\nnltk.download()\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/rAwBe.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/rAwBe.png\" alt=\"enter image description here\"></a></p>\n\n<p>I downloaded NLTK with a pip3 command: <code>sudo pip3 install -U nltk</code>.</p>\n\n<p>Changing the index in the NLTK downloader allows the downloader to show all of NLTK's files, but when one tries to download all, one gets another SSL error (see bottom of photo):</p>\n\n<p><a href=\"https://i.stack.imgur.com/Pt2Oq.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Pt2Oq.png\" alt=\"enter image description here\"></a></p>\n\n<p>I am relatively new to computer science and am not at all savvy with respect to SSL.</p>\n\n<p>My question is how to simply resolve this issue?</p>\n\n<hr>\n\n<p>Here is a similar question by a user who is having the same problem:</p>\n\n<p><a href=\"https://stackoverflow.com/questions/38725583/unable-to-download-nltk-data\">Unable to download nltk data</a></p>\n\n<p>I decided to post a new question with screenshots, since my edit to that other question was rejected.</p>\n\n<p>Similar questions which I did not find helpful:</p>\n\n<p><a href=\"https://stackoverflow.com/questions/38916452/nltk-download-ssl-certificate-verify-failed\">NLTK download SSL: Certificate verify failed</a></p>\n\n<p><a href=\"https://stackoverflow.com/questions/27658409/downloading-error-using-nltk-download\">downloading error using nltk.download()</a></p>\n", "link": "https://stackoverflow.com/questions/41348621/ssl-error-downloading-nltk-data", "question_id": 41348621, "accepted_answer_id": 42890688, "answer_body": "<p>You don't need to disable SSL checking if you run the following terminal command:</p>\n\n<pre><code>/Applications/Python 3.6/Install Certificates.command\n</code></pre>\n\n<p>In the place of <code>3.6</code>, put your version of Python if it's an earlier one. Then you should be able to open your Python interpreter (using the command <code>python3</code>) and successfully run <code>nltk.download()</code> there.</p>\n\n<p>This is an issue wherein <code>urllib</code> uses an embedded version of OpenSSL that not in the system certificate store. <a href=\"https://stackoverflow.com/a/41692664/1031615\">Here's an answer</a> with more information on what's going on.</p>\n"}, {"title": "English grammar for parsing in NLTK", "question_body": "<p>Is there a ready-to-use English grammar that I can just load it and use in NLTK? I've searched around examples of parsing with NLTK, but it seems like that I have to manually specify grammar before parsing a sentence. </p>\n\n<p>Thanks a lot!</p>\n", "link": "https://stackoverflow.com/questions/6115677/english-grammar-for-parsing-in-nltk", "question_id": 6115677, "accepted_answer_id": null}, {"title": "How to extract common / significant phrases from a series of text entries", "question_body": "<p>I have a series of text items- raw HTML from a MySQL database. I want to find the most common phrases in these entries (not the single most common phrase, and ideally, not enforcing word-for-word matching). </p>\n\n<p>My example is any review on Yelp.com, that shows 3 snippets from hundreds of reviews of a given restaurant, in the format: </p>\n\n<p>\"Try the hamburger\" (in 44 reviews)</p>\n\n<p>e.g., the \"Review Highlights\" section of this page: </p>\n\n<p><a href=\"http://www.yelp.com/biz/sushi-gen-los-angeles\" rel=\"noreferrer\"><a href=\"http://www.yelp.com/biz/sushi-gen-los-angeles/\" rel=\"noreferrer\">http://www.yelp.com/biz/sushi-gen-los-angeles/</a></a></p>\n\n<p>I have NLTK installed and I've played around with it a bit, but am honestly overwhelmed by the options. This seems like a rather common problem and I haven't been able to find a straightforward solution by searching here.</p>\n", "link": "https://stackoverflow.com/questions/2452982/how-to-extract-common-significant-phrases-from-a-series-of-text-entries", "question_id": 2452982, "accepted_answer_id": null}, {"title": "Programmatically install NLTK corpora / models, i.e. without the GUI downloader?", "question_body": "<p>My project uses the NLTK. How can I list the project's corpus &amp; model requirements so they can be automatically installed? I don't want to click through the <code>nltk.download()</code> GUI, installing packages one by one.</p>\n\n<p>Also, any way to freeze that same list of requirements (like <code>pip freeze</code>)?</p>\n", "link": "https://stackoverflow.com/questions/5843817/programmatically-install-nltk-corpora-models-i-e-without-the-gui-downloader", "question_id": 5843817, "accepted_answer_id": 6052180, "answer_body": "<p>The NLTK site does list a command line interface for downloading packages and collections at the bottom of this page : </p>\n\n<p><a href=\"http://www.nltk.org/data\" rel=\"noreferrer\">http://www.nltk.org/data</a></p>\n\n<p>The command line usage varies by which version of Python you are using, but on my Python2.6 install I noticed I was missing the 'spanish_grammar' model and this worked fine:</p>\n\n<pre><code>python -m nltk.downloader spanish_grammars\n</code></pre>\n\n<p>You mention listing the project's corpus and model requirements and while I'm not sure of a way to automagically do that, I figured I would at least share this.</p>\n"}, {"title": "NLTK and Stopwords Fail #lookuperror", "question_body": "<p>I am trying to start a project of sentiment analysis and I will use the stop words method. I made some research and I found that nltk have stopwords but when I execute the command there is an error.</p>\n\n<p>What I do is the following, in order to know which are the words that nltk use (like what you may found here <a href=\"http://www.nltk.org/book/ch02.html\">http://www.nltk.org/book/ch02.html</a> in section4.1):</p>\n\n<pre><code>from nltk.corpus import stopwords\nstopwords.words('english')\n</code></pre>\n\n<p>But when I press enter I obtain</p>\n\n<pre><code>---------------------------------------------------------------------------\nLookupError                               Traceback (most recent call last)\n&lt;ipython-input-6-ff9cd17f22b2&gt; in &lt;module&gt;()\n----&gt; 1 stopwords.words('english')\n\nC:\\Users\\Usuario\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.pyc in __getattr__(self, attr)\n 66\n 67     def __getattr__(self, attr):\n---&gt; 68         self.__load()\n 69         # This looks circular, but its not, since __load() changes our\n 70         # __class__ to something new:\n\nC:\\Users\\Usuario\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.pyc in __load(self)\n 54             except LookupError, e:\n 55                 try: root = nltk.data.find('corpora/%s' % zip_name)\n---&gt; 56                 except LookupError: raise e\n 57\n 58         # Load the corpus.\n\nLookupError:\n**********************************************************************\n  Resource 'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  &gt;&gt;&gt; nltk.download()\n  Searched in:\n- 'C:\\\\Users\\\\Meru/nltk_data'\n- 'C:\\\\nltk_data'\n- 'D:\\\\nltk_data'\n- 'E:\\\\nltk_data'\n- 'C:\\\\Users\\\\Meru\\\\Anaconda\\\\nltk_data'\n- 'C:\\\\Users\\\\Meru\\\\Anaconda\\\\lib\\\\nltk_data'\n- 'C:\\\\Users\\\\Meru\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n</code></pre>\n\n<p>And, because of this problem things like this cannot run properly (obtaining the same error):</p>\n\n<pre><code>&gt;&gt;&gt; from nltk.corpus import stopwords\n&gt;&gt;&gt; stop = stopwords.words('english')\n&gt;&gt;&gt; sentence = \"this is a foo bar sentence\"\n&gt;&gt;&gt; print [i for i in sentence.split() if i not in stop]\n</code></pre>\n\n<p>Do you know what may be problem? I must use words in Spanish, do you recomend another method? I also thought using Goslate package with datasets in english</p>\n\n<p>Thanks for reading!</p>\n\n<p>P.D.: I use Ananconda</p>\n", "link": "https://stackoverflow.com/questions/26693736/nltk-and-stopwords-fail-lookuperror", "question_id": 26693736, "accepted_answer_id": 26693897, "answer_body": "<p>You don't seem to have the stopwords corpus on your computer.</p>\n\n<p>You need to start the NLTK Downloader and download all the data you need.</p>\n\n<p>Open a Python console and do the following:</p>\n\n<pre><code>&gt;&gt;&gt; import nltk\n&gt;&gt;&gt; nltk.download()\nshowing info http://nltk.github.com/nltk_data/\n</code></pre>\n\n<p>In the GUI window that opens simply press the 'Download' button to download all corpora or go to the 'Corpora' tab and only download the ones you need/want.</p>\n"}, {"title": "wordnet lemmatization and pos tagging in python", "question_body": "<p>I wanted to use wordnet lemmatizer in python and I have learnt that the default pos tag is NOUN and that it does not output the correct lemma for a verb, unless the pos tag is explicitly specified as VERB.</p>\n\n<p>My question is what is the best shot inorder to perform the above lemmatization accurately?</p>\n\n<p>I did the pos tagging using <code>nltk.pos_tag</code> and I am lost in integrating the tree bank pos tags to wordnet compatible pos tags. Please help</p>\n\n<pre><code>from nltk.stem.wordnet import WordNetLemmatizer\nlmtzr = WordNetLemmatizer()\ntagged = nltk.pos_tag(tokens)\n</code></pre>\n\n<p>I get the output tags in NN,JJ,VB,RB. How do I change these to wordnet compatible tags?</p>\n\n<p>Also do I have to train <code>nltk.pos_tag()</code> with a tagged corpus or can I use it directly on my data to evaluate?</p>\n", "link": "https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python", "question_id": 15586721, "accepted_answer_id": 15590384, "answer_body": "<p>First of all, you can use <code>nltk.pos_tag()</code> directly without training it. \nThe function will load a pretrained tagger from a file. You can see the file name \nwith <code>nltk.tag._POS_TAGGER</code>:</p>\n\n<pre><code>nltk.tag._POS_TAGGER\n&gt;&gt;&gt; 'taggers/maxent_treebank_pos_tagger/english.pickle' \n</code></pre>\n\n<p>As it was trained with the Treebank corpus, it also uses the <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\" rel=\"noreferrer\">Treebank tag set</a>.</p>\n\n<p>The following function would map the treebank tags to WordNet part of speech names: </p>\n\n<pre><code>from nltk.corpus import wordnet\n\ndef get_wordnet_pos(treebank_tag):\n\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return ''\n</code></pre>\n\n<p>You can then use the return value with the lemmatizer:</p>\n\n<pre><code>from nltk.stem.wordnet import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nlemmatizer.lemmatize('going', wordnet.VERB)\n&gt;&gt;&gt; 'go'\n</code></pre>\n\n<p>Check the return value before passing it to the Lemmatizer because an empty string would give a <code>KeyError</code>. </p>\n"}, {"title": "Python NLTK: SyntaxError: Non-ASCII character &#39;\\xc3&#39; in file (Senitment Analysis -NLP)", "question_body": "<p>I am playing around with NLTK to do an assignment on sentiment analysis. I am using Python 2.7. NLTK 3.0 and NUMPY 1.9.1 version. </p>\n\n<p>This is code :</p>\n\n<pre><code>__author__ = 'karan'\nimport nltk\nimport re\nimport sys\n\n\n\ndef main():\n    print(\"Start\");\n    # getting the stop words\n    stopWords = open(\"english.txt\",\"r\");\n    stop_word = stopWords.read().split();\n    AllStopWrd = []\n    for wd in stop_word:\n        AllStopWrd.append(wd);\n    print(\"stop words-&gt; \",AllStopWrd);\n\n    # sample and also cleaning it\n    tweet1= 'Love, my new toy\u00ed\u00a0\u00bd\u00ed\u00b8\u00ed\u00a0\u00bd\u00ed\u00b8#iPhone6. Its good http://t.co/sHY1cab7sx'\n    print(\"old tweet-&gt; \",tweet1)\n    tweet1 = tweet1.lower()\n    tweet1 = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",tweet1).split())\n    print(tweet1);\n    tw = tweet1.split()\n    print(tw)\n\n\n    #tokenize\n    sentences = nltk.word_tokenize(tweet1)\n    print(\"tokenized -&gt;\", sentences)\n\n\n    #remove stop words\n    Otweet =[]\n    for w in tw:\n        if w not in AllStopWrd:\n            Otweet.append(w);\n    print(\"sans stop word-&gt; \",Otweet)\n\n\n    # get taggers for neg/pos/inc/dec/inv words\n    taggers ={}\n    negWords = open(\"neg.txt\",\"r\");\n    neg_word = negWords.read().split();\n    print(\"ned words-&gt; \",neg_word)\n    posWords = open(\"pos.txt\",\"r\");\n    pos_word = posWords.read().split();\n    print(\"pos words-&gt; \",pos_word)\n    incrWords = open(\"incr.txt\",\"r\");\n    inc_word = incrWords.read().split();\n    print(\"incr words-&gt; \",inc_word)\n    decrWords = open(\"decr.txt\",\"r\");\n    dec_word = decrWords.read().split();\n    print(\"dec wrds-&gt; \",dec_word)\n    invWords = open(\"inverse.txt\",\"r\");\n    inv_word = invWords.read().split();\n    print(\"inverse words-&gt; \",inv_word)\n    for nw in neg_word:\n        taggers.update({nw:'negative'});\n    for pw in pos_word:\n        taggers.update({pw:'positive'});\n    for iw in inc_word:\n        taggers.update({iw:'inc'});\n    for dw in dec_word:\n        taggers.update({dw:'dec'});\n    for ivw in inv_word:\n        taggers.update({ivw:'inv'});\n    print(\"tagger-&gt; \",taggers)\n    print(taggers.get('little'))\n\n    # get parts of speech\n    posTagger = [nltk.pos_tag(tw)]\n    print(\"posTagger-&gt; \",posTagger)\n\nmain();\n</code></pre>\n\n<p>This is the error that I am getting when running my code:</p>\n\n<pre><code>SyntaxError: Non-ASCII character '\\xc3' in file C:/Users/karan/PycharmProjects/mainProject/sentiment.py on line 19, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details\n</code></pre>\n\n<p>How do I fix this error?</p>\n\n<p>I also tried the code using Python 3.4.2 and with nltk 3.0 and NumPy 1.9.1 but then I get the error:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"C:/Users/karan/PycharmProjects/mainProject/sentiment.py\", line 80, in &lt;module&gt;\n    main();\n  File \"C:/Users/karan/PycharmProjects/mainProject/sentiment.py\", line 72, in main\n    posTagger = [nltk.pos_tag(tw)]\n  File \"C:\\Python34\\lib\\site-packages\\nltk\\tag\\__init__.py\", line 100, in pos_tag\n    tagger = load(_POS_TAGGER)\n  File \"C:\\Python34\\lib\\site-packages\\nltk\\data.py\", line 779, in load\n    resource_val = pickle.load(opened_resource)\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xcb in position 0: ordinal not in range(128)\n</code></pre>\n", "link": "https://stackoverflow.com/questions/26899235/python-nltk-syntaxerror-non-ascii-character-xc3-in-file-senitment-analysis", "question_id": 26899235, "accepted_answer_id": 26899264, "answer_body": "<p>Add the following to the top of your file   <code># coding=utf-8</code></p>\n\n<p>If you go to the link in the error you can seen the reason why:</p>\n\n<p><strong>Defining the Encoding</strong></p>\n\n<p><em>Python will default to ASCII as standard encoding if no other\n    encoding hints are given.\n    To define a source code encoding, a magic comment must\n    be placed into the source files either as first or second\n    line in the file, such as:\n          # coding=</em></p>\n"}, {"title": "How do I tokenize a string sentence in NLTK?", "question_body": "<p>I am using nltk, so I want to create my own custom texts just like the default ones on nltk.books. However, I've just got up to the method like</p>\n\n<pre><code>my_text = ['This', 'is', 'my', 'text']\n</code></pre>\n\n<p>I'd like to discover any way to input my \"text\" as:</p>\n\n<pre><code>my_text = \"This is my text, this is a nice way to input text.\"\n</code></pre>\n\n<p>Which method, python's or from nltk allows me to do this. And more important, how can I dismiss punctuation symbols?</p>\n", "link": "https://stackoverflow.com/questions/15057945/how-do-i-tokenize-a-string-sentence-in-nltk", "question_id": 15057945, "accepted_answer_id": 15057966, "answer_body": "<p>This is actually on the <a href=\"http://nltk.org/\">main page of nltk.org</a>:</p>\n\n<pre><code>&gt;&gt;&gt; import nltk\n&gt;&gt;&gt; sentence = \"\"\"At eight o'clock on Thursday morning\n... Arthur didn't feel very good.\"\"\"\n&gt;&gt;&gt; tokens = nltk.word_tokenize(sentence)\n&gt;&gt;&gt; tokens\n['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning',\n'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n</code></pre>\n"}, {"title": "str.translate gives TypeError - Translate takes one argument (2 given), worked in Python 2", "question_body": "<p>I have the following code </p>\n\n<pre><code>import nltk, os, json, csv, string, cPickle\nfrom scipy.stats import scoreatpercentile\n\nlmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n\ndef sanitize(wordList): \nanswer = [word.translate(None, string.punctuation) for word in wordList] \nanswer = [lmtzr.lemmatize(word.lower()) for word in answer]\nreturn answer\n\nwords = []\nfor filename in json_list:\n    words.extend([sanitize(nltk.word_tokenize(' '.join([tweet['text'] \n                   for tweet in json.load(open(filename,READ))])))])\n</code></pre>\n\n<p>I've tested lines 2-4 in a separate testing.py file when I wrote</p>\n\n<pre><code>import nltk, os, json, csv, string, cPickle\nfrom scipy.stats import scoreatpercentile\n\nwordList= ['\\'the', 'the', '\"the']\nprint wordList\nwordList2 = [word.translate(None, string.punctuation) for word in wordList]\nprint wordList2\nanswer = [lmtzr.lemmatize(word.lower()) for word in wordList2]\nprint answer\n\nfreq = nltk.FreqDist(wordList2)\nprint freq\n</code></pre>\n\n<p>and the command prompt returns ['the','the','the'], which is what I wanted (removing punctuation).</p>\n\n<p>However, when I put the exact same code in a different file, python returns a TypeError stating that</p>\n\n<pre><code>File \"foo.py\", line 8, in &lt;module&gt;\n  for tweet in json.load(open(filename, READ))])))])\nFile \"foo.py\", line 2, in sanitize\n  answer = [word.translate(None, string.punctuation) for word in wordList]\nTypeError: translate() takes exactly one argument (2 given)\n</code></pre>\n\n<p>json_list is a list of all the file paths (I printed and check that this list is valid). I'm confused on this TypeError because everything works perfectly fine when I'm just testing it in a different file.</p>\n", "link": "https://stackoverflow.com/questions/23175809/str-translate-gives-typeerror-translate-takes-one-argument-2-given-worked-i", "question_id": 23175809, "accepted_answer_id": null}, {"title": "Save Naive Bayes Trained Classifier in NLTK", "question_body": "<p>I'm slightly confused in regard to how I save a trained classifier. As in, re-training a classifier each time I want to use it is obviously really bad and slow, how do I save it and the load it again when I need it? Code is below, thanks in advance for your help. I'm using Python with NLTK Naive Bayes Classifier. </p>\n\n<pre><code>classifier = nltk.NaiveBayesClassifier.train(training_set)\n# look inside the classifier train method in the source code of the NLTK library\n\ndef train(labeled_featuresets, estimator=nltk.probability.ELEProbDist):\n    # Create the P(label) distribution\n    label_probdist = estimator(label_freqdist)\n    # Create the P(fval|label, fname) distribution\n    feature_probdist = {}\n    return NaiveBayesClassifier(label_probdist, feature_probdist)\n</code></pre>\n", "link": "https://stackoverflow.com/questions/10017086/save-naive-bayes-trained-classifier-in-nltk", "question_id": 10017086, "accepted_answer_id": 10019929, "answer_body": "<p>To save:</p>\n\n<pre><code>import pickle\nf = open('my_classifier.pickle', 'wb')\npickle.dump(classifier, f)\nf.close()\n</code></pre>\n\n<p>To load later:</p>\n\n<pre><code>import pickle\nf = open('my_classifier.pickle', 'rb')\nclassifier = pickle.load(f)\nf.close()\n</code></pre>\n"}, {"title": "NLTK Named Entity Recognition with Custom Data", "question_body": "<p>I'm trying to extract named entities from my text using NLTK. I find that NLTK NER is not very accurate for my purpose and I want to add some more tags of my own as well. I've been trying to find a way to train my own NER, but I don't seem to be able to find the right resources. \nI have a couple of questions regarding NLTK-</p>\n\n<ol>\n<li>Can I use my own data to train an Named Entity Recognizer in NLTK?</li>\n<li>If I can train using my own data, is the named_entity.py the file to be modified?</li>\n<li>Does the input file format have to be in IOB eg. Eric NNP B-PERSON ?</li>\n<li>Are there any resources - apart from the nltk cookbook and nlp with python that I can use?</li>\n</ol>\n\n<p>I would really appreciate help in this regard</p>\n", "link": "https://stackoverflow.com/questions/11333903/nltk-named-entity-recognition-with-custom-data", "question_id": 11333903, "accepted_answer_id": null}, {"title": "tag generation from a text content", "question_body": "<p>I am curious if there is an algorithm/method exists to generate keywords/tags from a given text, by using some weight calculations, occurrence ratio or other tools.</p>\n\n<p>Additionally, I will be grateful if you point any Python based solution / library for this.</p>\n\n<p>Thanks</p>\n", "link": "https://stackoverflow.com/questions/2661778/tag-generation-from-a-text-content", "question_id": 2661778, "accepted_answer_id": 2664351, "answer_body": "<p>One way to do this would be to extract words that occur more frequently in a document than you would expect them to by chance. For example, say in a larger collection of documents the term 'Markov' is almost never seen. However, in a particular document from the same collection Markov shows up very frequently. This would suggest that Markov might be a good keyword or tag to associate with the document.</p>\n\n<p>To identify keywords like this, you could use the <a href=\"http://en.wikipedia.org/wiki/Pointwise_mutual_information\" rel=\"noreferrer\">point-wise mutual information</a> of the keyword and the document. This is given by <code>PMI(term, doc) = log [ P(term, doc) / (P(term)*P(doc)) ]</code>. This will roughly tell you how much less (or more) surprised you are to come across the term in the specific document as appose to coming across it in the larger collection.</p>\n\n<p>To identify the 5 best keywords to associate with a document, you would just sort the terms by their PMI score with the document and pick the 5 with the highest score. </p>\n\n<p>If you want to extract <strong>multiword tags</strong>, see the StackOverflow question <a href=\"https://stackoverflow.com/questions/2452982/how-to-extract-common-significant-phrases-from-a-series-of-text-entries\">How to extract common / significant phrases from a series of text entries</a>. </p>\n\n<p>Borrowing from my answer to that question, the <a href=\"http://www.nltk.org/howto/collocations.html\" rel=\"noreferrer\">NLTK collocations how-to</a> covers how to do \nextract interesting multiword expressions using n-gram PMI in a about 7 lines of code, e.g.:</p>\n\n<pre><code>import nltk\nfrom nltk.collocations import *\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n\n# change this to read in your data\nfinder = BigramCollocationFinder.from_words(\n   nltk.corpus.genesis.words('english-web.txt'))\n\n# only bigrams that appear 3+ times\nfinder.apply_freq_filter(3) \n\n# return the 5 n-grams with the highest PMI\nfinder.nbest(bigram_measures.pmi, 5)  \n</code></pre>\n"}, {"title": "How do I download NLTK data?", "question_body": "<p>Updated answer:NLTK works for 2.7 well. I had 3.2. I uninstalled 3.2 and installed 2.7. Now it works!!</p>\n\n<p>I have installed NLTK and tried to download NLTK Data. What I did was to follow the instrution on this site: <a href=\"http://www.nltk.org/data.html\">http://www.nltk.org/data.html</a></p>\n\n<p>I downloaded NLTK, installed it, and then tried to run the following code:</p>\n\n<pre><code>&gt;&gt;&gt; import nltk\n&gt;&gt;&gt; nltk.download()\n</code></pre>\n\n<p>It gave me the error message like below:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"&lt;pyshell#6&gt;\", line 1, in &lt;module&gt;\n    nltk.download()\nAttributeError: 'module' object has no attribute 'download'\n Directory of C:\\Python32\\Lib\\site-packages\n</code></pre>\n\n<p>Tried both <code>nltk.download()</code> and <code>nltk.downloader()</code>, both gave me error messages.</p>\n\n<p>Then I used <code>help(nltk)</code> to pull out the package, it shows the following info:</p>\n\n<pre><code>NAME\n    nltk\n\nPACKAGE CONTENTS\n    align\n    app (package)\n    book\n    ccg (package)\n    chat (package)\n    chunk (package)\n    classify (package)\n    cluster (package)\n    collocations\n    corpus (package)\n    data\n    decorators\n    downloader\n    draw (package)\n    examples (package)\n    featstruct\n    grammar\n    help\n    inference (package)\n    internals\n    lazyimport\n    metrics (package)\n    misc (package)\n    model (package)\n    parse (package)\n    probability\n    sem (package)\n    sourcedstring\n    stem (package)\n    tag (package)\n    test (package)\n    text\n    tokenize (package)\n    toolbox\n    tree\n    treetransforms\n    util\n    yamltags\n\nFILE\n    c:\\python32\\lib\\site-packages\\nltk\n</code></pre>\n\n<p>I do see Downloader there, not sure why it does not work. Python 3.2.2, system Windows vista.</p>\n", "link": "https://stackoverflow.com/questions/22211525/how-do-i-download-nltk-data", "question_id": 22211525, "accepted_answer_id": null}]