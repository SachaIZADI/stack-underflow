[{"answer_body": "<p><a href=\"https://docs.python.org/2/library/array.html?#array.array.append\" rel=\"noreferrer\"><code>append</code></a>: Appends object at the end.</p>\n\n<pre><code>x = [1, 2, 3]\nx.append([4, 5])\nprint (x)\n</code></pre>\n\n<p>gives you: <code>[1, 2, 3, [4, 5]]</code></p>\n\n<hr>\n\n<p><a href=\"https://docs.python.org/2/library/array.html?#array.array.extend\" rel=\"noreferrer\"><code>extend</code></a>: Extends list by appending elements from the iterable.</p>\n\n<pre><code>x = [1, 2, 3]\nx.extend([4, 5])\nprint (x)\n</code></pre>\n\n<p>gives you: <code>[1, 2, 3, 4, 5]</code></p>\n", "title": "What is the difference between Python&#39;s list methods append and extend?", "question_id": 252703, "labels": [{"sentence": "$shortcode$ append object at the end $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "give you $shortcode$", "label": "other"}, {"sentence": "$shortcode$ extend list by append element from the iterable $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "give you $shortcode$", "label": "other"}]}, {"answer_body": "<p>This depends a lot on what you mean by \"revert\".</p>\n\n<h2>Temporarily switch to a different commit</h2>\n\n<p>If you want to temporarily go back to it, fool around, then come back to where you are, all you have to do is check out the desired commit:</p>\n\n<pre class=\"lang-sh prettyprint-override\"><code># This will detach your HEAD, that is, leave you with no branch checked out:\ngit checkout 0d1d7fc32\n</code></pre>\n\n<p>Or if you want to make commits while you're there, go ahead and make a new branch while you're at it:</p>\n\n<pre><code>git checkout -b old-state 0d1d7fc32\n</code></pre>\n\n<p>To go back to where you were, just check out the branch you were on again. (If you've made changes, as always when switching branches, you'll have to deal with them as appropriate. You could reset to throw them away; you could stash, checkout, stash pop to take them with you; you could commit them to a branch there if you want a branch there.)</p>\n\n<h2>Hard delete unpublished commits</h2>\n\n<p>If, on the other hand, you want to really get rid of everything you've done since then, there are two possibilities. One, if you haven't published any of these commits, simply reset:</p>\n\n<pre class=\"lang-sh prettyprint-override\"><code># This will destroy any local modifications.\n# Don't do it if you have uncommitted work you want to keep.\ngit reset --hard 0d1d7fc32\n\n# Alternatively, if there's work to keep:\ngit stash\ngit reset --hard 0d1d7fc32\ngit stash pop\n# This saves the modifications, then reapplies that patch after resetting.\n# You could get merge conflicts, if you've modified things which were\n# changed since the commit you reset to.\n</code></pre>\n\n<p>If you mess up, you've already thrown away your local changes, but you can at least get back to where you were before by resetting again.</p>\n\n<h2>Undo published commits with new commits</h2>\n\n<p>On the other hand, if you've published the work, you probably don't want to reset the branch, since that's effectively rewriting history. In that case, you could indeed revert the commits. With Git, revert has a very specific meaning: create a commit with the reverse patch to cancel it out. This way you don't rewrite any history.</p>\n\n<pre class=\"lang-sh prettyprint-override\"><code># This will create three separate revert commits:\ngit revert a867b4af 25eee4ca 0766c053\n\n# It also takes ranges. This will revert the last two commits:\ngit revert HEAD~2..HEAD\n\n#Similarly, you can revert a range of commits using commit hashes:\ngit revert a867b4af..0766c053 \n\n# Reverting a merge commit\ngit revert -m 1 &lt;merge_commit_sha&gt;\n\n# To get just one, you could use `rebase -i` to squash them afterwards\n# Or, you could do it manually (be sure to do this at top level of the repo)\n# get your index and work tree into the desired state, without changing HEAD:\ngit checkout 0d1d7fc32 .\n\n# Then commit. Be sure and write a good message describing what you just did\ngit commit\n</code></pre>\n\n<p>The <a href=\"http://schacon.github.com/git/git-revert.html\" rel=\"noreferrer\"><code>git-revert</code> manpage</a> actually covers a lot of this in its description. Another useful link is <a href=\"https://git-scm.com/book/en/v2/Git-Tools-Advanced-Merging#_undoing_merges\" rel=\"noreferrer\">this git-scm.com section discussing git-revert</a>.</p>\n\n<p>If you decide you didn't want to revert after all, you can revert the revert (as described here) or reset back to before the revert (see the previous section).</p>\n\n<p>You may also find this answer helpful in this case:<br>\n<a href=\"https://stackoverflow.com/questions/34519665/how-to-move-head-forward-checkout-revet-reflog-reset/34519716#34519716\">How to move HEAD back to a previous location? (Detached head)</a></p>\n", "title": "How do I revert a Git repository to a previous commit?", "question_id": 4114095, "labels": [{"sentence": "this depend a lot on what you mean by revert", "label": "other"}, {"sentence": "temporarily switch to a different commit", "label": "solution"}, {"sentence": "if you want to temporarily go back to it fool around then come back to where you be all you have to do be check out the desire commit $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "or if you want to make commit while you be there go ahead and make a new branch while you be at it $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "to go back to where you be just check out the branch you be on again", "label": "solution"}, {"sentence": "if you have make change a always when switch branch you you will have to deal with them a appropriate", "label": "other"}, {"sentence": "you could reset to throw them away you could stash checkout stash pop to take them with you you could commit them to a branch there if you want a branch there", "label": "solution"}, {"sentence": "hard delete unpublished commit", "label": "other"}, {"sentence": "if on the other hand you want to really get rid of everything you have do since then there be two possibility", "label": "other"}, {"sentence": "one if you have not publish any of these commit simply reset $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "if you mess up you have already throw away your local change but you can at least get back to where you be before by reset again", "label": "solution"}, {"sentence": "undo publish commit with new commit", "label": "solution"}, {"sentence": "on the other hand if you have publish the work you probably do not want to reset the branch since that be effectively rewrite history", "label": "solution"}, {"sentence": "in that case you could indeed revert the commit", "label": "root_cause"}, {"sentence": "with git revert have a very specific mean create a commit with the reverse patch to cancel it out", "label": "root_cause"}, {"sentence": "this way you do not rewrite any history $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "the $shortcode$ manpage actually cover a lot of this in it description", "label": "root_cause"}, {"sentence": "another useful link be this git scm com section discus git revert", "label": "solution"}, {"sentence": "if you decide you do not want to revert after all you can revert the revert a describe here or reset back to before the revert see the previous section", "label": "other"}, {"sentence": "you may also find this answer helpful in this case", "label": "solution"}, {"sentence": "how to move head back to a previous location", "label": "other"}, {"sentence": "detach head", "label": "solution"}]}, {"answer_body": "<pre><code>from flask import request\n\n@app.route('/data')\ndef data():\n    # here we want to get the value of user (i.e. ?user=some-value)\n    user = request.args.get('user')\n</code></pre>\n", "title": "How do you get a query string on Flask?", "question_id": 11774265, "labels": [{"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<p>As pointed out in <a href=\"https://stackoverflow.com/questions/324477/in-a-django-form-how-do-i-make-a-field-readonly-or-disabled-so-that-it-cannot/34538169#34538169\">this answer</a>, Django 1.9 added the <a href=\"https://docs.djangoproject.com/en/stable/ref/forms/fields/#disabled\" rel=\"noreferrer\">Field.disabled</a> attribute:</p>\n\n<blockquote>\n  <p>The disabled boolean argument, when set to True, disables a form field using the disabled HTML attribute so that it won\u2019t be editable by users. Even if a user tampers with the field\u2019s value submitted to the server, it will be ignored in favor of the value from the form\u2019s initial data.</p>\n</blockquote>\n\n<p>With Django 1.8 and earlier, to disable entry on the widget and prevent malicious POST hacks you must scrub the input in addition to setting the <code>readonly</code> attribute on the form field:</p>\n\n<pre><code>class ItemForm(ModelForm):\n    def __init__(self, *args, **kwargs):\n        super(ItemForm, self).__init__(*args, **kwargs)\n        instance = getattr(self, 'instance', None)\n        if instance and instance.pk:\n            self.fields['sku'].widget.attrs['readonly'] = True\n\n    def clean_sku(self):\n        instance = getattr(self, 'instance', None)\n      \u00a0 if instance and instance.pk:\n        \u00a0 \u00a0 return instance.sku\n      \u00a0 else:\n        \u00a0 \u00a0 return self.cleaned_data['sku']\n</code></pre>\n\n<p>Or, replace <code>if instance and instance.pk</code> with another condition indicating you're editing.  You could also set the attribute <code>disabled</code> on the input field, instead of <code>readonly</code>.</p>\n\n<p>The <code>clean_sku</code> function will ensure that the <code>readonly</code> value won't be overridden by a <code>POST</code>.</p>\n\n<p>Otherwise, there is no built-in Django form field which will render a value while rejecting bound input data.  If this is what you desire, you should instead create a separate <code>ModelForm</code> that excludes the uneditable field(s), and just print them inside your template.</p>\n", "title": "In a Django form, how do I make a field readonly (or disabled) so that it cannot be edited?", "question_id": 324477, "labels": [{"sentence": "a point out in this answer django 1 9 add the field disable attribute", "label": "root_cause"}, {"sentence": "the disable boolean argument when set to true disable a form field use the disable html attribute so that it won\u2019t be editable by user", "label": "solution"}, {"sentence": "even if a user tamper with the field\u2019s value submit to the server it will be ignore in favor of the value from the form\u2019s initial data", "label": "other"}, {"sentence": "with django 1 8 and early to disable entry on the widget and prevent malicious post hack you must scrub the input in addition to set the $shortcode$ attribute on the form field $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "or replace $shortcode$ with another condition indicate you be edit", "label": "solution"}, {"sentence": "you could also set the attribute $shortcode$ on the input field instead of $shortcode$", "label": "solution"}, {"sentence": "the $shortcode$ function will ensure that the $shortcode$ value will not be override by a $shortcode$", "label": "root_cause"}, {"sentence": "otherwise there be no build in django form field which will render a value while reject bind input data", "label": "other"}, {"sentence": "if this be what you desire you should instead create a separate $shortcode$ that exclude the uneditable field s and just print them inside your template", "label": "solution"}]}, {"answer_body": "<p>No, I don't think \"open CUDA library\" is enough to tell, because different nodes of the graph may be on different devices.</p>\n\n<p>To find out which device is used, you can enable log device placement like this:</p>\n\n<pre><code>sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n</code></pre>\n\n<p>Check your console for this type of output.</p>\n\n<p><a href=\"https://i.stack.imgur.com/RtRiB.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/RtRiB.png\" alt=\"\"></a></p>\n", "title": "How to tell if tensorflow is using gpu acceleration from inside python shell?", "question_id": 38009682, "labels": [{"sentence": "no i do not think open cuda library be enough to tell because different nod of the graph may be on different device", "label": "root_cause"}, {"sentence": "to find out which device be use you can enable log device placement like this $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "check your console for this type of output", "label": "solution"}]}, {"answer_body": "<pre class=\"lang-py prettyprint-override\"><code>keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0,  \n          write_graph=True, write_images=True)\n</code></pre>\n\n<p>This line creates a Callback Tensorboard object, you should capture that object and give it to the <code>fit</code> function of your model.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n...\nmodel.fit(...inputs and parameters..., callbacks=[tbCallBack])\n</code></pre>\n\n<p>This way you gave your callback object to the function. It will be run during the training and will output files that can be used with tensorboard.</p>\n\n<p>If you want to visualize the files created during training, run in your terminal</p>\n\n<pre><code>tensorboard --logdir path_to_current_dir/Graph \n</code></pre>\n\n<p>Hope this helps !</p>\n", "title": "How do I use the Tensorboard callback of Keras?", "question_id": 42112260, "labels": [{"sentence": "$longcode$", "label": "solution"}, {"sentence": "this line create a callback tensorboard object you should capture that object and give it to the $shortcode$ function of your model $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "this way you give your callback object to the function", "label": "root_cause"}, {"sentence": "it will be run during the train and will output file that can be use with tensorboard", "label": "root_cause"}, {"sentence": "if you want to visualize the file create during train run in your terminal $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "hope this help", "label": "other"}]}, {"answer_body": "<p><strong>July 2018 Update</strong> </p>\n\n<p>Most people should now use <code>pip install setuptools</code> (possibly with <code>sudo</code>).</p>\n\n<p>Some may need to (re)install the <code>python-setuptools</code> package via their package manager (<code>apt-get install</code>, <code>yum install</code>, etc.).</p>\n\n<p>This issue can be highly dependent on your OS and dev environment. See the legacy/other answers below if the above isn't working for you.</p>\n\n<p><strong>Explanation</strong></p>\n\n<p>This error message is caused by a missing/broken Python <code>setuptools</code> package. Per Matt M.'s comment and <a href=\"https://github.com/pypa/setuptools/issues/581\" rel=\"noreferrer\">setuptools issue #581</a>, the bootstrap script referred to below is no longer the recommended installation method.</p>\n\n<p>The bootstrap script instructions will remain below, in case it's still helpful to anyone.</p>\n\n<p><strong>Legacy Answer</strong></p>\n\n<p>I encountered the same <code>ImportError</code> today while trying to use pip. Somehow the <code>setuptools</code> package had been deleted in my Python environment.</p>\n\n<p>To fix the issue, run the setup script for <code>setuptools</code>:</p>\n\n<pre><code>wget https://bootstrap.pypa.io/ez_setup.py -O - | python\n</code></pre>\n\n<p>(or if you don't have <code>wget</code> installed (e.g. OS X), try</p>\n\n<pre><code>curl https://bootstrap.pypa.io/ez_setup.py | python\n</code></pre>\n\n<p>possibly with <code>sudo</code> prepended.)</p>\n\n<p>If you have any version of <a href=\"http://pythonhosted.org/setuptools/merge-faq.html\" rel=\"noreferrer\"><code>distribute</code></a>, or any <code>setuptools</code> below 0.6, you will have to uninstall it first.*</p>\n\n<p>See <a href=\"https://pypi.python.org/pypi/setuptools/0.9.8#installation-instructions\" rel=\"noreferrer\">Installation Instructions</a> for further details.</p>\n\n<hr>\n\n<p>* If you already have a working <code>distribute</code>, upgrading it to the \"compatibility wrapper\" that switches you over to <code>setuptools</code> is easier. But if things are already broken, don't try that.</p>\n", "title": "No module named pkg_resources", "question_id": 7446187, "labels": [{"sentence": "july 2018 update", "label": "other"}, {"sentence": "most people should now use $shortcode$ possibly with $shortcode$", "label": "solution"}, {"sentence": "some may need to re install the $shortcode$ package via their package manager $shortcode$ $shortcode$ etc", "label": "solution"}, {"sentence": "this issue can be highly dependent on your o and dev environment", "label": "root_cause"}, {"sentence": "see the legacy/other answer below if the above be not work for you", "label": "solution"}, {"sentence": "explanation", "label": "root_cause"}, {"sentence": "this error message be cause by a missing/broken python $shortcode$ package", "label": "root_cause"}, {"sentence": "per matt m s comment and setuptools issue 581 the bootstrap script refer to below be no long the recommend installation method", "label": "other"}, {"sentence": "the bootstrap script instruction will remain below in case it be still helpful to anyone", "label": "root_cause"}, {"sentence": "legacy answer", "label": "other"}, {"sentence": "i encounter the same $shortcode$ today while try to use pip", "label": "other"}, {"sentence": "somehow the $shortcode$ package have be delete in my python environment", "label": "root_cause"}, {"sentence": "to fix the issue run the setup script for $shortcode$ $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "or if you do not have $shortcode$ instal e g", "label": "solution"}, {"sentence": "o x try $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "possibly with $shortcode$ prepended", "label": "solution"}, {"sentence": "if you have any version of $shortcode$ or any $shortcode$ below 0 6 you will have to uninstall it first", "label": "solution"}, {"sentence": "see installation instruction for further detail", "label": "solution"}, {"sentence": "if you already have a work $shortcode$ upgrade it to the compatibility wrapper that switch you over to $shortcode$ be easy", "label": "solution"}, {"sentence": "but if thing be already break do not try that", "label": "other"}]}, {"answer_body": "<p>So it's throwing up that error message because you have <code>apache2</code> running on port 80.</p>\n\n<p>If this is for development, I would just leave it as it is on port 5000.</p>\n\n<p>If it's for production either:</p>\n\n<p><strong>Not Recommended</strong></p>\n\n<ul>\n<li>Stop <code>apache2</code> first; </li>\n</ul>\n\n<p>Not recommended as it states in the documentation:</p>\n\n<blockquote>\n  <p>You can use the builtin server during development, but you should use a full deployment option for production applications. (Do not use the builtin development server in production.)</p>\n</blockquote>\n\n<p><strong>Recommended</strong></p>\n\n<ul>\n<li>Proxy <code>HTTP</code> traffic through <code>apache2</code> to Flask.</li>\n</ul>\n\n<p>This way, <code>apache2</code> can handle all your static files (which it's very good at - much better than the debug server built into Flask) and act as a reverse proxy for your dynamic content, passing those requests to Flask.</p>\n\n<p><a href=\"http://flask.pocoo.org/docs/deploying/mod_wsgi/#configuring-apache\" rel=\"noreferrer\">Here's a link</a> to the official documentation about setting up Flask with Apache + mod_wsgi.</p>\n\n<p><strong>Edit 1 - Clarification for @Djack</strong></p>\n\n<blockquote>\n  <p>Proxy HTTP traffic to Flask through apache2</p>\n</blockquote>\n\n<p>When a request comes to the server on port 80 (<code>HTTP</code>) or port 443 (<code>HTTPS</code>) a web server like Apache or Nginx handles the connection of the request and works out what to do with it. In our case a request received should be configured to be passed through to Flask on the WSGI protocol and handled by the Python code. This is the \"dynamic\" part. </p>\n\n<blockquote>\n  <p>reverse proxy for dynamic content</p>\n</blockquote>\n\n<p>There are a few advantages to configuring your web server like the above; </p>\n\n<ul>\n<li>SSL Termination - The web server will be optimized to handle HTTPS requests with only a little configuration. Don't \"roll your own\" in Python which is probably very insecure in comparison.</li>\n<li>Security - Opening a port to the internet requires careful consideration of security. Flask's development server is not designed for this and could have open bugs or security issues in comparison to a web server designed for this purpose. Note that a badly configured web server can also be insecure! </li>\n<li>Static File Handling - It is possible for the builtin Flask web server to handle static files however this is not recommended; Nginx/Apache are much more efficient at handling static files like images, CSS, Javascript files and will only pass \"dynamic\" requests (those where the content is often read from a database or the content changes) to be handled by the Python code. </li>\n<li>+more. This is bordering on scope for this question. If you want more info do some research into this area. </li>\n</ul>\n", "title": "How do I get Flask to run on port 80?", "question_id": 20212894, "labels": [{"sentence": "so it be throw up that error message because you have $shortcode$ run on port 80", "label": "root_cause"}, {"sentence": "if this be for development i would just leave it a it be on port 5000", "label": "solution"}, {"sentence": "if it be for production either", "label": "other"}, {"sentence": "not recommend", "label": "other"}, {"sentence": "stop $shortcode$ first", "label": "solution"}, {"sentence": "not recommend a it state in the documentation", "label": "other"}, {"sentence": "you can use the builtin server during development but you should use a full deployment option for production application", "label": "solution"}, {"sentence": "do not use the builtin development server in production", "label": "solution"}, {"sentence": "recommend", "label": "other"}, {"sentence": "proxy $shortcode$ traffic through $shortcode$ to flask", "label": "other"}, {"sentence": "this way $shortcode$ can handle all your static file which it be very good at much good than the debug server build into flask and act a a reverse proxy for your dynamic content pas those request to flask", "label": "root_cause"}, {"sentence": "here s a link to the official documentation about set up flask with apache + mod_wsgi", "label": "solution"}, {"sentence": "edit 1 clarification for @djack", "label": "other"}, {"sentence": "proxy http traffic to flask through apache2", "label": "other"}, {"sentence": "when a request come to the server on port 80 $shortcode$ or port 443 $shortcode$ a web server like apache or nginx handle the connection of the request and work out what to do with it", "label": "root_cause"}, {"sentence": "in our case a request receive should be configure to be pas through to flask on the wsgi protocol and handle by the python code", "label": "root_cause"}, {"sentence": "this be the dynamic part", "label": "other"}, {"sentence": "reverse proxy for dynamic content", "label": "other"}, {"sentence": "there be a few advantage to configure your web server like the above", "label": "other"}, {"sentence": "ssl termination the web server will be optimize to handle http request with only a little configuration", "label": "root_cause"}, {"sentence": "do not roll your own in python which be probably very insecure in comparison", "label": "root_cause"}, {"sentence": "security open a port to the internet require careful consideration of security", "label": "root_cause"}, {"sentence": "flask s development server be not design for this and could have open bug or security issue in comparison to a web server design for this purpose", "label": "root_cause"}, {"sentence": "note that a badly configure web server can also be insecure", "label": "other"}, {"sentence": "static file handle it be possible for the builtin flask web server to handle static file however this be not recommend nginx/apache be much more efficient at handle static file like image cs javascript file and will only pas dynamic request those where the content be often read from a database or the content change to be handle by the python code", "label": "root_cause"}, {"sentence": "+more", "label": "other"}, {"sentence": "this be border on scope for this question", "label": "other"}, {"sentence": "if you want more info do some research into this area", "label": "other"}]}, {"answer_body": "<p>You're looking for </p>\n\n<pre><code>torch.mm(a,b)\n</code></pre>\n\n<p>Note that <code>torch.dot()</code> behaves differently to <code>np.dot()</code>. There's been some discussion about what would be desirable <a href=\"https://github.com/pytorch/pytorch/issues/138\" rel=\"noreferrer\">here</a>. Specifically, <code>torch.dot()</code> treats both <code>a</code> and <code>b</code> as 1D vectors (irrespective of their original shape) and computes their inner product. The error is thrown, because this behaviour makes your <code>a</code> a vector of length 6 and your <code>b</code> a vector of length 2; hence their inner product can't be computed. For matrix multiplication in PyTorch, use <code>torch.mm()</code>. Numpy's <code>np.dot()</code> in contrast is more flexible; it computes the inner product for 1D arrays and performs matrix multiplication for 2D arrays. </p>\n", "title": "How to do product of matrices in PyTorch", "question_id": 44524901, "labels": [{"sentence": "you be look for $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "note that $shortcode$ behave differently to $shortcode$", "label": "other"}, {"sentence": "there be be some discussion about what would be desirable here", "label": "other"}, {"sentence": "specifically $shortcode$ treat both $shortcode$ and $shortcode$ a 1d vector irrespective of their original shape and compute their inner product", "label": "root_cause"}, {"sentence": "the error be throw because this behaviour make your $shortcode$ a vector of length 6 and your $shortcode$ a vector of length 2 hence their inner product cannot be compute", "label": "root_cause"}, {"sentence": "for matrix multiplication in pytorch use $shortcode$", "label": "solution"}, {"sentence": "numpy s $shortcode$ in contrast be more flexible it compute the inner product for 1d array and perform matrix multiplication for 2d array", "label": "other"}]}, {"answer_body": "<p><code>TensorFlow 2.0</code> now supports<code>TensorBoard</code>in<code>Jupyter</code>via magic commands (e.g <code>%tensorboard --logdir logs/train</code>). Here's a <a href=\"https://github.com/tensorflow/tensorboard/tree/master/docs/r2\" rel=\"noreferrer\">link</a> to tutorials and examples.</p>\n\n<p><strong>[EDITS 1, 2]</strong></p>\n\n<p>As @MiniQuark mentioned in a comment, we need to load the extension first(<code>%load_ext tensorboard.notebook</code>). </p>\n\n<p>Below are usage examples for using <em>graph mode</em>, <em><code>@tf.function</code></em> and <em><code>tf.keras</code></em> (in <code>tensorflow==2.0.0-alpha0</code>):</p>\n\n<h3>1. Example using <em>graph mode</em> in TF2 (via <code>tf.compat.v1.disable_eager_execution()</code>)</h3>\n\n<pre class=\"lang-py prettyprint-override\"><code>%load_ext tensorboard.notebook\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()\nfrom tensorflow.python.ops.array_ops import placeholder\nfrom tensorflow.python.training.gradient_descent import GradientDescentOptimizer\nfrom tensorflow.python.summary.writer.writer import FileWriter\n\nwith tf.name_scope('inputs'):\n   x = placeholder(tf.float32, shape=[None, 2], name='x')\n   y = placeholder(tf.int32, shape=[None], name='y')\n\nwith tf.name_scope('logits'):\n   layer = tf.keras.layers.Dense(units=2)\n   logits = layer(x)\n\nwith tf.name_scope('loss'):\n   xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n   loss_op = tf.reduce_mean(xentropy)\n\nwith tf.name_scope('optimizer'):\n   optimizer = GradientDescentOptimizer(0.01)\n   train_op = optimizer.minimize(loss_op)\n\nFileWriter('logs/train', graph=train_op.graph).close()\n%tensorboard --logdir logs/train\n</code></pre>\n\n<h3>2. Same example as above but now using <em><code>@tf.function</code></em> decorator for forward-backward passes and without disabling eager execution:</h3>\n\n<pre class=\"lang-py prettyprint-override\"><code>%load_ext tensorboard.notebook\nimport tensorflow as tf\nimport numpy as np\n\nlogdir = 'logs/'\nwriter = tf.summary.create_file_writer(logdir)\ntf.summary.trace_on(graph=True, profiler=True)\n\n@tf.function\ndef forward_and_backward(x, y, w, b, lr=tf.constant(0.01)):\n\n    with tf.name_scope('logits'):\n        logits = tf.matmul(x, w) + b\n\n    with tf.name_scope('loss'):\n        loss_fn = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            labels=y, logits=logits)\n        reduced = tf.reduce_sum(loss_fn)\n\n    with tf.name_scope('optimizer'):\n        grads = tf.gradients(reduced, [w, b])\n        _ = [x.assign(x - g*lr) for g, x in zip(grads, [w, b])]\n    return reduced\n\n# inputs\nx = tf.convert_to_tensor(np.ones([1, 2]), dtype=tf.float32)\ny = tf.convert_to_tensor(np.array([1]))\n# params\nw = tf.Variable(tf.random.normal([2, 2]), dtype=tf.float32)\nb = tf.Variable(tf.zeros([1, 2]), dtype=tf.float32)\n\nloss_val = forward_and_backward(x, y, w, b)\n\nwith writer.as_default():\n    tf.summary.trace_export(\n        name='NN',\n        step=0,\n        profiler_outdir=logdir)\n\n%tensorboard --logdir logs/\n</code></pre>\n\n<h3>3. Using <code>tf.keras</code> API:</h3>\n\n<pre class=\"lang-py prettyprint-override\"><code>%load_ext tensorboard.notebook\nimport tensorflow as tf\nimport numpy as np\nx_train = [np.ones((1, 2))]\ny_train = [np.ones(1)]\n\nmodel = tf.keras.models.Sequential([tf.keras.layers.Dense(2, input_shape=(2, ))])\n\nmodel.compile(\n    optimizer='sgd',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy'])\n\nlogdir = \"logs/\"\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n\nmodel.fit(x_train,\n          y_train,\n          batch_size=1,\n          epochs=1,\n          callbacks=[tensorboard_callback])\n\n%tensorboard --logdir logs/\n</code></pre>\n\n<p>These examples will produce something like this below the cell:</p>\n\n<p><a href=\"https://i.stack.imgur.com/JsFT5.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/JsFT5.png\" alt=\"enter image description here\"></a></p>\n", "title": "Simple way to visualize a TensorFlow graph in Jupyter?", "question_id": 38189119, "labels": [{"sentence": "$shortcode$ now supports$shortcode$in$shortcode$via magic command e g $shortcode$", "label": "solution"}, {"sentence": "here s a link to tutorial and example", "label": "solution"}, {"sentence": "[edits 1 2]", "label": "other"}, {"sentence": "a @miniquark mention in a comment we need to load the extension first $shortcode$", "label": "solution"}, {"sentence": "below be usage example for use graph mode $shortcode$ and $shortcode$ in $shortcode$", "label": "solution"}, {"sentence": "1 example use graph mode in tf2 via $shortcode$ $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "2 same example a above but now use $shortcode$ decorator for forward backward pas and without disable eager execution $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "3 use $shortcode$ api $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "these example will produce something like this below the cell", "label": "other"}]}, {"answer_body": "<h1>Docs</h1>\n\n<p>They built an exhaustive and useful tutorial -> <a href=\"https://www.tensorflow.org/guide/saved_model\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/guide/saved_model</a></p>\n\n<p>From the docs:</p>\n\n<h3>Save</h3>\n\n<pre><code># Create some variables.\nv1 = tf.get_variable(\"v1\", shape=[3], initializer = tf.zeros_initializer)\nv2 = tf.get_variable(\"v2\", shape=[5], initializer = tf.zeros_initializer)\n\ninc_v1 = v1.assign(v1+1)\ndec_v2 = v2.assign(v2-1)\n\n# Add an op to initialize the variables.\ninit_op = tf.global_variables_initializer()\n\n# Add ops to save and restore all the variables.\nsaver = tf.train.Saver()\n\n# Later, launch the model, initialize the variables, do some work, and save the\n# variables to disk.\nwith tf.Session() as sess:\n  sess.run(init_op)\n  # Do some work with the model.\n  inc_v1.op.run()\n  dec_v2.op.run()\n  # Save the variables to disk.\n  save_path = saver.save(sess, \"/tmp/model.ckpt\")\n  print(\"Model saved in path: %s\" % save_path)\n</code></pre>\n\n<h3>Restore</h3>\n\n<pre><code>tf.reset_default_graph()\n\n# Create some variables.\nv1 = tf.get_variable(\"v1\", shape=[3])\nv2 = tf.get_variable(\"v2\", shape=[5])\n\n# Add ops to save and restore all the variables.\nsaver = tf.train.Saver()\n\n# Later, launch the model, use the saver to restore variables from disk, and\n# do some work with the model.\nwith tf.Session() as sess:\n  # Restore variables from disk.\n  saver.restore(sess, \"/tmp/model.ckpt\")\n  print(\"Model restored.\")\n  # Check the values of the variables\n  print(\"v1 : %s\" % v1.eval())\n  print(\"v2 : %s\" % v2.eval())\n</code></pre>\n\n<h1>Tensorflow 2</h1>\n\n<p>This is still beta so I'd advise against for now. If you still want to go down that road here is the <a href=\"https://www.tensorflow.org/beta/guide/saved_model\" rel=\"nofollow noreferrer\"><code>tf.saved_model</code> usage guide</a></p>\n\n<h1>Tensorflow &lt; 2</h1>\n\n<h2><code>simple_save</code></h2>\n\n<p>Many good answer, for completeness I'll add my 2 cents: <strong><a href=\"https://www.tensorflow.org/programmers_guide/saved_model\" rel=\"nofollow noreferrer\">simple_save</a></strong>. Also a standalone code example using the <code>tf.data.Dataset</code> API.</p>\n\n<p>Python 3 ; Tensorflow <strong>1.14</strong></p>\n\n<pre><code>import tensorflow as tf\nfrom tensorflow.saved_model import tag_constants\n\nwith tf.Graph().as_default():\n    with tf.Session() as sess:\n        ...\n\n        # Saving\n        inputs = {\n            \"batch_size_placeholder\": batch_size_placeholder,\n            \"features_placeholder\": features_placeholder,\n            \"labels_placeholder\": labels_placeholder,\n        }\n        outputs = {\"prediction\": model_output}\n        tf.saved_model.simple_save(\n            sess, 'path/to/your/location/', inputs, outputs\n        )\n</code></pre>\n\n<p>Restoring:</p>\n\n<pre><code>graph = tf.Graph()\nwith restored_graph.as_default():\n    with tf.Session() as sess:\n        tf.saved_model.loader.load(\n            sess,\n            [tag_constants.SERVING],\n            'path/to/your/location/',\n        )\n        batch_size_placeholder = graph.get_tensor_by_name('batch_size_placeholder:0')\n        features_placeholder = graph.get_tensor_by_name('features_placeholder:0')\n        labels_placeholder = graph.get_tensor_by_name('labels_placeholder:0')\n        prediction = restored_graph.get_tensor_by_name('dense/BiasAdd:0')\n\n        sess.run(prediction, feed_dict={\n            batch_size_placeholder: some_value,\n            features_placeholder: some_other_value,\n            labels_placeholder: another_value\n        })\n</code></pre>\n\n<h1>Standalone example</h1>\n\n<p><strong><a href=\"http://vict0rsch.github.io/2018/05/17/restore-tf-model-dataset/\" rel=\"nofollow noreferrer\">Original blog post</a></strong></p>\n\n<p>The following code generates random data for the sake of the demonstration.</p>\n\n<ol>\n<li>We start by creating the placeholders. They will hold the data at runtime. From them, we create the <code>Dataset</code> and then its <code>Iterator</code>. We get the iterator's generated tensor, called <code>input_tensor</code> which will serve as input to our model.</li>\n<li>The model itself is built from <code>input_tensor</code>: a GRU-based bidirectional RNN followed by a dense classifier. Because why not.</li>\n<li>The loss is a <code>softmax_cross_entropy_with_logits</code>, optimized with <code>Adam</code>. After 2 epochs (of 2 batches each), we save the \"trained\" model with <code>tf.saved_model.simple_save</code>. If you run the code as is, then the model will be saved in a folder called <code>simple/</code> in your current working directory.</li>\n<li>In a new graph, we then restore the saved model with <code>tf.saved_model.loader.load</code>. We grab the placeholders and logits with <code>graph.get_tensor_by_name</code> and the <code>Iterator</code> initializing operation with <code>graph.get_operation_by_name</code>.</li>\n<li>Lastly we run an inference for both batches in the dataset, and check that the saved and restored model both yield the same values. They do!</li>\n</ol>\n\n<p>Code:</p>\n\n<pre><code>import os\nimport shutil\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.saved_model import tag_constants\n\n\ndef model(graph, input_tensor):\n    \"\"\"Create the model which consists of\n    a bidirectional rnn (GRU(10)) followed by a dense classifier\n\n    Args:\n        graph (tf.Graph): Tensors' graph\n        input_tensor (tf.Tensor): Tensor fed as input to the model\n\n    Returns:\n        tf.Tensor: the model's output layer Tensor\n    \"\"\"\n    cell = tf.nn.rnn_cell.GRUCell(10)\n    with graph.as_default():\n        ((fw_outputs, bw_outputs), (fw_state, bw_state)) = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw=cell,\n            cell_bw=cell,\n            inputs=input_tensor,\n            sequence_length=[10] * 32,\n            dtype=tf.float32,\n            swap_memory=True,\n            scope=None)\n        outputs = tf.concat((fw_outputs, bw_outputs), 2)\n        mean = tf.reduce_mean(outputs, axis=1)\n        dense = tf.layers.dense(mean, 5, activation=None)\n\n        return dense\n\n\ndef get_opt_op(graph, logits, labels_tensor):\n    \"\"\"Create optimization operation from model's logits and labels\n\n    Args:\n        graph (tf.Graph): Tensors' graph\n        logits (tf.Tensor): The model's output without activation\n        labels_tensor (tf.Tensor): Target labels\n\n    Returns:\n        tf.Operation: the operation performing a stem of Adam optimizer\n    \"\"\"\n    with graph.as_default():\n        with tf.variable_scope('loss'):\n            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n                    logits=logits, labels=labels_tensor, name='xent'),\n                    name=\"mean-xent\"\n                    )\n        with tf.variable_scope('optimizer'):\n            opt_op = tf.train.AdamOptimizer(1e-2).minimize(loss)\n        return opt_op\n\n\nif __name__ == '__main__':\n    # Set random seed for reproducibility\n    # and create synthetic data\n    np.random.seed(0)\n    features = np.random.randn(64, 10, 30)\n    labels = np.eye(5)[np.random.randint(0, 5, (64,))]\n\n    graph1 = tf.Graph()\n    with graph1.as_default():\n        # Random seed for reproducibility\n        tf.set_random_seed(0)\n        # Placeholders\n        batch_size_ph = tf.placeholder(tf.int64, name='batch_size_ph')\n        features_data_ph = tf.placeholder(tf.float32, [None, None, 30], 'features_data_ph')\n        labels_data_ph = tf.placeholder(tf.int32, [None, 5], 'labels_data_ph')\n        # Dataset\n        dataset = tf.data.Dataset.from_tensor_slices((features_data_ph, labels_data_ph))\n        dataset = dataset.batch(batch_size_ph)\n        iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\n        dataset_init_op = iterator.make_initializer(dataset, name='dataset_init')\n        input_tensor, labels_tensor = iterator.get_next()\n\n        # Model\n        logits = model(graph1, input_tensor)\n        # Optimization\n        opt_op = get_opt_op(graph1, logits, labels_tensor)\n\n        with tf.Session(graph=graph1) as sess:\n            # Initialize variables\n            tf.global_variables_initializer().run(session=sess)\n            for epoch in range(3):\n                batch = 0\n                # Initialize dataset (could feed epochs in Dataset.repeat(epochs))\n                sess.run(\n                    dataset_init_op,\n                    feed_dict={\n                        features_data_ph: features,\n                        labels_data_ph: labels,\n                        batch_size_ph: 32\n                    })\n                values = []\n                while True:\n                    try:\n                        if epoch &lt; 2:\n                            # Training\n                            _, value = sess.run([opt_op, logits])\n                            print('Epoch {}, batch {} | Sample value: {}'.format(epoch, batch, value[0]))\n                            batch += 1\n                        else:\n                            # Final inference\n                            values.append(sess.run(logits))\n                            print('Epoch {}, batch {} | Final inference | Sample value: {}'.format(epoch, batch, values[-1][0]))\n                            batch += 1\n                    except tf.errors.OutOfRangeError:\n                        break\n            # Save model state\n            print('\\nSaving...')\n            cwd = os.getcwd()\n            path = os.path.join(cwd, 'simple')\n            shutil.rmtree(path, ignore_errors=True)\n            inputs_dict = {\n                \"batch_size_ph\": batch_size_ph,\n                \"features_data_ph\": features_data_ph,\n                \"labels_data_ph\": labels_data_ph\n            }\n            outputs_dict = {\n                \"logits\": logits\n            }\n            tf.saved_model.simple_save(\n                sess, path, inputs_dict, outputs_dict\n            )\n            print('Ok')\n    # Restoring\n    graph2 = tf.Graph()\n    with graph2.as_default():\n        with tf.Session(graph=graph2) as sess:\n            # Restore saved values\n            print('\\nRestoring...')\n            tf.saved_model.loader.load(\n                sess,\n                [tag_constants.SERVING],\n                path\n            )\n            print('Ok')\n            # Get restored placeholders\n            labels_data_ph = graph2.get_tensor_by_name('labels_data_ph:0')\n            features_data_ph = graph2.get_tensor_by_name('features_data_ph:0')\n            batch_size_ph = graph2.get_tensor_by_name('batch_size_ph:0')\n            # Get restored model output\n            restored_logits = graph2.get_tensor_by_name('dense/BiasAdd:0')\n            # Get dataset initializing operation\n            dataset_init_op = graph2.get_operation_by_name('dataset_init')\n\n            # Initialize restored dataset\n            sess.run(\n                dataset_init_op,\n                feed_dict={\n                    features_data_ph: features,\n                    labels_data_ph: labels,\n                    batch_size_ph: 32\n                }\n\n            )\n            # Compute inference for both batches in dataset\n            restored_values = []\n            for i in range(2):\n                restored_values.append(sess.run(restored_logits))\n                print('Restored values: ', restored_values[i][0])\n\n    # Check if original inference and restored inference are equal\n    valid = all((v == rv).all() for v, rv in zip(values, restored_values))\n    print('\\nInferences match: ', valid)\n</code></pre>\n\n<p>This will print:</p>\n\n<pre><code>$ python3 save_and_restore.py\n\nEpoch 0, batch 0 | Sample value: [-0.13851789 -0.3087595   0.12804556  0.20013677 -0.08229901]\nEpoch 0, batch 1 | Sample value: [-0.00555491 -0.04339041 -0.05111827 -0.2480045  -0.00107776]\nEpoch 1, batch 0 | Sample value: [-0.19321944 -0.2104792  -0.00602257  0.07465433  0.11674127]\nEpoch 1, batch 1 | Sample value: [-0.05275984  0.05981954 -0.15913513 -0.3244143   0.10673307]\nEpoch 2, batch 0 | Final inference | Sample value: [-0.26331693 -0.13013336 -0.12553    -0.04276478  0.2933622 ]\nEpoch 2, batch 1 | Final inference | Sample value: [-0.07730117  0.11119192 -0.20817074 -0.35660955  0.16990358]\n\nSaving...\nINFO:tensorflow:Assets added to graph.\nINFO:tensorflow:No assets to write.\nINFO:tensorflow:SavedModel written to: b'/some/path/simple/saved_model.pb'\nOk\n\nRestoring...\nINFO:tensorflow:Restoring parameters from b'/some/path/simple/variables/variables'\nOk\nRestored values:  [-0.26331693 -0.13013336 -0.12553    -0.04276478  0.2933622 ]\nRestored values:  [-0.07730117  0.11119192 -0.20817074 -0.35660955  0.16990358]\n\nInferences match:  True\n</code></pre>\n", "title": "Tensorflow: how to save/restore a model?", "question_id": 33759623, "labels": [{"sentence": "doc", "label": "other"}, {"sentence": "they build an exhaustive and useful tutorial > $url$", "label": "solution"}, {"sentence": "from the doc", "label": "other"}, {"sentence": "save $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "restore $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "tensorflow 2", "label": "other"}, {"sentence": "this be still beta so i would advise against for now", "label": "other"}, {"sentence": "if you still want to go down that road here be the $shortcode$ usage guide", "label": "solution"}, {"sentence": "tensorflow < 2 $shortcode$", "label": "other"}, {"sentence": "$shortcode$", "label": "other"}, {"sentence": "many good answer for completeness i will add my 2 cent simple_save", "label": "other"}, {"sentence": "also a standalone code example use the $shortcode$ api", "label": "other"}, {"sentence": "python 3 tensorflow 1 14 $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "restore $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "standalone example", "label": "other"}, {"sentence": "original blog post", "label": "other"}, {"sentence": "the follow code generate random data for the sake of the demonstration", "label": "other"}, {"sentence": "we start by create the placeholder", "label": "solution"}, {"sentence": "they will hold the data at runtime", "label": "root_cause"}, {"sentence": "from them we create the $shortcode$ and then it $shortcode$", "label": "solution"}, {"sentence": "we get the iterator s generate tensor call $shortcode$ which will serve a input to our model", "label": "root_cause"}, {"sentence": "the model itself be build from $shortcode$ a gru base bidirectional rnn follow by a dense classifier", "label": "root_cause"}, {"sentence": "because why not", "label": "other"}, {"sentence": "the loss be a $shortcode$ optimize with $shortcode$", "label": "other"}, {"sentence": "after 2 epoch of 2 batch each we save the train model with $shortcode$", "label": "other"}, {"sentence": "if you run the code a be then the model will be save in a folder call $shortcode$ in your current work directory", "label": "other"}, {"sentence": "in a new graph we then restore the save model with $shortcode$", "label": "other"}, {"sentence": "we grab the placeholder and logits with $shortcode$ and the $shortcode$ initialize operation with $shortcode$", "label": "other"}, {"sentence": "lastly we run an inference for both batch in the dataset and check that the save and restore model both yield the same value", "label": "other"}, {"sentence": "they do", "label": "other"}, {"sentence": "code $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "this will print $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<p><a href=\"http://flask.pocoo.org/docs/api/#flask.Flask.run\" rel=\"nofollow noreferrer\"><code>flask.Flask.run</code></a> accepts additional keyword arguments (<code>**options</code>) that it forwards to <a href=\"http://werkzeug.pocoo.org/docs/serving/#werkzeug.serving.run_simple\" rel=\"nofollow noreferrer\"><code>werkzeug.serving.run_simple</code></a> - two of those arguments are <code>threaded</code> (a boolean) and <code>processes</code> (which you can set to a number greater than one to have werkzeug spawn more than one process to handle requests).</p>\n\n<p><code>threaded</code> defaults to <code>True</code> as of Flask 1.0, so for the latest versions of Flask, the default development server will be able to serve multiple clients simultaneously by default. For older versions of Flask, you can explicitly pass <code>threaded=True</code> to enable this behaviour.</p>\n\n<p>For example, you can do</p>\n\n<pre><code>if __name__ == '__main__':\n    app.run(threaded=True)\n</code></pre>\n\n<p>to handle multiple clients using threads in a way compatible with old Flask versions, or</p>\n\n<pre><code>if __name__ == '__main__':\n    app.run(threaded=False, processes=3)\n</code></pre>\n\n<p>to tell Werkzeug to spawn three processes to handle incoming requests, or just</p>\n\n<pre><code>if __name__ == '__main__':\n    app.run()\n</code></pre>\n\n<p>to handle multiple clients using threads if you know that you will be using Flask 1.0 or later.</p>\n\n<p>That being said, Werkzeug's <code>serving.run_simple</code> wraps the standard library's <a href=\"http://docs.python.org/2/library/wsgiref.html\" rel=\"nofollow noreferrer\"><code>wsgiref</code></a> package - and that package contains a reference implementation of WSGI, not a production-ready web server.  If you are going to use Flask in production (assuming that \"production\" is not a low-traffic internal application with no more than 10 concurrent users) make sure to stand it up behind a real web server (see the section of Flask's docs entitled <a href=\"http://flask.pocoo.org/docs/deploying/\" rel=\"nofollow noreferrer\">Deployment Options</a> for some suggested methods).</p>\n", "title": "Can I serve multiple clients using just Flask app.run() as standalone?", "question_id": 14814201, "labels": [{"sentence": "$shortcode$ accept additional keyword argument $shortcode$ that it forward to $shortcode$ two of those argument be $shortcode$ a boolean and $shortcode$ which you can set to a number great than one to have werkzeug spawn more than one process to handle request", "label": "root_cause"}, {"sentence": "$shortcode$ default to $shortcode$ a of flask 1 0 so for the late version of flask the default development server will be able to serve multiple client simultaneously by default", "label": "solution"}, {"sentence": "for old version of flask you can explicitly pas $shortcode$ to enable this behaviour", "label": "solution"}, {"sentence": "for example you can do $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "to handle multiple client use thread in a way compatible with old flask version or $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "to tell werkzeug to spawn three process to handle incoming request or just $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "to handle multiple client use thread if you know that you will be use flask 1 0 or late", "label": "other"}, {"sentence": "that be say werkzeug s $shortcode$ wrap the standard library s $shortcode$ package and that package contain a reference implementation of wsgi not a production ready web server", "label": "root_cause"}, {"sentence": "if you be go to use flask in production assume that production be not a low traffic internal application with no more than 10 concurrent user make sure to stand it up behind a real web server see the section of flask s doc entitle deployment option for some suggest method", "label": "root_cause"}]}, {"answer_body": "<p>Right-click the the left side (with line numbering) and choose your soft wrap option:</p>\n\n<p><a href=\"https://i.stack.imgur.com/xTwsC.jpg\"><img src=\"https://i.stack.imgur.com/xTwsC.jpg\" alt=\"enter image description here\"></a></p>\n", "title": "Pycharm to toggle soft wrap in code editor", "question_id": 38585204, "labels": [{"sentence": "right click the the leave side with line number and choose your soft wrap option", "label": "solution"}]}, {"answer_body": "<p>Use  <a href=\"http://pytorch.org/docs/torch.html?highlight=unsqueeze#torch.unsqueeze\" rel=\"noreferrer\">torch.unsqueeze(input, dim, out=None)</a></p>\n\n<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; a = torch.Tensor([1,2,3,4,5])\n&gt;&gt;&gt; a\n\n 1\n 2\n 3\n 4\n 5\n[torch.FloatTensor of size 5]\n\n&gt;&gt;&gt; a = a.unsqueeze(0)\n&gt;&gt;&gt; a\n\n 1  2  3  4  5\n[torch.FloatTensor of size 1x5]\n</code></pre>\n", "title": "Pytorch reshape tensor dimension", "question_id": 43328632, "labels": [{"sentence": "use torch unsqueeze input dim out=none $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<blockquote>\n  <h2>How can I merge two Python dictionaries in a single expression?</h2>\n</blockquote>\n\n<p>For dictionaries <code>x</code> and <code>y</code>, <code>z</code> becomes a shallowly merged dictionary with values from <code>y</code> replacing those from <code>x</code>.</p>\n\n<ul>\n<li><p>In Python 3.5 or greater:</p>\n\n<pre><code>z = {**x, **y}\n</code></pre></li>\n<li><p>In Python 2, (or 3.4 or lower) write a function:</p>\n\n<pre><code>def merge_two_dicts(x, y):\n    z = x.copy()   # start with x's keys and values\n    z.update(y)    # modifies z with y's keys and values &amp; returns None\n    return z\n</code></pre>\n\n<p>and now:</p>\n\n<pre><code>z = merge_two_dicts(x, y)\n</code></pre></li>\n</ul>\n\n<h3>Explanation</h3>\n\n<p>Say you have two dicts and you want to merge them into a new dict without altering the original dicts:</p>\n\n<pre><code>x = {'a': 1, 'b': 2}\ny = {'b': 3, 'c': 4}\n</code></pre>\n\n<p>The desired result is to get a new dictionary (<code>z</code>) with the values merged, and the second dict's values overwriting those from the first.</p>\n\n<pre><code>&gt;&gt;&gt; z\n{'a': 1, 'b': 3, 'c': 4}\n</code></pre>\n\n<p>A new syntax for this, proposed in <a href=\"https://www.python.org/dev/peps/pep-0448\" rel=\"noreferrer\">PEP 448</a> and <a href=\"https://mail.python.org/pipermail/python-dev/2015-February/138564.html\" rel=\"noreferrer\">available as of Python 3.5</a>, is </p>\n\n<pre><code>z = {**x, **y}\n</code></pre>\n\n<p>And it is indeed a single expression. </p>\n\n<p>Note that we can merge in with literal notation as well:</p>\n\n<pre><code>z = {**x, 'foo': 1, 'bar': 2, **y}\n</code></pre>\n\n<p>and now: </p>\n\n<pre><code>&gt;&gt;&gt; z\n{'a': 1, 'b': 3, 'foo': 1, 'bar': 2, 'c': 4}\n</code></pre>\n\n<p>It is now showing as implemented in the <a href=\"https://www.python.org/dev/peps/pep-0478/#features-for-3-5\" rel=\"noreferrer\">release schedule for 3.5, PEP 478</a>, and it has now made its way into <a href=\"https://docs.python.org/dev/whatsnew/3.5.html#pep-448-additional-unpacking-generalizations\" rel=\"noreferrer\">What's New in Python 3.5</a> document.</p>\n\n<p>However, since many organizations are still on Python 2, you may wish to do this in a backwards compatible way. The classically Pythonic way, available in Python 2 and Python 3.0-3.4, is to do this as a two-step process:</p>\n\n<pre><code>z = x.copy()\nz.update(y) # which returns None since it mutates z\n</code></pre>\n\n<p>In both approaches, <code>y</code> will come second and its values will replace <code>x</code>'s values, thus <code>'b'</code> will point to <code>3</code> in our final result.</p>\n\n<h2>Not yet on Python 3.5, but want a <em>single expression</em></h2>\n\n<p>If you are not yet on Python 3.5, or need to write backward-compatible code, and you want this in a <em>single expression</em>, the most performant while correct approach is to put it in a function:</p>\n\n<pre><code>def merge_two_dicts(x, y):\n    \"\"\"Given two dicts, merge them into a new dict as a shallow copy.\"\"\"\n    z = x.copy()\n    z.update(y)\n    return z\n</code></pre>\n\n<p>and then you have a single expression:</p>\n\n<pre><code>z = merge_two_dicts(x, y)\n</code></pre>\n\n<p>You can also make a function to merge an undefined number of dicts, from zero to a very large number:</p>\n\n<pre><code>def merge_dicts(*dict_args):\n    \"\"\"\n    Given any number of dicts, shallow copy and merge into a new dict,\n    precedence goes to key value pairs in latter dicts.\n    \"\"\"\n    result = {}\n    for dictionary in dict_args:\n        result.update(dictionary)\n    return result\n</code></pre>\n\n<p>This function will work in Python 2 and 3 for all dicts. e.g. given dicts <code>a</code> to <code>g</code>:</p>\n\n<pre><code>z = merge_dicts(a, b, c, d, e, f, g) \n</code></pre>\n\n<p>and key value pairs in <code>g</code> will take precedence over dicts <code>a</code> to <code>f</code>, and so on.</p>\n\n<h2>Critiques of Other Answers</h2>\n\n<p>Don't use what you see in the formerly accepted answer:</p>\n\n<pre><code>z = dict(x.items() + y.items())\n</code></pre>\n\n<p>In Python 2, you create two lists in memory for each dict, create a third list in memory with length equal to the length of the first two put together, and then discard all three lists to create the dict. <strong>In Python 3, this will fail</strong> because you're adding two <code>dict_items</code> objects together, not two lists - </p>\n\n<pre><code>&gt;&gt;&gt; c = dict(a.items() + b.items())\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: unsupported operand type(s) for +: 'dict_items' and 'dict_items'\n</code></pre>\n\n<p>and you would have to explicitly create them as lists, e.g. <code>z = dict(list(x.items()) + list(y.items()))</code>. This is a waste of resources and computation power. </p>\n\n<p>Similarly, taking the union of <code>items()</code> in Python 3 (<code>viewitems()</code> in Python 2.7) will also fail when values are unhashable objects (like lists, for example). Even if your values are hashable, <strong>since sets are semantically unordered, the behavior is undefined in regards to precedence. So don't do this:</strong></p>\n\n<pre><code>&gt;&gt;&gt; c = dict(a.items() | b.items())\n</code></pre>\n\n<p>This example demonstrates what happens when values are unhashable:</p>\n\n<pre><code>&gt;&gt;&gt; x = {'a': []}\n&gt;&gt;&gt; y = {'b': []}\n&gt;&gt;&gt; dict(x.items() | y.items())\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: unhashable type: 'list'\n</code></pre>\n\n<p>Here's an example where y should have precedence, but instead the value from x is retained due to the arbitrary order of sets:</p>\n\n<pre><code>&gt;&gt;&gt; x = {'a': 2}\n&gt;&gt;&gt; y = {'a': 1}\n&gt;&gt;&gt; dict(x.items() | y.items())\n{'a': 2}\n</code></pre>\n\n<p>Another hack you should not use:</p>\n\n<pre><code>z = dict(x, **y)\n</code></pre>\n\n<p>This uses the <code>dict</code> constructor, and is very fast and memory efficient (even slightly more-so than our two-step process) but unless you know precisely what is happening here (that is, the second dict is being passed as keyword arguments to the dict constructor), it's difficult to read, it's not the intended usage, and so it is not Pythonic. </p>\n\n<p>Here's an example of the usage being <a href=\"https://code.djangoproject.com/attachment/ticket/13357/django-pypy.2.diff\" rel=\"noreferrer\">remediated in django</a>.</p>\n\n<p>Dicts are intended to take hashable keys (e.g. frozensets or tuples), but <strong>this method fails in Python 3 when keys are not strings.</strong></p>\n\n<pre><code>&gt;&gt;&gt; c = dict(a, **b)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: keyword arguments must be strings\n</code></pre>\n\n<p>From the <a href=\"https://mail.python.org/pipermail/python-dev/2010-April/099459.html\" rel=\"noreferrer\">mailing list</a>, Guido van Rossum, the creator of the language, wrote:</p>\n\n<blockquote>\n  <p>I am fine with\n  declaring dict({}, **{1:3}) illegal, since after all it is abuse of\n  the ** mechanism.</p>\n</blockquote>\n\n<p>and </p>\n\n<blockquote>\n  <p>Apparently dict(x, **y) is going around as \"cool hack\" for \"call\n  x.update(y) and return x\". Personally I find it more despicable than\n  cool.</p>\n</blockquote>\n\n<p>It is my understanding (as well as the understanding of the <a href=\"https://mail.python.org/pipermail/python-dev/2010-April/099485.html\" rel=\"noreferrer\">creator of the language</a>) that the intended usage for <code>dict(**y)</code> is for creating dicts for readability purposes, e.g.:</p>\n\n<pre><code>dict(a=1, b=10, c=11)\n</code></pre>\n\n<p>instead of </p>\n\n<pre><code>{'a': 1, 'b': 10, 'c': 11}\n</code></pre>\n\n<h2>Response to comments</h2>\n\n<blockquote>\n  <p>Despite what Guido says, <code>dict(x, **y)</code> is in line with the dict specification, which btw. works for both Python 2 and 3. The fact that this only works for string keys is a direct consequence of how keyword parameters work and not a short-comming of dict. Nor is using the ** operator in this place an abuse of the mechanism, in fact ** was designed precisely to pass dicts as keywords. </p>\n</blockquote>\n\n<p>Again, it doesn't work for 3 when keys are non-strings. The implicit calling contract is that namespaces take ordinary dicts, while users must only pass keyword arguments that are strings. All other callables enforced it. <code>dict</code> broke this consistency in Python 2:</p>\n\n<pre><code>&gt;&gt;&gt; foo(**{('a', 'b'): None})\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: foo() keywords must be strings\n&gt;&gt;&gt; dict(**{('a', 'b'): None})\n{('a', 'b'): None}\n</code></pre>\n\n<p>This inconsistency was bad given other implementations of Python (Pypy, Jython, IronPython). Thus it was fixed in Python 3, as this usage could be a breaking change.</p>\n\n<p>I submit to you that it is malicious incompetence to intentionally write code that only works in one version of a language or that only works given certain arbitrary constraints.</p>\n\n<p>More comments:</p>\n\n<blockquote>\n  <p><code>dict(x.items() + y.items())</code> is still the most readable solution for Python 2. Readability counts. </p>\n</blockquote>\n\n<p>My response: <code>merge_two_dicts(x, y)</code> actually seems much clearer to me, if we're actually concerned about readability. And it is not forward compatible, as Python 2 is increasingly deprecated.</p>\n\n<blockquote>\n  <p><code>{**x, **y}</code> does not seem to handle nested dictionaries. the contents of nested keys are simply overwritten, not merged [...] I ended up being burnt by these answers that do not merge recursively and I was surprised no one mentioned it. In my interpretation of the word \"merging\" these answers describe \"updating one dict with another\", and not merging.</p>\n</blockquote>\n\n<p>Yes. I must refer you back to the question, which is asking for a <em>shallow</em> merge of <strong><em>two</em></strong> dictionaries, with the first's values being overwritten by the second's - in a single expression.</p>\n\n<p>Assuming two dictionary of dictionaries, one might recursively merge them in a single function, but you should be careful not to modify the dicts from either source, and the surest way to avoid that is to make a copy when assigning values. As keys must be hashable and are usually therefore immutable, it is pointless to copy them:</p>\n\n<pre><code>from copy import deepcopy\n\ndef dict_of_dicts_merge(x, y):\n    z = {}\n    overlapping_keys = x.keys() &amp; y.keys()\n    for key in overlapping_keys:\n        z[key] = dict_of_dicts_merge(x[key], y[key])\n    for key in x.keys() - overlapping_keys:\n        z[key] = deepcopy(x[key])\n    for key in y.keys() - overlapping_keys:\n        z[key] = deepcopy(y[key])\n    return z\n</code></pre>\n\n<p>Usage:</p>\n\n<pre><code>&gt;&gt;&gt; x = {'a':{1:{}}, 'b': {2:{}}}\n&gt;&gt;&gt; y = {'b':{10:{}}, 'c': {11:{}}}\n&gt;&gt;&gt; dict_of_dicts_merge(x, y)\n{'b': {2: {}, 10: {}}, 'a': {1: {}}, 'c': {11: {}}}\n</code></pre>\n\n<p>Coming up with contingencies for other value types is far beyond the scope of this question, so I will point you at <a href=\"https://stackoverflow.com/a/24088493/541136\">my answer to the canonical question on a \"Dictionaries of dictionaries merge\"</a>.</p>\n\n<h2>Less Performant But Correct Ad-hocs</h2>\n\n<p>These approaches are less performant, but they will provide correct behavior.\nThey will be <em>much less</em> performant than <code>copy</code> and <code>update</code> or the new unpacking because they iterate through each key-value pair at a higher level of abstraction, but they <em>do</em> respect the order of precedence (latter dicts have precedence)</p>\n\n<p>You can also chain the dicts manually inside a dict comprehension:</p>\n\n<pre><code>{k: v for d in dicts for k, v in d.items()} # iteritems in Python 2.7\n</code></pre>\n\n<p>or in python 2.6 (and perhaps as early as 2.4 when generator expressions were introduced):</p>\n\n<pre><code>dict((k, v) for d in dicts for k, v in d.items())\n</code></pre>\n\n<p><code>itertools.chain</code> will chain the iterators over the key-value pairs in the correct order:</p>\n\n<pre><code>import itertools\nz = dict(itertools.chain(x.iteritems(), y.iteritems()))\n</code></pre>\n\n<h2>Performance Analysis</h2>\n\n<p>I'm only going to do the performance analysis of the usages known to behave correctly. </p>\n\n<pre><code>import timeit\n</code></pre>\n\n<p>The following is done on Ubuntu 14.04</p>\n\n<p>In Python 2.7 (system Python):</p>\n\n<pre><code>&gt;&gt;&gt; min(timeit.repeat(lambda: merge_two_dicts(x, y)))\n0.5726828575134277\n&gt;&gt;&gt; min(timeit.repeat(lambda: {k: v for d in (x, y) for k, v in d.items()} ))\n1.163769006729126\n&gt;&gt;&gt; min(timeit.repeat(lambda: dict(itertools.chain(x.iteritems(), y.iteritems()))))\n1.1614501476287842\n&gt;&gt;&gt; min(timeit.repeat(lambda: dict((k, v) for d in (x, y) for k, v in d.items())))\n2.2345519065856934\n</code></pre>\n\n<p>In Python 3.5 (deadsnakes PPA):</p>\n\n<pre><code>&gt;&gt;&gt; min(timeit.repeat(lambda: {**x, **y}))\n0.4094954460160807\n&gt;&gt;&gt; min(timeit.repeat(lambda: merge_two_dicts(x, y)))\n0.7881555100320838\n&gt;&gt;&gt; min(timeit.repeat(lambda: {k: v for d in (x, y) for k, v in d.items()} ))\n1.4525277839857154\n&gt;&gt;&gt; min(timeit.repeat(lambda: dict(itertools.chain(x.items(), y.items()))))\n2.3143140770262107\n&gt;&gt;&gt; min(timeit.repeat(lambda: dict((k, v) for d in (x, y) for k, v in d.items())))\n3.2069112799945287\n</code></pre>\n\n<h2>Resources on Dictionaries</h2>\n\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/327311/how-are-pythons-built-in-dictionaries-implemented/44509302#44509302\">My explanation of Python's <strong>dictionary implementation</strong>, updated for 3.6.</a></li>\n<li><a href=\"https://stackoverflow.com/questions/1024847/add-new-keys-to-a-dictionary/27208535#27208535\">Answer on how to add new keys to a dictionary</a></li>\n<li><a href=\"https://stackoverflow.com/questions/209840/map-two-lists-into-a-dictionary-in-python/33737067#33737067\">Mapping two lists into a dictionary</a></li>\n<li>The official Python <a href=\"https://docs.python.org/3/tutorial/datastructures.html#dictionaries\" rel=\"noreferrer\">docs on dictionaries</a> </li>\n<li><a href=\"https://www.youtube.com/watch?v=66P5FMkWoVU\" rel=\"noreferrer\">The Dictionary Even Mightier</a> - talk by Brandon Rhodes at Pycon 2017</li>\n<li><a href=\"https://www.youtube.com/watch?v=npw4s1QTmPg\" rel=\"noreferrer\">Modern Python Dictionaries, A Confluence of Great Ideas</a> - talk by Raymond Hettinger at Pycon 2017</li>\n</ul>\n", "title": "How to merge two dictionaries in a single expression?", "question_id": 38987, "labels": [{"sentence": "how can i merge two python dictionary in a single expression", "label": "other"}, {"sentence": "for dictionary $shortcode$ and $shortcode$ $shortcode$ become a shallowly merge dictionary with value from $shortcode$ replace those from $shortcode$", "label": "solution"}, {"sentence": "in python 3 5 or great $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "in python 2 or 3 4 or low write a function $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "and now $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "explanation", "label": "root_cause"}, {"sentence": "say you have two dicts and you want to merge them into a new dict without alter the original dicts $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "the desire result be to get a new dictionary $shortcode$ with the value merge and the second dict s value overwrite those from the first $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "a new syntax for this propose in pep 448 and available a of python 3 5 be $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "and it be indeed a single expression", "label": "other"}, {"sentence": "note that we can merge in with literal notation a well $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "and now $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "it be now show a implement in the release schedule for 3 5 pep 478 and it have now make it way into what be new in python 3 5 document", "label": "other"}, {"sentence": "however since many organization be still on python 2 you may wish to do this in a backwards compatible way", "label": "other"}, {"sentence": "the classically pythonic way available in python 2 and python 3 0 3 4 be to do this a a two step process $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "in both approach $shortcode$ will come second and it value will replace $shortcode$ s value thus $shortcode$ will point to $shortcode$ in our final result", "label": "root_cause"}, {"sentence": "not yet on python 3 5 but want a single expression", "label": "other"}, {"sentence": "if you be not yet on python 3 5 or need to write backward compatible code and you want this in a single expression the most performant while correct approach be to put it in a function $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "and then you have a single expression $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "you can also make a function to merge an undefined number of dicts from zero to a very large number $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "this function will work in python 2 and 3 for all dicts", "label": "other"}, {"sentence": "e g", "label": "other"}, {"sentence": "give dicts $shortcode$ to $shortcode$ $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "and key value pair in $shortcode$ will take precedence over dicts $shortcode$ to $shortcode$ and so on", "label": "other"}, {"sentence": "critique of other answer", "label": "other"}, {"sentence": "do not use what you see in the formerly accept answer $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "in python 2 you create two list in memory for each dict create a third list in memory with length equal to the length of the first two put together and then discard all three list to create the dict", "label": "solution"}, {"sentence": "in python 3 this will fail because you be add two $shortcode$ object together not two list $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "and you would have to explicitly create them a list e g", "label": "other"}, {"sentence": "$shortcode$", "label": "other"}, {"sentence": "this be a waste of resource and computation power", "label": "root_cause"}, {"sentence": "similarly take the union of $shortcode$ in python 3 $shortcode$ in python 2 7 will also fail when value be unhashable object like list for example", "label": "root_cause"}, {"sentence": "even if your value be hashable since set be semantically unordered the behavior be undefined in regard to precedence", "label": "root_cause"}, {"sentence": "so do not do this $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "this example demonstrate what happen when value be unhashable $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "here s an example where y should have precedence but instead the value from x be retain due to the arbitrary order of set $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "another hack you should not use $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "this use the $shortcode$ constructor and be very fast and memory efficient even slightly more so than our two step process but unless you know precisely what be happen here that be the second dict be be pas a keyword argument to the dict constructor it be difficult to read it be not the intend usage and so it be not pythonic", "label": "root_cause"}, {"sentence": "here s an example of the usage be remediate in django", "label": "other"}, {"sentence": "dicts be intend to take hashable key e g", "label": "other"}, {"sentence": "frozensets or tuples but this method fail in python 3 when key be not string $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "from the mail list guido van rossum the creator of the language write", "label": "other"}, {"sentence": "i be fine with", "label": "other"}, {"sentence": "declare dict {} {1 3} illegal since after all it be abuse of", "label": "other"}, {"sentence": "the mechanism", "label": "other"}, {"sentence": "and", "label": "other"}, {"sentence": "apparently dict x y be go around a cool hack for call", "label": "other"}, {"sentence": "x update y and return x", "label": "other"}, {"sentence": "personally i find it more despicable than", "label": "other"}, {"sentence": "cool", "label": "other"}, {"sentence": "it be my understand a well a the understand of the creator of the language that the intend usage for $shortcode$ be for create dicts for readability purpose e g $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "instead of $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "response to comment", "label": "other"}, {"sentence": "despite what guido say $shortcode$ be in line with the dict specification which btw", "label": "other"}, {"sentence": "work for both python 2 and 3", "label": "solution"}, {"sentence": "the fact that this only work for string key be a direct consequence of how keyword parameter work and not a short comming of dict", "label": "root_cause"}, {"sentence": "nor be use the operator in this place an abuse of the mechanism in fact be design precisely to pas dicts a keywords", "label": "root_cause"}, {"sentence": "again it do not work for 3 when key be non string", "label": "other"}, {"sentence": "the implicit call contract be that namespaces take ordinary dicts while user must only pas keyword argument that be string", "label": "root_cause"}, {"sentence": "all other callables enforce it", "label": "other"}, {"sentence": "$shortcode$ break this consistency in python 2 $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "this inconsistency be bad give other implementation of python pypy jython ironpython", "label": "other"}, {"sentence": "thus it be fix in python 3 a this usage could be a break change", "label": "other"}, {"sentence": "i submit to you that it be malicious incompetence to intentionally write code that only work in one version of a language or that only work give certain arbitrary constraint", "label": "root_cause"}, {"sentence": "more comment", "label": "other"}, {"sentence": "$shortcode$ be still the most readable solution for python 2", "label": "other"}, {"sentence": "readability count", "label": "other"}, {"sentence": "my response $shortcode$ actually seem much clear to me if we be actually concern about readability", "label": "other"}, {"sentence": "and it be not forward compatible a python 2 be increasingly deprecate", "label": "other"}, {"sentence": "$shortcode$ do not seem to handle nest dictionary", "label": "other"}, {"sentence": "the content of nest key be simply overwrite not merge [ ] i end up be burn by these answer that do not merge recursively and i be surprise no one mention it", "label": "root_cause"}, {"sentence": "in my interpretation of the word merge these answer describe update one dict with another and not merge", "label": "other"}, {"sentence": "yes", "label": "other"}, {"sentence": "i must refer you back to the question which be ask for a shallow merge of two dictionary with the first s value be overwrite by the second s in a single expression", "label": "other"}, {"sentence": "assume two dictionary of dictionary one might recursively merge them in a single function but you should be careful not to modify the dicts from either source and the sure way to avoid that be to make a copy when assign value", "label": "root_cause"}, {"sentence": "a key must be hashable and be usually therefore immutable it be pointless to copy them $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "usage $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "come up with contingency for other value type be far beyond the scope of this question so i will point you at my answer to the canonical question on a dictionary of dictionary merge", "label": "other"}, {"sentence": "le performant but correct ad hocs", "label": "other"}, {"sentence": "these approach be le performant but they will provide correct behavior", "label": "other"}, {"sentence": "they will be much le performant than $shortcode$ and $shortcode$ or the new unpack because they iterate through each key value pair at a high level of abstraction but they do respect the order of precedence latter dicts have precedence", "label": "root_cause"}, {"sentence": "you can also chain the dicts manually inside a dict comprehension $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "or in python 2 6 and perhaps a early a 2 4 when generator expression be introduce $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "$shortcode$ will chain the iterators over the key value pair in the correct order $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "performance analysis", "label": "other"}, {"sentence": "i be only go to do the performance analysis of the usage know to behave correctly $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "the follow be do on ubuntu 14 04", "label": "other"}, {"sentence": "in python 2 7 system python $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "in python 3 5 deadsnakes ppa $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "resource on dictionary", "label": "other"}, {"sentence": "my explanation of python s dictionary implementation update for 3 6", "label": "other"}, {"sentence": "answer on how to add new key to a dictionary", "label": "other"}, {"sentence": "map two list into a dictionary", "label": "other"}, {"sentence": "the official python doc on dictionary", "label": "other"}, {"sentence": "the dictionary even mighty talk by brandon rhodes at pycon 2017", "label": "other"}, {"sentence": "modern python dictionary a confluence of great idea talk by raymond hettinger at pycon 2017", "label": "other"}]}, {"answer_body": "<h2>Units:</h2>\n\n<blockquote>\n  <p>The amount of \"neurons\", or \"cells\", or whatever the layer has inside it.   </p>\n</blockquote>\n\n<p>It's a property of each layer, and yes, it's related to the output shape (as we will see later). In your picture, except for the input layer, which is conceptually different from other layers, you have: </p>\n\n<ul>\n<li>Hidden layer 1: 4 units (4 neurons)   </li>\n<li>Hidden layer 2: 4 units     </li>\n<li>Last layer: 1 unit</li>\n</ul>\n\n<h2>Shapes</h2>\n\n<p>Shapes are consequences of the model's configuration. Shapes are tuples representing how many elements an array or tensor has in each dimension. </p>\n\n<p><strong>Ex:</strong> a shape <code>(30,4,10)</code> means an array or tensor with 3 dimensions, containing 30 elements in the first dimension, 4 in the second and 10 in the third, totaling 30*4*10 = 1200 elements or numbers.   </p>\n\n<h2>The input shape</h2>\n\n<p>What flows between layers are tensors. Tensors can be seen as matrices, with shapes.   </p>\n\n<p>In Keras, the input layer itself is not a layer, but a tensor. It's the starting tensor you send to the first hidden layer. This tensor must have the same shape as your training data. </p>\n\n<p><strong>Example:</strong> if you have 30 images of 50x50 pixels in RGB (3 channels), the shape of your input data is <code>(30,50,50,3)</code>. Then your input layer tensor, must have this shape (see details in the \"shapes in keras\" section).   </p>\n\n<p>Each type of layer requires the input with a certain number of dimensions:</p>\n\n<ul>\n<li><code>Dense</code> layers require inputs as <code>(batch_size, input_size)</code>    \n\n<ul>\n<li>or <code>(batch_size, optional,...,optional, input_size)</code>   </li>\n</ul></li>\n<li>2D convolutional layers need inputs as:\n\n<ul>\n<li>if using <code>channels_last</code>: <code>(batch_size, imageside1, imageside2, channels)</code>    </li>\n<li>if using <code>channels_first</code>: <code>(batch_size, channels, imageside1, imageside2)</code>   </li>\n</ul></li>\n<li>1D convolutions and recurrent layers use <code>(batch_size, sequence_length, features)</code>   \n\n<ul>\n<li><a href=\"https://stackoverflow.com/a/50235563/2097240\">Details on how to prepare data for recurrent layers</a></li>\n</ul></li>\n</ul>\n\n<p>Now, the input shape is the only one you must define, because your model cannot know it. Only you know that, based on your training data.   </p>\n\n<p>All the other shapes are calculated automatically based on the units and particularities of each layer.  </p>\n\n<h2>Relation between shapes and units - The output shape</h2>\n\n<p>Given the input shape, all other shapes are results of layers calculations.   </p>\n\n<p>The \"units\" of each layer will define the output shape (the shape of the tensor that is produced by the layer and that will be the input of the next layer).  </p>\n\n<p>Each type of layer works in a particular way. Dense layers have output shape based on \"units\", convolutional layers have output shape based on \"filters\". But it's always based on some layer property. (See the documentation for what each layer outputs)   </p>\n\n<p>Let's show what happens with \"Dense\" layers, which is the type shown in your graph.   </p>\n\n<p>A dense layer has an output shape of <code>(batch_size,units)</code>. So, yes, units, the property of the layer, also defines the output shape.    </p>\n\n<ul>\n<li>Hidden layer 1: 4 units, output shape: <code>(batch_size,4)</code>.   </li>\n<li>Hidden layer 2: 4 units, output shape: <code>(batch_size,4)</code>.     </li>\n<li>Last layer: 1 unit, output shape: <code>(batch_size,1)</code>.      </li>\n</ul>\n\n<h2>Weights</h2>\n\n<p>Weights will be entirely automatically calculated based on the input and the output shapes. Again, each type of layer works in a certain way. But the weights will be a matrix capable of transforming the input shape into the output shape by some mathematical operation. </p>\n\n<p>In a dense layer, weights multiply all inputs. It's a matrix with one column per input and one row per unit, but this is often not important for basic works. </p>\n\n<p>In the image, if each arrow had a multiplication number on it, all numbers together would form the weight matrix.</p>\n\n<h2>Shapes in Keras</h2>\n\n<p>Earlier, I gave an example of 30 images, 50x50 pixels and 3 channels, having an input shape of <code>(30,50,50,3)</code>.   </p>\n\n<p>Since the input shape is the only one you need to define, Keras will demand it in the first layer. </p>\n\n<p>But in this definition, Keras ignores the first dimension, which is the batch size. Your model should be able to deal with any batch size, so you define only the other dimensions:</p>\n\n<pre><code>input_shape = (50,50,3)\n    #regardless of how many images I have, each image has this shape        \n</code></pre>\n\n<p>Optionally, or when it's required by certain kinds of models, you can pass the shape containing the batch size via <code>batch_input_shape=(30,50,50,3)</code> or <code>batch_shape=(30,50,50,3)</code>. This limits your training possibilities to this unique batch size, so it should be used only when really required.</p>\n\n<p>Either way you choose, tensors in the model will have the batch dimension.</p>\n\n<p>So, even if you used <code>input_shape=(50,50,3)</code>, when keras sends you messages, or when you print the model summary, it will show <code>(None,50,50,3)</code>.</p>\n\n<p>The first dimension is the batch size, it's <code>None</code> because it can vary depending on how many examples you give for training. (If you defined the batch size explicitly, then the number you defined will appear instead of <code>None</code>)</p>\n\n<p>Also, in advanced works, when you actually operate directly on the tensors (inside Lambda layers or in the loss function, for instance), the batch size dimension will be there.   </p>\n\n<ul>\n<li>So, when defining the input shape, you ignore the batch size: <code>input_shape=(50,50,3)</code>   </li>\n<li>When doing operations directly on tensors, the shape will be again <code>(30,50,50,3)</code>    </li>\n<li>When keras sends you a message, the shape will be <code>(None,50,50,3)</code> or <code>(30,50,50,3)</code>, depending on what type of message it sends you.   </li>\n</ul>\n\n<h1>Dim</h1>\n\n<p>And in the end, what is <code>dim</code>?   </p>\n\n<p>If your input shape has only one dimension, you don't need to give it as a tuple, you give <code>input_dim</code> as a scalar number. </p>\n\n<p>So, in your model, where your input layer has 3 elements, you can use any of these two:   </p>\n\n<ul>\n<li><code>input_shape=(3,)</code> -- The comma is necessary when you have only one dimension    </li>\n<li><code>input_dim = 3</code>    </li>\n</ul>\n\n<p>But when dealing directly with the tensors, often <code>dim</code> will refer to how many dimensions a tensor has. For instance a tensor with shape (25,10909) has 2 dimensions. </p>\n\n<hr>\n\n<h2>Defining your image in Keras</h2>\n\n<p>Keras has two ways of doing it, <code>Sequential</code> models, or the functional API <code>Model</code>. I don't like using the sequential model, later you will have to forget it anyway because you will want models with branches.  </p>\n\n<p>PS: here I ignored other aspects, such as activation functions.</p>\n\n<p><strong>With the Sequential model</strong>:</p>\n\n<pre><code>from keras.models import Sequential  \nfrom keras.layers import *  \n\nmodel = Sequential()    \n\n#start from the first hidden layer, since the input is not actually a layer   \n#but inform the shape of the input, with 3 elements.    \nmodel.add(Dense(units=4,input_shape=(3,))) #hidden layer 1 with input\n\n#further layers:    \nmodel.add(Dense(units=4)) #hidden layer 2\nmodel.add(Dense(units=1)) #output layer   \n</code></pre>\n\n<p><strong>With the functional API Model</strong>:</p>\n\n<pre><code>from keras.models import Model   \nfrom keras.layers import * \n\n#Start defining the input tensor:\ninpTensor = Input((3,))   \n\n#create the layers and pass them the input tensor to get the output tensor:    \nhidden1Out = Dense(units=4)(inpTensor)    \nhidden2Out = Dense(units=4)(hidden1Out)    \nfinalOut = Dense(units=1)(hidden2Out)   \n\n#define the model's start and end points    \nmodel = Model(inpTensor,finalOut)\n</code></pre>\n\n<p><strong>Shapes of the tensors</strong></p>\n\n<p>Remember you ignore batch sizes when defining layers: </p>\n\n<ul>\n<li>inpTensor: <code>(None,3)</code>   </li>\n<li>hidden1Out: <code>(None,4)</code>    </li>\n<li>hidden2Out: <code>(None,4)</code>  </li>\n<li>finalOut: <code>(None,1)</code>   </li>\n</ul>\n", "title": "Keras input explanation: input_shape, units, batch_size, dim, etc", "question_id": 44747343, "labels": [{"sentence": "unit", "label": "other"}, {"sentence": "the amount of neuron or cell or whatever the layer have inside it", "label": "root_cause"}, {"sentence": "it be a property of each layer and yes it be relate to the output shape a we will see late", "label": "root_cause"}, {"sentence": "in your picture except for the input layer which be conceptually different from other layer you have", "label": "root_cause"}, {"sentence": "hide layer 1 4 unit 4 neuron", "label": "other"}, {"sentence": "hide layer 2 4 unit", "label": "other"}, {"sentence": "last layer 1 unit", "label": "other"}, {"sentence": "shape", "label": "other"}, {"sentence": "shape be consequence of the model s configuration", "label": "solution"}, {"sentence": "shape be tuples represent how many element an array or tensor have in each dimension", "label": "solution"}, {"sentence": "ex a shape $shortcode$ mean an array or tensor with 3 dimension contain 30 element in the first dimension 4 in the second and 10 in the third total 30 4 10 = 1200 element or number", "label": "other"}, {"sentence": "the input shape", "label": "other"}, {"sentence": "what flow between layer be tensor", "label": "solution"}, {"sentence": "tensor can be see a matrix with shape", "label": "root_cause"}, {"sentence": "in kera the input layer itself be not a layer but a tensor", "label": "root_cause"}, {"sentence": "it be the start tensor you send to the first hide layer", "label": "other"}, {"sentence": "this tensor must have the same shape a your train data", "label": "other"}, {"sentence": "example if you have 30 image of 50x50 pixel in rgb 3 channel the shape of your input data be $shortcode$", "label": "other"}, {"sentence": "then your input layer tensor must have this shape see detail in the shape in kera section", "label": "solution"}, {"sentence": "each type of layer require the input with a certain number of dimension", "label": "root_cause"}, {"sentence": "$shortcode$ layer require input a $shortcode$", "label": "root_cause"}, {"sentence": "or $shortcode$", "label": "other"}, {"sentence": "2d convolutional layer need input a", "label": "root_cause"}, {"sentence": "if use $shortcode$ $shortcode$", "label": "other"}, {"sentence": "if use $shortcode$ $shortcode$", "label": "other"}, {"sentence": "1d convolution and recurrent layer use $shortcode$", "label": "other"}, {"sentence": "detail on how to prepare data for recurrent layer", "label": "other"}, {"sentence": "now the input shape be the only one you must define because your model cannot know it", "label": "root_cause"}, {"sentence": "only you know that base on your train data", "label": "root_cause"}, {"sentence": "all the other shape be calculate automatically base on the unit and particularity of each layer", "label": "root_cause"}, {"sentence": "relation between shape and unit the output shape", "label": "other"}, {"sentence": "give the input shape all other shape be result of layer calculation", "label": "other"}, {"sentence": "the unit of each layer will define the output shape the shape of the tensor that be produce by the layer and that will be the input of the next layer", "label": "root_cause"}, {"sentence": "each type of layer work in a particular way", "label": "root_cause"}, {"sentence": "dense layer have output shape base on unit convolutional layer have output shape base on filter", "label": "root_cause"}, {"sentence": "but it be always base on some layer property", "label": "other"}, {"sentence": "see the documentation for what each layer output", "label": "solution"}, {"sentence": "let u show what happen with dense layer which be the type show in your graph", "label": "other"}, {"sentence": "a dense layer have an output shape of $shortcode$", "label": "root_cause"}, {"sentence": "so yes unit the property of the layer also define the output shape", "label": "root_cause"}, {"sentence": "hide layer 1 4 unit output shape $shortcode$", "label": "other"}, {"sentence": "hide layer 2 4 unit output shape $shortcode$", "label": "other"}, {"sentence": "last layer 1 unit output shape $shortcode$", "label": "other"}, {"sentence": "weight", "label": "other"}, {"sentence": "weight will be entirely automatically calculate base on the input and the output shape", "label": "solution"}, {"sentence": "again each type of layer work in a certain way", "label": "root_cause"}, {"sentence": "but the weight will be a matrix capable of transform the input shape into the output shape by some mathematical operation", "label": "root_cause"}, {"sentence": "in a dense layer weight multiply all input", "label": "root_cause"}, {"sentence": "it be a matrix with one column per input and one row per unit but this be often not important for basic work", "label": "root_cause"}, {"sentence": "in the image if each arrow have a multiplication number on it all number together would form the weight matrix", "label": "other"}, {"sentence": "shape in kera", "label": "other"}, {"sentence": "early i give an example of 30 image 50x50 pixel and 3 channel have an input shape of $shortcode$", "label": "other"}, {"sentence": "since the input shape be the only one you need to define kera will demand it in the first layer", "label": "root_cause"}, {"sentence": "but in this definition kera ignore the first dimension which be the batch size", "label": "root_cause"}, {"sentence": "your model should be able to deal with any batch size so you define only the other dimension $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "optionally or when it be require by certain kind of model you can pas the shape contain the batch size via $shortcode$ or $shortcode$", "label": "solution"}, {"sentence": "this limit your train possibility to this unique batch size so it should be use only when really require", "label": "root_cause"}, {"sentence": "either way you choose tensor in the model will have the batch dimension", "label": "solution"}, {"sentence": "so even if you use $shortcode$ when kera send you message or when you print the model summary it will show $shortcode$", "label": "root_cause"}, {"sentence": "the first dimension be the batch size it be $shortcode$ because it can vary depend on how many example you give for train", "label": "root_cause"}, {"sentence": "if you define the batch size explicitly then the number you define will appear instead of $shortcode$", "label": "root_cause"}, {"sentence": "also in advance work when you actually operate directly on the tensor inside lambda layer or in the loss function for instance the batch size dimension will be there", "label": "root_cause"}, {"sentence": "so when define the input shape you ignore the batch size $shortcode$", "label": "solution"}, {"sentence": "when do operation directly on tensor the shape will be again $shortcode$", "label": "root_cause"}, {"sentence": "when kera send you a message the shape will be $shortcode$ or $shortcode$ depend on what type of message it send you", "label": "root_cause"}, {"sentence": "dim", "label": "other"}, {"sentence": "and in the end what be $shortcode$", "label": "other"}, {"sentence": "if your input shape have only one dimension you do not need to give it a a tuple you give $shortcode$ a a scalar number", "label": "solution"}, {"sentence": "so in your model where your input layer have 3 element you can use any of these two", "label": "other"}, {"sentence": "$shortcode$ the comma be necessary when you have only one dimension", "label": "solution"}, {"sentence": "$shortcode$", "label": "other"}, {"sentence": "but when deal directly with the tensor often $shortcode$ will refer to how many dimension a tensor have", "label": "root_cause"}, {"sentence": "for instance a tensor with shape 25 10909 have 2 dimension", "label": "other"}, {"sentence": "define your image in kera", "label": "other"}, {"sentence": "kera have two way of do it $shortcode$ model or the functional api $shortcode$", "label": "root_cause"}, {"sentence": "i do not like use the sequential model late you will have to forget it anyway because you will want model with branch", "label": "other"}, {"sentence": "p here i ignore other aspect such a activation function", "label": "other"}, {"sentence": "with the sequential model $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "with the functional api model $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "shape of the tensor", "label": "other"}, {"sentence": "remember you ignore batch size when define layer", "label": "solution"}, {"sentence": "inptensor $shortcode$", "label": "other"}, {"sentence": "hidden1out $shortcode$", "label": "other"}, {"sentence": "hidden2out $shortcode$", "label": "other"}, {"sentence": "finalout $shortcode$", "label": "other"}]}, {"answer_body": "<p>Use <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tolist.html\" rel=\"noreferrer\"><code>tolist()</code></a>:</p>\n\n<pre><code>import numpy as np\n&gt;&gt;&gt; np.array([[1,2,3],[4,5,6]]).tolist()\n[[1, 2, 3], [4, 5, 6]]\n</code></pre>\n\n<p>Note that this converts the values from whatever numpy type they may have (e.g. np.int32 or np.float32) to the \"nearest compatible Python type\" (in a list). If you want to preserve the numpy data types, you could call list() on your array instead, and you'll end up with a list of <a href=\"https://docs.scipy.org/doc/numpy-1.10.0/reference/arrays.scalars.html\" rel=\"noreferrer\">numpy scalars</a>. (Thanks to Mr_and_Mrs_D for pointing that out in a comment.)</p>\n", "title": "Converting NumPy array into Python List structure?", "question_id": 1966207, "labels": [{"sentence": "use $shortcode$ $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "note that this convert the value from whatever numpy type they may have e g", "label": "root_cause"}, {"sentence": "np int32 or np float32 to the near compatible python type in a list", "label": "other"}, {"sentence": "if you want to preserve the numpy data type you could call list on your array instead and you you will end up with a list of numpy scalar", "label": "solution"}, {"sentence": "thank to mr_and_mrs_d for point that out in a comment", "label": "other"}]}, {"answer_body": "<p>Its not really possible to perform a 'mouse hover' action, instead you need to chain all of the actions that you want to achieve in one go. So move to the element that reveals the others, then during the same chain, move to the now revealed element and click on it.</p>\n\n<p>When using Action Chains you have to remember to 'do it like a user would'.</p>\n\n<pre><code>Actions action = new Actions(webdriver);\nWebElement we = webdriver.findElement(By.xpath(\"html/body/div[13]/ul/li[4]/a\"));\naction.moveToElement(we).moveToElement(webdriver.findElement(By.xpath(\"/expression-here\"))).click().build().perform();\n</code></pre>\n", "title": "How to perform mouseover function in Selenium WebDriver using Java?", "question_id": 17293914, "labels": [{"sentence": "it not really possible to perform a mouse hover action instead you need to chain all of the action that you want to achieve in one go", "label": "solution"}, {"sentence": "so move to the element that reveal the others then during the same chain move to the now reveal element and click on it", "label": "solution"}, {"sentence": "when use action chain you have to remember to do it like a user would $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<p>In October 2014 the <a href=\"http://blog.docker.com/2014/10/docker-1-3-signed-images-process-injection-security-options-mac-shared-directories/\" rel=\"noreferrer\">Docker team introduced <code>docker exec</code> command</a>: <a href=\"https://docs.docker.com/engine/reference/commandline/exec/\" rel=\"noreferrer\">https://docs.docker.com/engine/reference/commandline/exec/</a></p>\n\n<p>So now you can run any command in a running container just knowing its ID (or name):</p>\n\n<pre><code>docker exec -it &lt;container_id_or_name&gt; echo \"Hello from container!\"\n</code></pre>\n\n<p>Note that <code>exec</code> command works only on already running container. If the container is currently stopped, you need to first run it with the following command:</p>\n\n<pre><code>docker run -it -d shykes/pybuilder /bin/bash\n</code></pre>\n\n<p>The most important thing here is the <code>-d</code> option, which stands for <code>detached</code>. It means that the command you initially provided to the container (<code>/bin/bash</code>) will be run in the background and the container will not <em>stop immediately</em>.</p>\n", "title": "How do I run a command on an already existing Docker container?", "question_id": 26153686, "labels": [{"sentence": "in october 2014 the docker team introduce $shortcode$ command $url$", "label": "solution"}, {"sentence": "so now you can run any command in a run container just know it id or name $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "note that $shortcode$ command work only on already run container", "label": "other"}, {"sentence": "if the container be currently stop you need to first run it with the follow command $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "the most important thing here be the $shortcode$ option which stand for $shortcode$", "label": "root_cause"}, {"sentence": "it mean that the command you initially provide to the container $shortcode$ will be run in the background and the container will not stop immediately", "label": "root_cause"}]}, {"answer_body": "<p>First, make sure you have the latest version of conda by running</p>\n\n<pre><code>conda update conda\n</code></pre>\n\n<p>Then run</p>\n\n<pre><code>conda install python=3.5\n</code></pre>\n\n<p>This will attempt to update all your packages in your root environment to Python 3 versions. If it is not possible (e.g., because some package is not built for Python 3.5), it will give you an error message indicating which package(s) caused the issue.</p>\n\n<p>If you installed packages with pip, you'll have to reinstall them. </p>\n", "title": "How to change default Anaconda python environment", "question_id": 28436769, "labels": [{"sentence": "first make sure you have the late version of conda by run $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "then run $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "this will attempt to update all your package in your root environment to python 3 version", "label": "root_cause"}, {"sentence": "if it be not possible e g because some package be not build for python 3 5 it will give you an error message indicate which package s cause the issue", "label": "root_cause"}, {"sentence": "if you instal package with pip you you will have to reinstall them", "label": "other"}]}, {"answer_body": "<p>Use <code>matplotlib</code>'s calls that won't block:</p>\n\n<p>Using <code>draw()</code>:</p>\n\n<pre><code>from matplotlib.pyplot import plot, draw, show\nplot([1,2,3])\ndraw()\nprint 'continue computation'\n\n# at the end call show to ensure window won't close.\nshow()\n</code></pre>\n\n<p>Using interactive mode:</p>\n\n<pre><code>from matplotlib.pyplot import plot, ion, show\nion() # enables interactive mode\nplot([1,2,3]) # result shows immediatelly (implicit draw())\n\nprint 'continue computation'\n\n# at the end call show to ensure window won't close.\nshow()\n</code></pre>\n", "title": "Is there a way to detach matplotlib plots so that the computation can continue?", "question_id": 458209, "labels": [{"sentence": "use $shortcode$ s call that will not block", "label": "solution"}, {"sentence": "use $shortcode$ $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "use interactive mode $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<p>Python library authors put the version number in <code>&lt;module&gt;.__version__</code>. You can print it by running this on the command line:</p>\n\n<pre><code>python -c 'import keras; print(keras.__version__)'\n</code></pre>\n\n<p>If it's Windows terminal, enclose snippet with double-quotes like below</p>\n\n<pre><code>python -c \"import keras; print(keras.__version__)\"\n</code></pre>\n", "title": "How to check which version of Keras is installed?", "question_id": 46086030, "labels": [{"sentence": "python library author put the version number in $shortcode$", "label": "solution"}, {"sentence": "you can print it by run this on the command line $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "if it be window terminal enclose snippet with double quote like below $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<p>The pooling and convolutional ops slide a \"window\" across the input tensor.  Using <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/nn.html#conv2d\"><code>tf.nn.conv2d</code></a> as an example: If the input tensor has 4 dimensions:  <code>[batch, height, width, channels]</code>, then the convolution operates on a 2D window on the <code>height, width</code> dimensions.</p>\n\n<p><code>strides</code> determines how much the window shifts by in each of the dimensions.  The typical use sets the first (the batch) and last (the depth) stride to 1.</p>\n\n<p>Let's use a very concrete example:  Running a 2-d convolution over a 32x32 greyscale input image.  I say greyscale because then the input image has depth=1, which helps keep it simple.  Let that image look like this:</p>\n\n<pre><code>00 01 02 03 04 ...\n10 11 12 13 14 ...\n20 21 22 23 24 ...\n30 31 32 33 34 ...\n...\n</code></pre>\n\n<p>Let's run a 2x2 convolution window over a single example (batch size = 1).  We'll give the convolution an output channel depth of 8.</p>\n\n<p>The input to the convolution has <code>shape=[1, 32, 32, 1]</code>.</p>\n\n<p>If you specify <code>strides=[1,1,1,1]</code> with <code>padding=SAME</code>, then the output of the filter will be [1, 32, 32, 8].</p>\n\n<p>The filter will first create an output for:</p>\n\n<pre><code>F(00 01\n  10 11)\n</code></pre>\n\n<p>And then for:</p>\n\n<pre><code>F(01 02\n  11 12)\n</code></pre>\n\n<p>and so on.  Then it will move to the second row, calculating:</p>\n\n<pre><code>F(10, 11\n  20, 21)\n</code></pre>\n\n<p>then</p>\n\n<pre><code>F(11, 12\n  21, 22)\n</code></pre>\n\n<p>If you specify a stride of [1, 2, 2, 1] it won't do overlapping windows.  It will compute:</p>\n\n<pre><code>F(00, 01\n  10, 11)\n</code></pre>\n\n<p>and then</p>\n\n<pre><code>F(02, 03\n  12, 13)\n</code></pre>\n\n<p>The stride operates similarly for the pooling operators.</p>\n\n<p><strong>Question 2:  Why strides [1, x, y, 1] for convnets</strong></p>\n\n<p>The first 1 is the batch:  You don't usually want to skip over examples in your batch, or you shouldn't have included them in the first place. :)</p>\n\n<p>The last 1 is the depth of the convolution:  You don't usually want to skip inputs, for the same reason.</p>\n\n<p>The conv2d operator is more general, so you <em>could</em> create convolutions that slide the window along other dimensions, but that's not a typical use in convnets.  The typical use is to use them spatially.</p>\n\n<p><strong>Why reshape to -1</strong>  -1 is a placeholder that says \"adjust as necessary to match the size needed for the full tensor.\"  It's a way of making the code be independent of the input batch size, so that you can change your pipeline and not have to adjust the batch size everywhere in the code.</p>\n", "title": "Tensorflow Strides Argument", "question_id": 34642595, "labels": [{"sentence": "the pool and convolutional ops slide a window across the input tensor", "label": "root_cause"}, {"sentence": "use $shortcode$ a an example if the input tensor have 4 dimension $shortcode$ then the convolution operate on a 2d window on the $shortcode$ dimension", "label": "solution"}, {"sentence": "$shortcode$ determine how much the window shift by in each of the dimension", "label": "other"}, {"sentence": "the typical use set the first the batch and last the depth stride to 1", "label": "other"}, {"sentence": "let u use a very concrete example run a 2 d convolution over a 32x32 greyscale input image", "label": "other"}, {"sentence": "i say greyscale because then the input image have depth=1 which help keep it simple", "label": "other"}, {"sentence": "let that image look like this $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "let u run a 2x2 convolution window over a single example batch size = 1", "label": "other"}, {"sentence": "we will give the convolution an output channel depth of 8", "label": "other"}, {"sentence": "the input to the convolution have $shortcode$", "label": "other"}, {"sentence": "if you specify $shortcode$ with $shortcode$ then the output of the filter will be [1 32 32 8]", "label": "root_cause"}, {"sentence": "the filter will first create an output for $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "and then for $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "and so on", "label": "other"}, {"sentence": "then it will move to the second row calculate $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "then $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "if you specify a stride of [1 2 2 1] it will not do overlap window", "label": "other"}, {"sentence": "it will compute $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "and then $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "the stride operate similarly for the pool operator", "label": "other"}, {"sentence": "question 2 why stride [1 x y 1] for convnets", "label": "other"}, {"sentence": "the first 1 be the batch you do not usually want to skip over example in your batch or you should not have include them in the first place", "label": "root_cause"}, {"sentence": "", "label": "other"}, {"sentence": "the last 1 be the depth of the convolution you do not usually want to skip input for the same reason", "label": "root_cause"}, {"sentence": "the conv2d operator be more general so you could create convolution that slide the window along other dimension but that be not a typical use in convnets", "label": "root_cause"}, {"sentence": "the typical use be to use them spatially", "label": "other"}, {"sentence": "why reshape to 1 1 be a placeholder that say adjust a necessary to match the size need for the full tensor it be a way of make the code be independent of the input batch size so that you can change your pipeline and not have to adjust the batch size everywhere in the code", "label": "root_cause"}]}, {"answer_body": "<p>Given a list of lists <code>l</code>,</p>\n\n<p><code>flat_list = [item for sublist in l for item in sublist]</code></p>\n\n<p>which means:</p>\n\n<pre><code>flat_list = []\nfor sublist in l:\n    for item in sublist:\n        flat_list.append(item)\n</code></pre>\n\n<p>is faster than the shortcuts posted so far. (<code>l</code> is the list to flatten.)</p>\n\n<p>Here is the corresponding function:</p>\n\n<pre><code>flatten = lambda l: [item for sublist in l for item in sublist]\n</code></pre>\n\n<p>As evidence, you can use the <code>timeit</code> module in the standard library:</p>\n\n<pre><code>$ python -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' '[item for sublist in l for item in sublist]'\n10000 loops, best of 3: 143 usec per loop\n$ python -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' 'sum(l, [])'\n1000 loops, best of 3: 969 usec per loop\n$ python -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' 'reduce(lambda x,y: x+y,l)'\n1000 loops, best of 3: 1.1 msec per loop\n</code></pre>\n\n<p>Explanation: the shortcuts based on <code>+</code> (including the implied use in <code>sum</code>) are, of necessity, <code>O(L**2)</code> when there are L sublists -- as the intermediate result list keeps getting longer, at each step a new intermediate result list object gets allocated, and all the items in the previous intermediate result must be copied over (as well as a few new ones added at the end). So, for simplicity and without actual loss of generality, say you have L sublists of I items each: the first I items are copied back and forth L-1 times, the second I items L-2 times, and so on; total number of copies is I times the sum of x for x from 1 to L excluded, i.e., <code>I * (L**2)/2</code>.</p>\n\n<p>The list comprehension just generates one list, once, and copies each item over (from its original place of residence to the result list) also exactly once.</p>\n", "title": "How to make a flat list out of list of lists", "question_id": 952914, "labels": [{"sentence": "give a list of list $shortcode$ $shortcode$", "label": "other"}, {"sentence": "$shortcode$", "label": "other"}, {"sentence": "which mean $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "be fast than the shortcut post so far", "label": "other"}, {"sentence": "$shortcode$ be the list to flatten", "label": "other"}, {"sentence": "here be the correspond function $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "a evidence you can use the $shortcode$ module in the standard library $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "explanation the shortcut base on $shortcode$ include the imply use in $shortcode$ be of necessity $shortcode$ when there be l sublists a the intermediate result list keep get long at each step a new intermediate result list object get allocate and all the item in the previous intermediate result must be copy over a well a a few new one add at the end", "label": "root_cause"}, {"sentence": "so for simplicity and without actual loss of generality say you have l sublists of i item each the first i item be copy back and forth l 1 time the second i item l 2 time and so on total number of copy be i time the sum of x for x from 1 to l exclude i e $shortcode$", "label": "root_cause"}, {"sentence": "the list comprehension just generate one list once and copy each item over from it original place of residence to the result list also exactly once", "label": "root_cause"}]}, {"answer_body": "<h2>Important: If you have any local changes, they will be lost. With or without <code>--hard</code> option, any local commits that haven't been pushed will be lost.<sup>[*]</sup></h2>\n\n<p>If you have any files that are <em>not</em> tracked by Git (e.g. uploaded user content), these files will not be affected.</p>\n\n<hr>\n\n<p>I think this is the right way:</p>\n\n<pre><code>git fetch --all\n</code></pre>\n\n<p>Then, you have two options:</p>\n\n<pre><code>git reset --hard origin/master\n</code></pre>\n\n<p>OR If you are on some other branch:</p>\n\n<pre><code>git reset --hard origin/&lt;branch_name&gt;\n</code></pre>\n\n<h3>Explanation:</h3>\n\n<p><code>git fetch</code> downloads the latest from remote without trying to merge or rebase anything.</p>\n\n<p>Then the <code>git reset</code> resets the master branch to what you just fetched. The <code>--hard</code> option changes all the files in your working tree to match the files in <code>origin/master</code></p>\n\n<hr>\n\n<h3>Maintain current local commits</h3>\n\n<p><sup>[*]</sup>: It's worth noting that it is possible to maintain current local commits by creating a branch from <code>master</code> before resetting:</p>\n\n<pre><code>git checkout master\ngit branch new-branch-to-save-current-commits\ngit fetch --all\ngit reset --hard origin/master\n</code></pre>\n\n<p>After this, all of the old commits will be kept in <code>new-branch-to-save-current-commits</code>. </p>\n\n<h3>Uncommitted changes</h3>\n\n<p>Uncommitted changes, however (even staged), will be lost. Make sure to stash and commit anything you need. For that you can run the following:</p>\n\n<pre><code>git stash\n</code></pre>\n\n<p>And then to reapply these uncommitted changes:</p>\n\n<pre><code>git stash pop\n</code></pre>\n", "title": "How do I force &quot;git pull&quot; to overwrite local files?", "question_id": 1125968, "labels": [{"sentence": "important if you have any local change they will be lose", "label": "other"}, {"sentence": "with or without $shortcode$ option any local commit that have not be push will be lose [ ]", "label": "other"}, {"sentence": "if you have any file that be not track by git e g", "label": "other"}, {"sentence": "upload user content these file will not be affect", "label": "root_cause"}, {"sentence": "i think this be the right way $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "then you have two option $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "or if you be on some other branch $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "explanation", "label": "root_cause"}, {"sentence": "$shortcode$ download the late from remote without try to merge or rebase anything", "label": "root_cause"}, {"sentence": "then the $shortcode$ reset the master branch to what you just fetch", "label": "root_cause"}, {"sentence": "the $shortcode$ option change all the file in your work tree to match the file in $shortcode$", "label": "root_cause"}, {"sentence": "maintain current local commit", "label": "other"}, {"sentence": "[ ] it be worth note that it be possible to maintain current local commit by create a branch from $shortcode$ before reset $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "after this all of the old commit will be keep in $shortcode$", "label": "root_cause"}, {"sentence": "uncommitted change", "label": "other"}, {"sentence": "uncommitted change however even stag will be lose", "label": "other"}, {"sentence": "make sure to stash and commit anything you need", "label": "solution"}, {"sentence": "for that you can run the follow $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "and then to reapply these uncommitted change $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<p>The activity regularizer works as a function of the output of the net, and is mostly used to regularize hidden units, while weight_regularizer, as the name says, works on the weights, making them decay. Basically you can express the regularization loss as a function of the output (<code>activity_regularizer</code>) or of the weights (<code>weight_regularizer</code>). </p>\n\n<p>The new <code>kernel_regularizer</code> replaces <code>weight_regularizer</code> - although it's not very clear from the documentation.</p>\n\n<p>From the definition of <code>kernel_regularizer</code>:</p>\n\n<blockquote>\n  <p>kernel_regularizer: Regularizer function applied to\n              the <code>kernel</code> weights matrix\n              (see regularizer).</p>\n</blockquote>\n\n<p>And <code>activity_regularizer</code>:</p>\n\n<blockquote>\n  <p>activity_regularizer: Regularizer function applied to\n              the output of the layer (its \"activation\").\n              (see regularizer).</p>\n</blockquote>\n\n<p><strong>Important Edit</strong>: Note that there is a bug in the <em>activity_regularizer</em> that was <strong>only fixed in version 2.1.4 of Keras</strong> (at least with Tensorflow backend). Indeed, in the older versions, the activity regularizer function is applied to the input of the layer, instead of being applied to the output (the actual activations of the layer, as intended). So beware if you are using an older version of Keras (before 2.1.4), activity regularization may probably not work as intended.</p>\n\n<p>You can see the commit on <a href=\"https://github.com/keras-team/keras/commits/master?after=75114feeac5ee6aa7679802ce7e5172c63565e2c+279\" rel=\"noreferrer\">GitHub</a></p>\n\n<p><a href=\"https://i.stack.imgur.com/EQSyd.png\" rel=\"noreferrer\">Five months ago Fran\u00e7ois Chollet provided a fix to the activity regularizer, that was then included in Keras 2.1.4</a></p>\n", "title": "Keras: Difference between Kernel and Activity regularizers", "question_id": 44495698, "labels": [{"sentence": "the activity regularizer work a a function of the output of the net and be mostly use to regularize hide unit while weight_regularizer a the name say work on the weight make them decay", "label": "root_cause"}, {"sentence": "basically you can express the regularization loss a a function of the output $shortcode$ or of the weight $shortcode$", "label": "root_cause"}, {"sentence": "the new $shortcode$ replace $shortcode$ although it be not very clear from the documentation", "label": "solution"}, {"sentence": "from the definition of $shortcode$", "label": "other"}, {"sentence": "kernel_regularizer regularizer function apply to", "label": "other"}, {"sentence": "the $shortcode$ weight matrix", "label": "other"}, {"sentence": "see regularizer", "label": "other"}, {"sentence": "and $shortcode$", "label": "other"}, {"sentence": "activity_regularizer regularizer function apply to", "label": "root_cause"}, {"sentence": "the output of the layer it activation", "label": "other"}, {"sentence": "see regularizer", "label": "other"}, {"sentence": "important edit note that there be a bug in the activity_regularizer that be only fix in version 2 1 4 of kera at least with tensorflow backend", "label": "other"}, {"sentence": "indeed in the old version the activity regularizer function be apply to the input of the layer instead of be apply to the output the actual activation of the layer a intend", "label": "root_cause"}, {"sentence": "so beware if you be use an old version of kera before 2 1 4 activity regularization may probably not work a intend", "label": "other"}, {"sentence": "you can see the commit on github", "label": "solution"}, {"sentence": "five month ago fran\u00e7ois chollet provide a fix to the activity regularizer that be then include in kera 2 1 4", "label": "root_cause"}]}, {"answer_body": "<p><strong>Try this code:</strong></p>\n\n<pre><code>  driver.manage().timeouts().pageLoadTimeout(10, TimeUnit.SECONDS);\n</code></pre>\n\n<p>The above code will wait up to 10 seconds for page loading. If the page loading exceeds the time it will throw the <code>TimeoutException</code>.  You catch the exception and do your needs. I am not sure whether it quits the page loading after the exception thrown. i didn't try this code yet. Want to just try it.</p>\n\n<p>This is an implicit wait. If you set this once it will have the scope until the Web Driver instance destroy.</p>\n\n<p>See the <a href=\"https://seleniumhq.github.io/selenium/docs/api/java/org/openqa/selenium/WebDriver.Timeouts.html\" rel=\"nofollow noreferrer\">documentation for <code>WebDriver.Timeouts</code></a> for more info.</p>\n", "title": "Selenium wait until document is ready", "question_id": 15122864, "labels": [{"sentence": "try this code $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "the above code will wait up to 10 second for page load", "label": "solution"}, {"sentence": "if the page load exceed the time it will throw the $shortcode$", "label": "other"}, {"sentence": "you catch the exception and do your need", "label": "root_cause"}, {"sentence": "i be not sure whether it quit the page load after the exception throw", "label": "other"}, {"sentence": "i do not try this code yet", "label": "other"}, {"sentence": "want to just try it", "label": "other"}, {"sentence": "this be an implicit wait", "label": "other"}, {"sentence": "if you set this once it will have the scope until the web driver instance destroy", "label": "root_cause"}, {"sentence": "see the documentation for $shortcode$ for more info", "label": "root_cause"}]}, {"answer_body": "<p>First, clone a remote <a href=\"http://en.wikipedia.org/wiki/Git_%28software%29\" rel=\"noreferrer\">Git</a> repository and <a href=\"http://en.wikipedia.org/wiki/Cd_%28command%29\" rel=\"noreferrer\">cd</a> into it:</p>\n\n<pre><code>$ git clone git://example.com/myproject\n$ cd myproject\n</code></pre>\n\n<p>Next, look at the local branches in your repository:</p>\n\n<pre><code>$ git branch\n* master\n</code></pre>\n\n<p>But there are other branches hiding in your repository! You can see these using the <code>-a</code> flag:</p>\n\n<pre><code>$ git branch -a\n* master\n  remotes/origin/HEAD\n  remotes/origin/master\n  remotes/origin/v1.0-stable\n  remotes/origin/experimental\n</code></pre>\n\n<p>If you just want to take a quick peek at an upstream branch, you can check it out directly:</p>\n\n<pre><code>$ git checkout origin/experimental\n</code></pre>\n\n<p>But if you want to work on that branch, you'll need to create a local tracking branch which is done automatically by:</p>\n\n<pre><code>$ git checkout experimental\n</code></pre>\n\n<p>and you will see</p>\n\n<pre><code>Branch experimental set up to track remote branch experimental from origin.\nSwitched to a new branch 'experimental'\n</code></pre>\n\n<p>That last line throws some people: \"New branch\" - huh?\nWhat it really means is that the branch is taken from the index and created locally for you.  The <em>previous</em> line is actually more informative as it tells you that the branch is being set up to track the remote branch, which usually means the origin/branch_name branch </p>\n\n<p>Now, if you look at your local branches, this is what you'll see:</p>\n\n<pre><code>$ git branch\n* experimental\n  master\n</code></pre>\n\n<p>You can actually track more than one remote repository using <code>git remote</code>.</p>\n\n<pre><code>$ git remote add win32 git://example.com/users/joe/myproject-win32-port\n$ git branch -a\n* master\n  remotes/origin/HEAD\n  remotes/origin/master\n  remotes/origin/v1.0-stable\n  remotes/origin/experimental\n  remotes/win32/master\n  remotes/win32/new-widgets\n</code></pre>\n\n<p>At this point, things are getting pretty crazy, so run <code>gitk</code> to see what's going on:</p>\n\n<pre><code>$ gitk --all &amp;\n</code></pre>\n", "title": "How to clone all remote branches in Git?", "question_id": 67699, "labels": [{"sentence": "first clone a remote git repository and cd into it $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "next look at the local branch in your repository $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "but there be other branch hide in your repository", "label": "other"}, {"sentence": "you can see these use the $shortcode$ flag $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "if you just want to take a quick peek at an upstream branch you can check it out directly $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "but if you want to work on that branch you you will need to create a local track branch which be do automatically by $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "and you will see $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "that last line throw some people new branch huh", "label": "other"}, {"sentence": "what it really mean be that the branch be take from the index and create locally for you", "label": "root_cause"}, {"sentence": "the previous line be actually more informative a it tell you that the branch be be set up to track the remote branch which usually mean the origin/branch_name branch", "label": "root_cause"}, {"sentence": "now if you look at your local branch this be what you you will see $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "you can actually track more than one remote repository use $shortcode$ $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "at this point thing be get pretty crazy so run $shortcode$ to see what be go on $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<h1>Multiple Apps</h1>\n\n<p>The application context (and its purpose) is indeed confusing until you realize that Flask can have multiple apps. Imagine the situation where you want to have a single WSGI Python interpreter run multiple Flask application. We're not talking Blueprints here, we're talking entirely different Flask applications.</p>\n\n<p>You might set this up similar to the <a href=\"http://flask.pocoo.org/docs/patterns/appdispatch/\" rel=\"noreferrer\">Flask documentation section on \"Application Dispatching\"</a> example:</p>\n\n<pre><code>from werkzeug.wsgi import DispatcherMiddleware\nfrom frontend_app import application as frontend\nfrom backend_app import application as backend\n\napplication = DispatcherMiddleware(frontend, {\n    '/backend':     backend\n})\n</code></pre>\n\n<p>Notice that there are two completely different Flask applications being created \"frontend\" and \"backend\". In other words, the <code>Flask(...)</code> application constructor has been called twice, creating two instances of a Flask application.</p>\n\n<h1>Contexts</h1>\n\n<p>When you are working with Flask, you often end up using global variables to access various functionality. For example, you probably have code that reads...</p>\n\n<pre><code>from flask import request\n</code></pre>\n\n<p>Then, during a view, you might use <code>request</code> to access the information of the current request. Obviously, <code>request</code> is not a normal global variable; in actuality, it is a <a href=\"http://flask.pocoo.org/docs/reqcontext/#diving-into-context-locals\" rel=\"noreferrer\">context local</a> value. In other words, there is some magic behind the scenes that says \"when I call <code>request.path</code>, get the <code>path</code> attribute from the <code>request</code> object of the CURRENT request.\" Two different requests will have a different results for <code>request.path</code>.</p>\n\n<p>In fact, even if you run Flask with multiple threads, Flask is smart enough to keep the request objects isolated. In doing so, it becomes possible for two threads, each handling a different request, to simultaneously call <code>request.path</code> and get the correct information for their respective requests.</p>\n\n<h1>Putting it Together</h1>\n\n<p>So we've already seen that Flask can handle multiple applications in the same interpreter, and also that because of the way that Flask allows you to use \"context local\" globals there must be some mechanism to determine what the \"current\" <em>request</em> is (in order to do things such as <code>request.path</code>).</p>\n\n<p>Putting these ideas together, it should also make sense that Flask must have some way to determine what the \"current\" application is!</p>\n\n<p>You probably also have code similar to the following:</p>\n\n<pre><code>from flask import url_for\n</code></pre>\n\n<p>Like our <code>request</code> example, the <code>url_for</code> function has logic that is dependent on the current environment. In this case, however, it is clear to see that the logic is heavily dependent on which app is considered the \"current\" app. In the frontend/backend example shown above, both the \"frontend\" and \"backend\" apps could have a \"/login\" route, and so <code>url_for('/login')</code> should return something different depending on if the view is handling the request for the frontend or backend app.</p>\n\n<h1>To answer your questions...</h1>\n\n<blockquote>\n  <p>What is the purpose of the \"stack\" when it comes to the request or\n  application context?</p>\n</blockquote>\n\n<p>From the Request Context docs:</p>\n\n<blockquote>\n  <p>Because the request context is internally maintained as a stack you\n  can push and pop multiple times. This is very handy to implement\n  things like internal redirects.</p>\n</blockquote>\n\n<p>In other words, even though you typically will have 0 or 1 items on these stack of \"current\" requests or \"current\" applications, it is possible that you could have more.</p>\n\n<p>The example given is where you would have your request return the results of an \"internal redirect\". Let's say a user requests A, but you want to return to the user B. In most cases, you issue a redirect to the user, and point the user to resource B, meaning the user will run a second request to fetch B. A slightly different way of handling this would be to do an internal redirect, which means that while processing A, Flask will make a new request to itself for resource B, and use the results of this second request as the results of the user's original request.</p>\n\n<blockquote>\n  <p>Are these two separate stacks, or are they both part of one stack?</p>\n</blockquote>\n\n<p>They are <a href=\"https://github.com/mitsuhiko/flask/blob/41b5d77e29e6158f7a67e9f9872efb323f1be18f/flask/globals.py#L55-L57\" rel=\"noreferrer\">two separate stacks</a>. However, this is an implementation detail. What's more important is not so much that there is a stack, but the fact that at any time you can get the \"current\" app or request (top of the stack).</p>\n\n<blockquote>\n  <p>Is the request context pushed onto a stack, or is it a stack itself?</p>\n</blockquote>\n\n<p>A \"request context\" is one item of the \"request context stack\". Similarly with the \"app context\" and \"app context stack\".</p>\n\n<blockquote>\n  <p>Am I able to push/pop multiple contexts on top of eachother? If so,\n  why would I want to do that?</p>\n</blockquote>\n\n<p>In a Flask application, you typically would not do this. One example of where you might want to is for an internal redirect (described above). Even in that case, however, you would probably end up having Flask handle a new request, and so Flask would do all of the pushing/popping for you.</p>\n\n<p>However, there are some cases where you'd want to manipulate the stack yourself.</p>\n\n<h2>Running code outside of a request</h2>\n\n<p>One typical problem people have is that they use the Flask-SQLAlchemy extension to set up a SQL database and model definition using code something like what is shown below...</p>\n\n<pre><code>app = Flask(__name__)\ndb = SQLAlchemy() # Initialize the Flask-SQLAlchemy extension object\ndb.init_app(app)\n</code></pre>\n\n<p>Then they use the <code>app</code> and <code>db</code> values in a script that should be run from the shell. For example, a \"setup_tables.py\" script...</p>\n\n<pre><code>from myapp import app, db\n\n# Set up models\ndb.create_all()\n</code></pre>\n\n<p>In this case, the Flask-SQLAlchemy extension knows about the <code>app</code> application, but during <code>create_all()</code> it will throw an error complaining about there not being an application context. This error is justified; you never told Flask what application it should be dealing with when running the <code>create_all</code> method.</p>\n\n<p>You might be wondering why you don't end up needing this <code>with app.app_context()</code> call when you run similar functions in your views. The reason is that Flask already handles the management of the application context for you when it is handling actual web requests. The problem really only comes up outside of these view functions (or other such callbacks), such as when using your models in a one-off script.</p>\n\n<p>The resolution is to push the application context yourself, which can be done by doing...</p>\n\n<pre><code>from myapp import app, db\n\n# Set up models\nwith app.app_context():\n    db.create_all()\n</code></pre>\n\n<p>This will push a new application context (using the application of <code>app</code>, remember there could be more than one application).</p>\n\n<h2>Testing</h2>\n\n<p>Another case where you would want to manipulate the stack is for testing. You could create a unit test that handles a request and you check the results:</p>\n\n<pre><code>import unittest\nfrom flask import request\n\nclass MyTest(unittest.TestCase):\n    def test_thing(self):\n        with app.test_request_context('/?next=http://example.com/') as ctx:\n            # You can now view attributes on request context stack by using `request`.\n\n        # Now the request context stack is empty\n</code></pre>\n", "title": "What is the purpose of Flask&#39;s context stacks?", "question_id": 20036520, "labels": [{"sentence": "multiple apps", "label": "other"}, {"sentence": "the application context and it purpose be indeed confuse until you realize that flask can have multiple apps", "label": "other"}, {"sentence": "imagine the situation where you want to have a single wsgi python interpreter run multiple flask application", "label": "root_cause"}, {"sentence": "we be not talk blueprint here we be talk entirely different flask application", "label": "other"}, {"sentence": "you might set this up similar to the flask documentation section on application dispatch example $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "notice that there be two completely different flask application be create frontend and backend", "label": "root_cause"}, {"sentence": "in other word the $shortcode$ application constructor have be call twice create two instance of a flask application", "label": "root_cause"}, {"sentence": "context", "label": "other"}, {"sentence": "when you be work with flask you often end up use global variable to access various functionality", "label": "root_cause"}, {"sentence": "for example you probably have code that read $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "then during a view you might use $shortcode$ to access the information of the current request", "label": "other"}, {"sentence": "obviously $shortcode$ be not a normal global variable in actuality it be a context local value", "label": "root_cause"}, {"sentence": "in other word there be some magic behind the scene that say when i call $shortcode$ get the $shortcode$ attribute from the $shortcode$ object of the current request two different request will have a different result for $shortcode$", "label": "root_cause"}, {"sentence": "in fact even if you run flask with multiple thread flask be smart enough to keep the request object isolate", "label": "other"}, {"sentence": "in do so it become possible for two thread each handle a different request to simultaneously call $shortcode$ and get the correct information for their respective request", "label": "root_cause"}, {"sentence": "put it together", "label": "other"}, {"sentence": "so we have already see that flask can handle multiple application in the same interpreter and also that because of the way that flask allow you to use context local globals there must be some mechanism to determine what the current request be in order to do thing such a $shortcode$", "label": "root_cause"}, {"sentence": "put these idea together it should also make sense that flask must have some way to determine what the current application be", "label": "root_cause"}, {"sentence": "you probably also have code similar to the follow $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "like our $shortcode$ example the $shortcode$ function have logic that be dependent on the current environment", "label": "other"}, {"sentence": "in this case however it be clear to see that the logic be heavily dependent on which app be consider the current app", "label": "root_cause"}, {"sentence": "in the frontend/backend example show above both the frontend and backend apps could have a /login route and so $shortcode$ should return something different depend on if the view be handle the request for the frontend or backend app", "label": "solution"}, {"sentence": "to answer your question", "label": "other"}, {"sentence": "what be the purpose of the stack when it come to the request or", "label": "other"}, {"sentence": "application context", "label": "other"}, {"sentence": "from the request context doc", "label": "other"}, {"sentence": "because the request context be internally maintain a a stack you", "label": "root_cause"}, {"sentence": "can push and pop multiple time", "label": "other"}, {"sentence": "this be very handy to implement", "label": "other"}, {"sentence": "thing like internal redirect", "label": "other"}, {"sentence": "in other word even though you typically will have 0 or 1 item on these stack of current request or current application it be possible that you could have more", "label": "root_cause"}, {"sentence": "the example give be where you would have your request return the result of an internal redirect", "label": "other"}, {"sentence": "let u say a user request a but you want to return to the user b", "label": "other"}, {"sentence": "in most case you issue a redirect to the user and point the user to resource b mean the user will run a second request to fetch b", "label": "other"}, {"sentence": "a slightly different way of handle this would be to do an internal redirect which mean that while process a flask will make a new request to itself for resource b and use the result of this second request a the result of the user s original request", "label": "solution"}, {"sentence": "be these two separate stack or be they both part of one stack", "label": "other"}, {"sentence": "they be two separate stack", "label": "other"}, {"sentence": "however this be an implementation detail", "label": "other"}, {"sentence": "what be more important be not so much that there be a stack but the fact that at any time you can get the current app or request top of the stack", "label": "other"}, {"sentence": "be the request context push onto a stack or be it a stack itself", "label": "other"}, {"sentence": "a request context be one item of the request context stack", "label": "other"}, {"sentence": "similarly with the app context and app context stack", "label": "other"}, {"sentence": "be i able to push/pop multiple context on top of eachother", "label": "other"}, {"sentence": "if so", "label": "other"}, {"sentence": "why would i want to do that", "label": "other"}, {"sentence": "in a flask application you typically would not do this", "label": "other"}, {"sentence": "one example of where you might want to be for an internal redirect describe above", "label": "other"}, {"sentence": "even in that case however you would probably end up have flask handle a new request and so flask would do all of the pushing/popping for you", "label": "root_cause"}, {"sentence": "however there be some case where you have want to manipulate the stack yourself", "label": "other"}, {"sentence": "run code outside of a request", "label": "other"}, {"sentence": "one typical problem people have be that they use the flask sqlalchemy extension to set up a sql database and model definition use code something like what be show below $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "then they use the $shortcode$ and $shortcode$ value in a script that should be run from the shell", "label": "other"}, {"sentence": "for example a setup_tables py script $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "in this case the flask sqlalchemy extension know about the $shortcode$ application but during $shortcode$ it will throw an error complain about there not be an application context", "label": "root_cause"}, {"sentence": "this error be justify you never tell flask what application it should be deal with when run the $shortcode$ method", "label": "root_cause"}, {"sentence": "you might be wonder why you do not end up need this $shortcode$ call when you run similar function in your view", "label": "other"}, {"sentence": "the reason be that flask already handle the management of the application context for you when it be handle actual web request", "label": "root_cause"}, {"sentence": "the problem really only come up outside of these view function or other such callback such a when use your model in a one off script", "label": "root_cause"}, {"sentence": "the resolution be to push the application context yourself which can be do by do $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "this will push a new application context use the application of $shortcode$ remember there could be more than one application", "label": "other"}, {"sentence": "test", "label": "other"}, {"sentence": "another case where you would want to manipulate the stack be for test", "label": "other"}, {"sentence": "you could create a unit test that handle a request and you check the result $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<p>I had to use the <code>install</code> function instead:</p>\n\n<pre><code>conda install pandas=0.13.1\n</code></pre>\n", "title": "How do I revert to a previous package in Anaconda?", "question_id": 23974217, "labels": [{"sentence": "i have to use the $shortcode$ function instead $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<p>In Java or JavaScript: </p>\n\n<pre><code>driver.navigate().refresh();\n</code></pre>\n\n<p>This should refresh page.</p>\n", "title": "Refreshing web page by WebDriver when waiting for specific condition", "question_id": 10245641, "labels": [{"sentence": "in java or javascript $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "this should refresh page", "label": "other"}]}, {"answer_body": "<p>Logits simply means that the function operates on the unscaled output of earlier layers and that the relative scale to understand the units is linear.  It means, in particular, the sum of the inputs may not equal 1, that the values are <em>not</em> probabilities (you might have an input of 5).</p>\n\n<p><code>tf.nn.softmax</code> produces just the result of applying the <a href=\"https://en.wikipedia.org/wiki/Softmax_function\" rel=\"noreferrer\">softmax function</a> to an input tensor.  The softmax \"squishes\" the inputs so that <code>sum(input) = 1</code>:  it's a way of normalizing.  The shape of output of a softmax is the same as the input: it just normalizes the values.  The outputs of softmax <em>can</em> be interpreted as probabilities.</p>\n\n<pre><code>a = tf.constant(np.array([[.1, .3, .5, .9]]))\nprint s.run(tf.nn.softmax(a))\n[[ 0.16838508  0.205666    0.25120102  0.37474789]]\n</code></pre>\n\n<p>In contrast, <code>tf.nn.softmax_cross_entropy_with_logits</code> computes the cross entropy of the result after applying the softmax function (but it does it all together in a more mathematically careful way).  It's similar to the result of:</p>\n\n<pre><code>sm = tf.nn.softmax(x)\nce = cross_entropy(sm)\n</code></pre>\n\n<p>The cross entropy is a summary metric: it sums across the elements.  The output of <code>tf.nn.softmax_cross_entropy_with_logits</code> on a shape <code>[2,5]</code> tensor is of shape <code>[2,1]</code> (the first dimension is treated as the batch).</p>\n\n<p>If you want to do optimization to minimize the cross entropy <strong>AND</strong> you're softmaxing after your last layer, you should use <code>tf.nn.softmax_cross_entropy_with_logits</code> instead of doing it yourself, because it covers numerically unstable corner cases in the mathematically right way.  Otherwise, you'll end up hacking it by adding little epsilons here and there.</p>\n\n<p><strong>Edited 2016-02-07:</strong> \nIf you have single-class labels, where an object can only belong to one class, you might now  consider using <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> so that you don't have to convert your labels to a dense one-hot array.  This function was added after release 0.6.0.</p>\n", "title": "What is logits, softmax and softmax_cross_entropy_with_logits?", "question_id": 34240703, "labels": [{"sentence": "logits simply mean that the function operate on the unscaled output of early layer and that the relative scale to understand the unit be linear", "label": "root_cause"}, {"sentence": "it mean in particular the sum of the input may not equal 1 that the value be not probability you might have an input of 5", "label": "root_cause"}, {"sentence": "$shortcode$ produce just the result of apply the softmax function to an input tensor", "label": "root_cause"}, {"sentence": "the softmax squish the input so that $shortcode$ it be a way of normalize", "label": "root_cause"}, {"sentence": "the shape of output of a softmax be the same a the input it just normalize the value", "label": "root_cause"}, {"sentence": "the output of softmax can be interpret a probability $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "in contrast $shortcode$ compute the cross entropy of the result after apply the softmax function but it do it all together in a more mathematically careful way", "label": "root_cause"}, {"sentence": "it be similar to the result of $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "the cross entropy be a summary metric it sum across the element", "label": "other"}, {"sentence": "the output of $shortcode$ on a shape $shortcode$ tensor be of shape $shortcode$ the first dimension be treat a the batch", "label": "other"}, {"sentence": "if you want to do optimization to minimize the cross entropy and you be softmaxing after your last layer you should use $shortcode$ instead of do it yourself because it cover numerically unstable corner case in the mathematically right way", "label": "root_cause"}, {"sentence": "otherwise you you will end up hack it by add little epsilon here and there", "label": "root_cause"}, {"sentence": "edit 2016 02 07", "label": "other"}, {"sentence": "if you have single class label where an object can only belong to one class you might now consider use $shortcode$ so that you do not have to convert your label to a dense one hot array", "label": "solution"}, {"sentence": "this function be add after release 0 6 0", "label": "other"}]}, {"answer_body": "<p>Try: <code>git mergetool</code></p>\n\n<p>It opens a GUI that steps you through each conflict, and you get to choose how to merge.  Sometimes it requires a bit of hand editing afterwards, but usually it's enough by itself.  It is much better than doing the whole thing by hand certainly.</p>\n\n<p>As per @JoshGlover comment:</p>\n\n<p>The command doesn't necessarily open a GUI unless you install one. Running <code>git mergetool</code> for me resulted in <code>vimdiff</code> being used. You can install one of the following tools to use it instead: <code>meld</code>, <code>opendiff</code>, <code>kdiff3</code>, <code>tkdiff</code>, <code>xxdiff</code>, <code>tortoisemerge</code>, <code>gvimdiff</code>, <code>diffuse</code>, <code>ecmerge</code>, <code>p4merge</code>, <code>araxis</code>, <code>vimdiff</code>, <code>emerge</code>.</p>\n\n<p>Below is the sample procedure to use <code>vimdiff</code> for resolve merge conflicts. Based on <a href=\"http://www.rosipov.com/blog/use-vimdiff-as-git-mergetool/#fromHistor\" rel=\"noreferrer\">this link</a></p>\n\n<p><strong>Step 1</strong>: Run following commands in your terminal</p>\n\n<pre><code>git config merge.tool vimdiff\ngit config merge.conflictstyle diff3\ngit config mergetool.prompt false\n</code></pre>\n\n<p>This will set vimdiff as the default merge tool.</p>\n\n<p><strong>Step 2</strong>: Run following command in terminal</p>\n\n<pre><code>git mergetool\n</code></pre>\n\n<p><strong>Step 3</strong>: You will see a vimdiff display in following format </p>\n\n<pre><code>  \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n  \u2551       \u2551      \u2551        \u2551\n  \u2551 LOCAL \u2551 BASE \u2551 REMOTE \u2551\n  \u2551       \u2551      \u2551        \u2551\n  \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n  \u2551                       \u2551\n  \u2551        MERGED         \u2551\n  \u2551                       \u2551\n  \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre>\n\n<p>These 4 views are </p>\n\n<blockquote>\n  <p>LOCAL \u2013 this is file from the current branch  </p>\n  \n  <p>BASE \u2013 common ancestor, how file looked before both changes </p>\n  \n  <p>REMOTE \u2013 file you are merging into your branch </p>\n  \n  <p>MERGED \u2013 merge result, this is what gets saved in the repo</p>\n</blockquote>\n\n<p>You can navigate among these views using <kbd>ctrl</kbd>+<kbd>w</kbd>. You can directly reach MERGED view using <kbd>ctrl</kbd>+<kbd>w</kbd> followed by <kbd>j</kbd>.</p>\n\n<p>More info about vimdiff navigation <a href=\"https://stackoverflow.com/questions/4556184/vim-move-window-left-right\">here</a> and <a href=\"https://stackoverflow.com/questions/27151456/how-do-i-jump-to-the-next-prev-diff-in-git-difftool\">here</a></p>\n\n<p><strong>Step 4</strong>. You could edit the MERGED view the following way </p>\n\n<p>If you want to get changes from REMOTE</p>\n\n<pre><code>:diffg RE  \n</code></pre>\n\n<p>If you want to get changes from BASE</p>\n\n<pre><code>:diffg BA  \n</code></pre>\n\n<p>If you want to get changes from LOCAL</p>\n\n<pre><code>:diffg LO \n</code></pre>\n\n<p><strong>Step 5</strong>. Save, Exit, Commit and Clean up</p>\n\n<p><code>:wqa</code> save and exit from vi</p>\n\n<p><code>git commit -m \"message\"</code></p>\n\n<p><code>git clean</code> Remove extra files (e.g. *.orig) created by diff tool.</p>\n", "title": "How to resolve merge conflicts in Git", "question_id": 161813, "labels": [{"sentence": "try $shortcode$", "label": "solution"}, {"sentence": "it open a gui that step you through each conflict and you get to choose how to merge", "label": "root_cause"}, {"sentence": "sometimes it require a bite of hand edit afterwards but usually it be enough by itself", "label": "other"}, {"sentence": "it be much good than do the whole thing by hand certainly", "label": "other"}, {"sentence": "a per @joshglover comment", "label": "other"}, {"sentence": "the command do not necessarily open a gui unless you install one", "label": "root_cause"}, {"sentence": "run $shortcode$ for me result in $shortcode$ be use", "label": "solution"}, {"sentence": "you can install one of the follow tool to use it instead $shortcode$ $shortcode$ $shortcode$ $shortcode$ $shortcode$ $shortcode$ $shortcode$ $shortcode$ $shortcode$ $shortcode$ $shortcode$ $shortcode$ $shortcode$", "label": "solution"}, {"sentence": "below be the sample procedure to use $shortcode$ for resolve merge conflict", "label": "solution"}, {"sentence": "base on this link", "label": "other"}, {"sentence": "step 1 run follow command in your terminal $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "this will set vimdiff a the default merge tool", "label": "root_cause"}, {"sentence": "step 2 run follow command in terminal $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "step 3 you will see a vimdiff display in follow format $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "these 4 view be", "label": "other"}, {"sentence": "local \u2013 this be file from the current branch", "label": "other"}, {"sentence": "base \u2013 common ancestor how file look before both change", "label": "other"}, {"sentence": "remote \u2013 file you be merge into your branch", "label": "other"}, {"sentence": "merge \u2013 merge result this be what get save in the repo", "label": "other"}, {"sentence": "you can navigate among these view use ctrl+w", "label": "solution"}, {"sentence": "you can directly reach merge view use ctrl+w follow by j", "label": "solution"}, {"sentence": "more info about vimdiff navigation here and here", "label": "solution"}, {"sentence": "step 4", "label": "other"}, {"sentence": "you could edit the merge view the follow way", "label": "solution"}, {"sentence": "if you want to get change from remote $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "if you want to get change from base $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "if you want to get change from local $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "step 5", "label": "other"}, {"sentence": "save exit commit and clean up", "label": "other"}, {"sentence": "$shortcode$ save and exit from vi $shortcode$", "label": "other"}, {"sentence": "$shortcode$", "label": "other"}, {"sentence": "$shortcode$ remove extra file e g", "label": "other"}, {"sentence": "orig create by diff tool", "label": "other"}]}, {"answer_body": "<p>I made a diagram. The names follow the <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.LSTM\" rel=\"noreferrer\">PyTorch docs</a>, although I renamed <code>num_layers</code> to <code>w</code>.</p>\n\n<p><code>output</code> comprises all the hidden states in the last layer (\"last\" depth-wise, not time-wise). <code>(h_n, c_n)</code> comprises the hidden states after the last timestep, <em>t</em> = <em>n</em>, so you could potentially feed them into another LSTM.</p>\n\n<p><a href=\"https://i.stack.imgur.com/SjnTl.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/SjnTl.png\" alt=\"LSTM diagram\"></a></p>\n\n<p>The batch dimension is not included.</p>\n", "title": "What&#39;s the difference between &quot;hidden&quot; and &quot;output&quot; in PyTorch LSTM?", "question_id": 48302810, "labels": [{"sentence": "i make a diagram", "label": "other"}, {"sentence": "the name follow the pytorch doc although i rename $shortcode$ to $shortcode$", "label": "other"}, {"sentence": "$shortcode$ comprise all the hide state in the last layer last depth wise not time wise", "label": "other"}, {"sentence": "$shortcode$ comprise the hide state after the last timestep t = n so you could potentially fee them into another lstm", "label": "other"}, {"sentence": "the batch dimension be not include", "label": "other"}]}, {"answer_body": "<p>Use <code>findElements</code> instead of <code>findElement</code>.</p>\n\n<p><code>findElements</code> will return an empty list if no matching elements are found instead of an exception.</p>\n\n<p>To check that an element is present, you could try this</p>\n\n<pre><code>Boolean isPresent = driver.findElements(By.yourLocator).size() &gt; 0\n</code></pre>\n\n<p>This will return true if at least one element is found and false if it does not exist.</p>\n", "title": "Test if element is present using Selenium WebDriver?", "question_id": 7991522, "labels": [{"sentence": "use $shortcode$ instead of $shortcode$", "label": "solution"}, {"sentence": "$shortcode$ will return an empty list if no match element be find instead of an exception", "label": "root_cause"}, {"sentence": "to check that an element be present you could try this $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "this will return true if at least one element be find and false if it do not exist", "label": "root_cause"}]}, {"answer_body": "<p>This is actually on the <a href=\"http://nltk.org/\">main page of nltk.org</a>:</p>\n\n<pre><code>&gt;&gt;&gt; import nltk\n&gt;&gt;&gt; sentence = \"\"\"At eight o'clock on Thursday morning\n... Arthur didn't feel very good.\"\"\"\n&gt;&gt;&gt; tokens = nltk.word_tokenize(sentence)\n&gt;&gt;&gt; tokens\n['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning',\n'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n</code></pre>\n", "title": "How do I tokenize a string sentence in NLTK?", "question_id": 15057945, "labels": [{"sentence": "this be actually on the main page of nltk org $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<p>Another way to make a directory stay (almost) empty (in the repository) is to create a <code>.gitignore</code> file inside that directory that contains these four lines:</p>\n\n<pre><code># Ignore everything in this directory\n*\n# Except this file\n!.gitignore\n</code></pre>\n\n<p>Then you don't have to get the order right the way that you have to do in m104's <a href=\"https://stackoverflow.com/a/180917/32453\">solution</a>.</p>\n\n<p>This also gives the benefit that files in that directory won't show up as \"untracked\" when you do a git status.</p>\n\n<p>Making <a href=\"https://stackoverflow.com/users/554807/greenasjade\">@GreenAsJade</a>'s comment persistent:</p>\n\n<blockquote>\n  <p>I think it's worth noting that this solution does precisely what the question asked for, but is not perhaps what many people looking at this question will have been looking for. This solution guarantees that the directory remains empty. It says \"I truly never want files checked in here\". As opposed to \"I don't have any files to check in here, yet, but I need the directory here, files may be coming later\".</p>\n</blockquote>\n", "title": "How can I add an empty directory to a Git repository?", "question_id": 115983, "labels": [{"sentence": "another way to make a directory stay almost empty in the repository be to create a $shortcode$ file inside that directory that contain these four line $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "then you do not have to get the order right the way that you have to do in m104 s solution", "label": "other"}, {"sentence": "this also give the benefit that file in that directory will not show up a untracked when you do a git status", "label": "root_cause"}, {"sentence": "make @greenasjade s comment persistent", "label": "other"}, {"sentence": "i think it be worth note that this solution do precisely what the question ask for but be not perhaps what many people look at this question will have be look for", "label": "other"}, {"sentence": "this solution guarantee that the directory remain empty", "label": "other"}, {"sentence": "it say i truly never want file check in here", "label": "other"}, {"sentence": "a oppose to i do not have any file to check in here yet but i need the directory here file may be come late", "label": "other"}]}, {"answer_body": "<h2>What is this warning about?</h2>\n\n<p>Modern CPUs provide a lot of low-level instructions, besides the usual arithmetic and logic, known as extensions, e.g. SSE2, SSE4, AVX, etc. From the <a href=\"https://en.wikipedia.org/wiki/Advanced_Vector_Extensions\" rel=\"noreferrer\">Wikipedia</a>:</p>\n\n<blockquote>\n  <p><strong>Advanced Vector Extensions</strong> (<strong>AVX</strong>) are extensions to the x86 instruction\n  set architecture for microprocessors from Intel and AMD proposed by\n  Intel in March 2008 and first supported by Intel with the Sandy\n  Bridge processor shipping in Q1 2011 and later on by AMD with the\n  Bulldozer processor shipping in Q3 2011. AVX provides new features,\n  new instructions and a new coding scheme.</p>\n</blockquote>\n\n<p>In particular, AVX introduces <a href=\"https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation#Fused_multiply.E2.80.93add\" rel=\"noreferrer\">fused multiply-accumulate</a> (FMA) operations, which speed up linear algebra computation, namely dot-product, matrix multiply, convolution, etc. Almost every machine-learning training involves a great deal of these operations, hence will be faster on a CPU that supports AVX and FMA (up to 300%). The warning states that your CPU does support AVX (hooray!).</p>\n\n<p>I'd like to stress here: it's all about <strong>CPU only</strong>.</p>\n\n<h2>Why isn't it used then?</h2>\n\n<p>Because tensorflow default distribution is built <a href=\"https://github.com/tensorflow/tensorflow/issues/7778\" rel=\"noreferrer\">without CPU extensions</a>, such as SSE4.1, SSE4.2, AVX, AVX2, FMA, etc. The default builds (ones from <code>pip install tensorflow</code>) are intended to be compatible with as many CPUs as possible. Another argument is that even with these extensions CPU is a lot slower than a GPU, and it's expected for medium- and large-scale machine-learning training to be performed on a GPU.</p>\n\n<h2>What should you do?</h2>\n\n<p><strong>If you have a GPU</strong>, you shouldn't care about AVX support, because most expensive ops will be dispatched on a GPU device (unless explicitly set not to). In this case, you can simply ignore this warning by</p>\n\n<pre><code># Just disables the warning, doesn't enable AVX/FMA\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n</code></pre>\n\n<p>... or by setting <code>export TF_CPP_MIN_LOG_LEVEL=2</code> if you're on Unix. Tensorflow is working fine anyway, but you won't see these annoying warnings.</p>\n\n<hr>\n\n<p><strong>If you don't have a GPU</strong> and want to utilize CPU as much as possible, <strong>you should build tensorflow from the source optimized for <em>your</em> CPU</strong> with AVX, AVX2, and FMA enabled if your CPU supports them. It's been discussed in <a href=\"https://stackoverflow.com/q/41293077/712995\">this question</a> and also <a href=\"https://github.com/tensorflow/tensorflow/issues/8037\" rel=\"noreferrer\">this GitHub issue</a>. Tensorflow uses an ad-hoc build system called <a href=\"https://bazel.build/\" rel=\"noreferrer\">bazel</a> and building it is not that trivial, but is certainly doable. After this, not only will the warning disappear, tensorflow performance should also improve.</p>\n", "title": "Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2", "question_id": 47068709, "labels": [{"sentence": "what be this warn about", "label": "other"}, {"sentence": "modern cpu provide a lot of low level instruction besides the usual arithmetic and logic know a extension e g", "label": "root_cause"}, {"sentence": "sse2 sse4 avx etc", "label": "other"}, {"sentence": "from the wikipedia", "label": "other"}, {"sentence": "advance vector extension avx be extension to the x86 instruction", "label": "other"}, {"sentence": "set architecture for microprocessor from intel and amd propose by", "label": "other"}, {"sentence": "intel in march 2008 and first support by intel with the sandy", "label": "other"}, {"sentence": "bridge processor ship in q1 2011 and late on by amd with the", "label": "other"}, {"sentence": "bulldozer processor ship in q3 2011", "label": "other"}, {"sentence": "avx provide new feature", "label": "other"}, {"sentence": "new instruction and a new cod scheme", "label": "other"}, {"sentence": "in particular avx introduce fuse multiply accumulate fma operation which speed up linear algebra computation namely dot product matrix multiply convolution etc", "label": "root_cause"}, {"sentence": "almost every machine learn train involve a great deal of these operation hence will be fast on a cpu that support avx and fma up to 300%", "label": "root_cause"}, {"sentence": "the warn state that your cpu do support avx hooray", "label": "other"}, {"sentence": "i would like to stress here it be all about cpu only", "label": "other"}, {"sentence": "why be not it use then", "label": "other"}, {"sentence": "because tensorflow default distribution be build without cpu extension such a sse4 1 sse4 2 avx avx2 fma etc", "label": "other"}, {"sentence": "the default build one from $shortcode$ be intend to be compatible with a many cpu a possible", "label": "other"}, {"sentence": "another argument be that even with these extension cpu be a lot slow than a gpu and it be expect for medium and large scale machine learn train to be perform on a gpu", "label": "root_cause"}, {"sentence": "what should you do", "label": "other"}, {"sentence": "if you have a gpu you should not care about avx support because most expensive ops will be dispatch on a gpu device unless explicitly set not to", "label": "solution"}, {"sentence": "in this case you can simply ignore this warn by $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "", "label": "other"}, {"sentence": "or by set $shortcode$ if you be on unix", "label": "solution"}, {"sentence": "tensorflow be work fine anyway but you will not see these annoy warn", "label": "other"}, {"sentence": "if you do not have a gpu and want to utilize cpu a much a possible you should build tensorflow from the source optimize for your cpu with avx avx2 and fma enable if your cpu support them", "label": "solution"}, {"sentence": "it be be discus in this question and also this github issue", "label": "other"}, {"sentence": "tensorflow use an ad hoc build system call bazel and build it be not that trivial but be certainly doable", "label": "root_cause"}, {"sentence": "after this not only will the warn disappear tensorflow performance should also improve", "label": "other"}]}, {"answer_body": "<p>On the word2vec-toolkit mailing list Thomas Mensink has provided an <a href=\"https://groups.google.com/forum/#!topic/word2vec-toolkit/5Qh-x2O1lV4\" rel=\"noreferrer\">answer</a> in the form of a small C program that will convert a .bin file to text.  This is a modification of the distance.c file.  I replaced the original distance.c with Thomas's code below and rebuilt word2vec (make clean; make), and renamed the compiled distance to readbin.  Then <code>./readbin vector.bin</code> will create a text version of vector.bin.</p>\n\n<pre><code>//  Copyright 2013 Google Inc. All Rights Reserved.\n//\n//  Licensed under the Apache License, Version 2.0 (the \"License\");\n//  you may not use this file except in compliance with the License.\n//  You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n//  Unless required by applicable law or agreed to in writing, software\n//  distributed under the License is distributed on an \"AS IS\" BASIS,\n//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n//  See the License for the specific language governing permissions and\n//  limitations under the License.\n\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n#include &lt;math.h&gt;\n#include &lt;malloc.h&gt;\n\nconst long long max_size = 2000;         // max length of strings\nconst long long N = 40;                  // number of closest words that will be shown\nconst long long max_w = 50;              // max length of vocabulary entries\n\nint main(int argc, char **argv) {\n  FILE *f;\n  char file_name[max_size];\n  float len;\n  long long words, size, a, b;\n  char ch;\n  float *M;\n  char *vocab;\n  if (argc &lt; 2) {\n    printf(\"Usage: ./distance &lt;FILE&gt;\\nwhere FILE contains word projections in the BINARY FORMAT\\n\");\n    return 0;\n  }\n  strcpy(file_name, argv[1]);\n  f = fopen(file_name, \"rb\");\n  if (f == NULL) {\n    printf(\"Input file not found\\n\");\n    return -1;\n  }\n  fscanf(f, \"%lld\", &amp;words);\n  fscanf(f, \"%lld\", &amp;size);\n  vocab = (char *)malloc((long long)words * max_w * sizeof(char));\n  M = (float *)malloc((long long)words * (long long)size * sizeof(float));\n  if (M == NULL) {\n    printf(\"Cannot allocate memory: %lld MB    %lld  %lld\\n\", (long long)words * size * sizeof(float) / 1048576, words, size);\n    return -1;\n  }\n  for (b = 0; b &lt; words; b++) {\n    fscanf(f, \"%s%c\", &amp;vocab[b * max_w], &amp;ch);\n    for (a = 0; a &lt; size; a++) fread(&amp;M[a + b * size], sizeof(float), 1, f);\n    len = 0;\n    for (a = 0; a &lt; size; a++) len += M[a + b * size] * M[a + b * size];\n    len = sqrt(len);\n    for (a = 0; a &lt; size; a++) M[a + b * size] /= len;\n  }\n  fclose(f);\n  //Code added by Thomas Mensink\n  //output the vectors of the binary format in text\n  printf(\"%lld %lld #File: %s\\n\",words,size,file_name);\n  for (a = 0; a &lt; words; a++){\n    printf(\"%s \",&amp;vocab[a * max_w]);\n    for (b = 0; b&lt; size; b++){ printf(\"%f \",M[a*size + b]); }\n    printf(\"\\b\\b\\n\");\n  }  \n\n  return 0;\n}\n</code></pre>\n\n<p>I removed the \"\\b\\b\" from the <code>printf</code>.  </p>\n\n<p>By the way, the resulting text file still contained the text word and some unnecessary whitespace which I did not want for some numerical calculations.  I removed the initial text column and the trailing blank from each line with bash commands.</p>\n\n<pre><code>cut --complement -d ' ' -f 1 GoogleNews-vectors-negative300.txt &gt; GoogleNews-vectors-negative300_tuples-only.txt\nsed 's/ $//' GoogleNews-vectors-negative300_tuples-only.txt\n</code></pre>\n", "title": "Convert word2vec bin file to text", "question_id": 27324292, "labels": [{"sentence": "on the word2vec toolkit mail list thomas mensink have provide an answer in the form of a small c program that will convert a bin file to text", "label": "solution"}, {"sentence": "this be a modification of the distance c file", "label": "other"}, {"sentence": "i replace the original distance c with thomas s code below and rebuild word2vec make clean make and rename the compile distance to readbin", "label": "solution"}, {"sentence": "then $shortcode$ will create a text version of vector bin $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "i remove the \\b\\b from the $shortcode$", "label": "solution"}, {"sentence": "by the way the result text file still contain the text word and some unnecessary whitespace which i do not want for some numerical calculation", "label": "root_cause"}, {"sentence": "i remove the initial text column and the trail blank from each line with bash command $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<p>Unless your click is firing some kind of ajax call to populate your list, you don't actually need to execute the click.</p>\n\n<p>Just find the element and then enumerate the options, selecting the option(s) you want.</p>\n\n<p>Here is an example:</p>\n\n<pre><code>from selenium import webdriver\nb = webdriver.Firefox()\nb.find_element_by_xpath(\"//select[@name='element_name']/option[text()='option_text']\").click()\n</code></pre>\n\n<p>You can read more in: <br />\n<a href=\"https://sqa.stackexchange.com/questions/1355/unable-to-select-an-option-using-seleniums-python-webdriver\">https://sqa.stackexchange.com/questions/1355/unable-to-select-an-option-using-seleniums-python-webdriver</a></p>\n", "title": "How to select a drop-down menu option value with Selenium (Python)", "question_id": 7867537, "labels": [{"sentence": "unless your click be fire some kind of ajax call to populate your list you do not actually need to execute the click", "label": "root_cause"}, {"sentence": "just find the element and then enumerate the option select the option s you want", "label": "solution"}, {"sentence": "here be an example $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "you can read more in", "label": "solution"}, {"sentence": "$url$", "label": "other"}]}, {"answer_body": "<p>I just wanted to report my findings about loading a gensim embedding with PyTorch.</p>\n\n<hr>\n\n<ul>\n<li><h2>Solution for PyTorch <code>0.4.0</code> and newer:</h2></li>\n</ul>\n\n<p>From <code>v0.4.0</code> there is a new function <a href=\"https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding.from_pretrained\" rel=\"noreferrer\"><code>from_pretrained()</code></a> which makes loading an embedding very comfortable.\nHere is an example from the documentation.</p>\n\n<pre><code>&gt;&gt; # FloatTensor containing pretrained weights\n&gt;&gt; weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n&gt;&gt; embedding = nn.Embedding.from_pretrained(weight)\n&gt;&gt; # Get embeddings for index 1\n&gt;&gt; input = torch.LongTensor([1])\n&gt;&gt; embedding(input)\n</code></pre>\n\n<p>The weights from <a href=\"https://radimrehurek.com/gensim/\" rel=\"noreferrer\"><em>gensim</em></a> can easily be obtained by:</p>\n\n<pre><code>import gensim\nmodel = gensim.models.KeyedVectors.load_word2vec_format('path/to/file')\nweights = torch.FloatTensor(model.vectors) # formerly syn0, which is soon deprecated\n</code></pre>\n\n<hr>\n\n<ul>\n<li><h2>Solution for PyTorch version <code>0.3.1</code> and older:</h2></li>\n</ul>\n\n<p>I'm using version <code>0.3.1</code> and <a href=\"https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding.from_pretrained\" rel=\"noreferrer\"><code>from_pretrained()</code></a> isn't available in this version.</p>\n\n<p>Therefore I created my own <code>from_pretrained</code> so I can also use it with <code>0.3.1</code>.</p>\n\n<p><em>Code for <code>from_pretrained</code> for PyTorch versions <code>0.3.1</code> or lower:</em></p>\n\n<pre><code>def from_pretrained(embeddings, freeze=True):\n    assert embeddings.dim() == 2, \\\n         'Embeddings parameter is expected to be 2-dimensional'\n    rows, cols = embeddings.shape\n    embedding = torch.nn.Embedding(num_embeddings=rows, embedding_dim=cols)\n    embedding.weight = torch.nn.Parameter(embeddings)\n    embedding.weight.requires_grad = not freeze\n    return embedding\n</code></pre>\n\n<p>The embedding can be loaded then just like this:</p>\n\n<pre><code>embedding = from_pretrained(weights)\n</code></pre>\n\n<p>I hope this is helpful for someone.</p>\n", "title": "PyTorch / Gensim - How to load pre-trained word embeddings", "question_id": 49710537, "labels": [{"sentence": "i just want to report my find about load a gensim embed with pytorch", "label": "other"}, {"sentence": "solution for pytorch $shortcode$ and new", "label": "solution"}, {"sentence": "from $shortcode$ there be a new function $shortcode$ which make load an embed very comfortable", "label": "solution"}, {"sentence": "here be an example from the documentation $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "the weight from gensim can easily be obtain by $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "solution for pytorch version $shortcode$ and old", "label": "other"}, {"sentence": "i be use version $shortcode$ and $shortcode$ be not available in this version", "label": "root_cause"}, {"sentence": "therefore i create my own $shortcode$ so i can also use it with $shortcode$", "label": "solution"}, {"sentence": "code for $shortcode$ for pytorch version $shortcode$ or low $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "the embed can be load then just like this $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "i hope this be helpful for someone", "label": "other"}]}, {"answer_body": "<p>File | Invalidate Caches... and restarting PyCharm helps.</p>\n", "title": "PyCharm shows unresolved references error for valid code", "question_id": 11725519, "labels": [{"sentence": "file | invalidate cache", "label": "other"}, {"sentence": "and restart pycharm help", "label": "solution"}]}, {"answer_body": "<p>My understanding is you don't need to install Anaconda again to start using a different version of python. Instead, conda has the <a href=\"http://conda.pydata.org/docs/py2or3.html\">ability to separately manage python 2 and 3 environments</a>.</p>\n", "title": "Is it ok having both Anacondas 2.7 and 3.5 installed in the same time?", "question_id": 37442494, "labels": [{"sentence": "my understand be you do not need to install anaconda again to start use a different version of python", "label": "root_cause"}, {"sentence": "instead conda have the ability to separately manage python 2 and 3 environment", "label": "root_cause"}]}, {"answer_body": "<p>This is going to work :</p>\n\n<pre><code>In [1]: import torch\n\nIn [2]: torch.cuda.current_device()\nOut[2]: 0\n\nIn [3]: torch.cuda.device(0)\nOut[3]: &lt;torch.cuda.device at 0x7efce0b03be0&gt;\n\nIn [4]: torch.cuda.device_count()\nOut[4]: 1\n\nIn [5]: torch.cuda.get_device_name(0)\nOut[5]: 'GeForce GTX 950M'\n\nIn [6]: torch.cuda.is_available()\nOut[6]: True\n</code></pre>\n\n<p>This tells me the GPU <code>GeForce GTX 950M</code> is being used by <code>PyTorch</code>.</p>\n", "title": "How to check if pytorch is using the GPU?", "question_id": 48152674, "labels": [{"sentence": "this be go to work $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "this tell me the gpu $shortcode$ be be use by $shortcode$", "label": "root_cause"}]}, {"answer_body": "<p>It's a way of generating a valid URL, generally using data already obtained. For instance, using the title of an article to generate a URL. I'd advise to generate the slug, using a function, given a title (or other piece of data), rather than setting it manually.</p>\n\n<p>An example:</p>\n\n<pre class=\"lang-html prettyprint-override\"><code>&lt;title&gt; The 46 Year Old Virgin &lt;/title&gt;\n&lt;content&gt; A silly comedy movie &lt;/content&gt;\n&lt;slug&gt; the-46-year-old-virgin &lt;/slug&gt;\n</code></pre>\n\n<p>Now let's pretend that we have a Django model such as:</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>class Article(models.Model):\n    title = models.CharField(max_length=100)\n    content = models.TextField(max_length=1000)\n    slug = models.SlugField(max_length=40)\n</code></pre>\n\n<p>How would you reference this object with a URL, with a meaningful name? You could use Article.id so the URL would look like this:</p>\n\n<pre><code>www.example.com/article/23\n</code></pre>\n\n<p>Or, you could reference the title like so:</p>\n\n<pre><code>www.example.com/article/The 46 Year Old Virgin\n</code></pre>\n\n<p>Problem is, spaces aren't valid in URLs, they need to be replaced by <code>%20</code> which is ugly, making it the following:</p>\n\n<pre><code>www.example.com/article/The%2046%20Year%20Old%20Virgin\n</code></pre>\n\n<p>That's not solving our meaningful URL. Wouldn't this be better:</p>\n\n<pre><code>www.example.com/article/the-46-year-old-virgin\n</code></pre>\n\n<p>That's a slug. <code>the-46-year-old-virgin</code>. All letters are downcased and spaces are replaced by hyphens <code>-</code>. See the URL of this very webpage for an example!</p>\n", "title": "What is a &quot;slug&quot; in Django?", "question_id": 427102, "labels": [{"sentence": "it be a way of generate a valid url generally use data already obtain", "label": "solution"}, {"sentence": "for instance use the title of an article to generate a url", "label": "solution"}, {"sentence": "i would advise to generate the slug use a function give a title or other piece of data rather than set it manually", "label": "other"}, {"sentence": "an example $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "now let u pretend that we have a django model such a $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "how would you reference this object with a url with a meaningful name", "label": "other"}, {"sentence": "you could use article id so the url would look like this $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "or you could reference the title like so $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "problem be space be not valid in url they need to be replace by $shortcode$ which be ugly make it the follow $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "that be not solve our meaningful url", "label": "root_cause"}, {"sentence": "would not this be good $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "that be a slug", "label": "other"}, {"sentence": "$shortcode$", "label": "other"}, {"sentence": "all letter be downcased and space be replace by hyphen $shortcode$", "label": "other"}, {"sentence": "see the url of this very webpage for an example", "label": "other"}]}, {"answer_body": "<p>Yes, that is possible. Just create the objects by yourself, e.g.</p>\n\n<pre><code>import torch.utils.data as data_utils\n\ntrain = data_utils.TensorDataset(features, targets)\ntrain_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)\n</code></pre>\n\n<p>where <code>features</code> and <code>targets</code> are tensors. <code>features</code> has to be 2-D, i.e. a matrix where each line represents one training sample, and <code>targets</code> may be 1-D or 2-D, depending on whether you are trying to predict a scalar or a vector.</p>\n\n<p>Hope that helps!</p>\n\n<hr>\n\n<p><strong>EDIT</strong>: response to @sarthak's question</p>\n\n<p>Basically yes. If you create an object of type <code>TensorData</code>, then the constructor investigates whether the first dimensions of the feature tensor (which is actually called <code>data_tensor</code>) and the target tensor (called <code>target_tensor</code>) have the same length:</p>\n\n<pre><code>assert data_tensor.size(0) == target_tensor.size(0)\n</code></pre>\n\n<p>However, if you want to feed these data into a neural network subsequently, then you need to be careful. While convolution layers work on data like yours, (I think) all of the other types of layers expect the data to be given in matrix form. So, if you run into an issue like this, then an easy solution would be to convert your 4D-dataset (given as some kind of tensor, e.g. <code>FloatTensor</code>) into a matrix by using the method <code>view</code>. For your 5000xnxnx3 dataset, this would look like this:</p>\n\n<pre><code>2d_dataset = 4d_dataset.view(5000, -1)\n</code></pre>\n\n<p>(The value <code>-1</code> tells PyTorch to figure out the length of the second dimension automatically.)</p>\n", "title": "PyTorch: How to use DataLoaders for custom Datasets", "question_id": 41924453, "labels": [{"sentence": "yes that be possible", "label": "other"}, {"sentence": "just create the object by yourself e g $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "where $shortcode$ and $shortcode$ be tensor", "label": "other"}, {"sentence": "$shortcode$ have to be 2 d i e", "label": "other"}, {"sentence": "a matrix where each line represent one train sample and $shortcode$ may be 1 d or 2 d depend on whether you be try to predict a scalar or a vector", "label": "root_cause"}, {"sentence": "hope that help", "label": "other"}, {"sentence": "edit response to @sarthak s question", "label": "other"}, {"sentence": "basically yes", "label": "other"}, {"sentence": "if you create an object of type $shortcode$ then the constructor investigate whether the first dimension of the feature tensor which be actually call $shortcode$ and the target tensor call $shortcode$ have the same length $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "however if you want to fee these data into a neural network subsequently then you need to be careful", "label": "root_cause"}, {"sentence": "while convolution layer work on data like yours i think all of the other type of layer expect the data to be give in matrix form", "label": "root_cause"}, {"sentence": "so if you run into an issue like this then an easy solution would be to convert your 4d dataset give a some kind of tensor e g", "label": "root_cause"}, {"sentence": "$shortcode$ into a matrix by use the method $shortcode$", "label": "other"}, {"sentence": "for your 5000xnxnx3 dataset this would look like this $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "the value $shortcode$ tell pytorch to figure out the length of the second dimension automatically", "label": "other"}]}, {"answer_body": "<p>@cleros is pretty on the point about the use of <code>retain_graph=True</code>. In essence, it will retain any necessary information to calculate a certain variable, so that we can do backward pass on it.</p>\n\n<h2>An illustrative example</h2>\n\n<p><a href=\"https://i.stack.imgur.com/Stmud.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Stmud.png\" alt=\"enter image description here\"></a></p>\n\n<p>Suppose that we have a computation graph shown above. The variable <code>d</code> and <code>e</code> is the output, and <code>a</code> is the input. For example,</p>\n\n<pre><code>import torch\nfrom torch.autograd import Variable\na = Variable(torch.rand(1, 4), requires_grad=True)\nb = a**2\nc = b*2\nd = c.mean()\ne = c.sum()\n</code></pre>\n\n<p>when we do <code>d.backward()</code>, that is fine. After this computation, the part of graph that calculate <code>d</code> will be freed by default to save memory. So if we do <code>e.backward()</code>, the error message will pop up. In order to do <code>e.backward()</code>, we have to set the parameter <code>retain_graph</code> to <code>True</code> in <code>d.backward()</code>, i.e.,</p>\n\n<pre><code>d.backward(retain_graph=True)\n</code></pre>\n\n<p>As long as you use <code>retain_graph=True</code> in your backward method, you can do backward any time you want:</p>\n\n<pre><code>d.backward(retain_graph=True) # fine\ne.backward(retain_graph=True) # fine\nd.backward() # also fine\ne.backward() # error will occur!\n</code></pre>\n\n<p>More useful discussion can be found <a href=\"https://discuss.pytorch.org/t/runtimeerror-trying-to-backward-through-the-graph-a-second-time-but-the-buffers-have-already-been-freed-specify-retain-graph-true-when-calling-backward-the-first-time/6795/2?u=jdhao\" rel=\"noreferrer\">here</a>.</p>\n\n<h2>A real use case</h2>\n\n<p>Right now, a real use case is multi-task learning where you have multiple loss which maybe be at different layers. Suppose that you have 2 losses: <code>loss1</code> and <code>loss2</code> and they reside in different layers. In order to backprop the gradient of <code>loss1</code> and <code>loss2</code> w.r.t to the learnable weight of your network independently. You have to use <code>retain_graph=True</code> in <code>backward()</code> method in the first back-propagated loss.</p>\n\n<pre><code># suppose you first back-propagate loss1, then loss2 (you can also do the reverse)\nloss1.backward(retain_graph=True)\nloss2.backward() # now the graph is freed, and next process of batch gradient descent is ready\noptimizer.step() # update the network parameters\n</code></pre>\n", "title": "What does the parameter retain_graph mean in the Variable&#39;s backward() method?", "question_id": 46774641, "labels": [{"sentence": "@cleros be pretty on the point about the use of $shortcode$", "label": "other"}, {"sentence": "in essence it will retain any necessary information to calculate a certain variable so that we can do backward pas on it", "label": "root_cause"}, {"sentence": "an illustrative example", "label": "other"}, {"sentence": "suppose that we have a computation graph show above", "label": "other"}, {"sentence": "the variable $shortcode$ and $shortcode$ be the output and $shortcode$ be the input", "label": "other"}, {"sentence": "for example $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "when we do $shortcode$ that be fine", "label": "other"}, {"sentence": "after this computation the part of graph that calculate $shortcode$ will be free by default to save memory", "label": "root_cause"}, {"sentence": "so if we do $shortcode$ the error message will pop up", "label": "root_cause"}, {"sentence": "in order to do $shortcode$ we have to set the parameter $shortcode$ to $shortcode$ in $shortcode$ i e $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "a long a you use $shortcode$ in your backward method you can do backward any time you want $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "more useful discussion can be find here", "label": "solution"}, {"sentence": "a real use case", "label": "other"}, {"sentence": "right now a real use case be multi task learn where you have multiple loss which maybe be at different layer", "label": "other"}, {"sentence": "suppose that you have 2 loss $shortcode$ and $shortcode$ and they reside in different layer", "label": "other"}, {"sentence": "in order to backprop the gradient of $shortcode$ and $shortcode$ w r t to the learnable weight of your network independently", "label": "other"}, {"sentence": "you have to use $shortcode$ in $shortcode$ method in the first back propagate loss $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<p>To show only <strong>running containers</strong> use the given command:</p>\n\n<pre><code>docker ps\n</code></pre>\n\n<p>To show <strong>all containers</strong> use the given command:</p>\n\n<pre><code>docker ps -a\n</code></pre>\n\n<p>To show the <strong>latest created container</strong> (includes all states) use the given command:</p>\n\n<pre><code>docker ps -l\n</code></pre>\n\n<p>To show <strong>n last created containers</strong> (includes all states) use the given command:</p>\n\n<pre><code>docker ps -n=-1\n</code></pre>\n\n<p>To display <strong>total file sizes</strong> use the given command:</p>\n\n<pre><code>docker ps -s\n</code></pre>\n\n<p>The content presented above is from <a href=\"https://docs.docker.com/v1.11/engine/reference/commandline/ps/\" rel=\"noreferrer\">docker.com</a>.</p>\n\n<p>In the new version of Docker, commands are updated, and some management commands are added:</p>\n\n<pre><code>docker container ls\n</code></pre>\n\n<p>Is used to list all the running containers.</p>\n\n<pre><code>docker container ls -a\n</code></pre>\n\n<p>And then, if you want to clean them all,</p>\n\n<pre><code>docker rm $(docker ps -aq)\n</code></pre>\n\n<p>Is used to list all the containers created irrespective of its state.</p>\n\n<p>Here container is the management command.</p>\n", "title": "How to list containers in Docker", "question_id": 16840409, "labels": [{"sentence": "to show only run container use the give command $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "to show all container use the give command $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "to show the late create container include all state use the give command $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "to show n last create container include all state use the give command $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "to display total file size use the give command $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "the content present above be from docker com", "label": "other"}, {"sentence": "in the new version of docker command be update and some management command be add $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "be use to list all the run container $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "and then if you want to clean them all $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "be use to list all the container create irrespective of it state", "label": "solution"}, {"sentence": "here container be the management command", "label": "root_cause"}]}, {"answer_body": "<p>There are few operations on Tensor in PyTorch that do not really change the content of the tensor, but only how to convert indices in to tensor to byte location. These operations include:</p>\n\n<blockquote>\n  <p><code>narrow()</code>, <code>view()</code>, <code>expand()</code> and <code>transpose()</code></p>\n</blockquote>\n\n<p><em>For example:</em> when you call <code>transpose()</code>, PyTorch doesn't generate new tensor with new layout, it just modifies meta information in Tensor object so offset and stride are for new shape. The transposed tensor and original tensor are indeed sharing the memory!</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>x = torch.randn(3,2)\ny = torch.transpose(x, 0, 1)\nx[0, 0] = 42\nprint(y[0,0])\n# prints 42\n</code></pre>\n\n<p>This is where the concept of <em>contiguous</em> comes in. Above <code>x</code> is contiguous but <code>y</code> is not because its memory layout is different than a tensor of same shape made from scratch. Note that the word <em>\"contiguous\"</em> is bit misleading because its not that the content of tensor is spread out around disconnected blocks of memory. Here bytes are still allocated in one block of memory but the order of the elements is different!</p>\n\n<p>When you call <code>contiguous()</code>, it actually makes a copy of tensor so the order of elements would be same as if tensor of same shape created from scratch.</p>\n\n<p>Normally you don't need to worry about this. If PyTorch expects contiguous tensor but if its not then you will get <code>RuntimeError: input is not contiguous</code> and then you just add a call to <code>contiguous()</code>.</p>\n", "title": "PyTorch - contiguous()", "question_id": 48915810, "labels": [{"sentence": "there be few operation on tensor in pytorch that do not really change the content of the tensor but only how to convert index in to tensor to byte location", "label": "root_cause"}, {"sentence": "these operation include", "label": "root_cause"}, {"sentence": "$shortcode$ $shortcode$ $shortcode$ and $shortcode$", "label": "other"}, {"sentence": "for example when you call $shortcode$ pytorch do not generate new tensor with new layout it just modify meta information in tensor object so offset and stride be for new shape", "label": "root_cause"}, {"sentence": "the transpose tensor and original tensor be indeed share the memory $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "this be where the concept of contiguous come in", "label": "root_cause"}, {"sentence": "above $shortcode$ be contiguous but $shortcode$ be not because it memory layout be different than a tensor of same shape make from scratch", "label": "root_cause"}, {"sentence": "note that the word contiguous be bite mislead because it not that the content of tensor be spread out around disconnect block of memory", "label": "other"}, {"sentence": "here byte be still allocate in one block of memory but the order of the element be different", "label": "other"}, {"sentence": "when you call $shortcode$ it actually make a copy of tensor so the order of element would be same a if tensor of same shape create from scratch", "label": "root_cause"}, {"sentence": "normally you do not need to worry about this", "label": "other"}, {"sentence": "if pytorch expect contiguous tensor but if it not then you will get $shortcode$ and then you just add a call to $shortcode$", "label": "solution"}]}, {"answer_body": "<p>You can use <code>set_printoptions</code> to set the precision of the output:</p>\n\n<pre><code>import numpy as np\nx=np.random.random(10)\nprint(x)\n# [ 0.07837821  0.48002108  0.41274116  0.82993414  0.77610352  0.1023732\n#   0.51303098  0.4617183   0.33487207  0.71162095]\n\nnp.set_printoptions(precision=3)\nprint(x)\n# [ 0.078  0.48   0.413  0.83   0.776  0.102  0.513  0.462  0.335  0.712]\n</code></pre>\n\n<p>And <code>suppress</code> suppresses the use of scientific notation for small numbers:</p>\n\n<pre><code>y=np.array([1.5e-10,1.5,1500])\nprint(y)\n# [  1.500e-10   1.500e+00   1.500e+03]\nnp.set_printoptions(suppress=True)\nprint(y)\n# [    0.      1.5  1500. ]\n</code></pre>\n\n<p>See the <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.set_printoptions.html\" rel=\"noreferrer\">docs for set_printoptions</a> for other options.</p>\n\n<hr>\n\n<p><strong>To apply print options locally</strong>, using NumPy 1.15.0 or later, you could use the <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.printoptions.html\" rel=\"noreferrer\">numpy.printoptions</a> context manager. \nFor example, inside the <code>with-suite</code> <code>precision=3</code> and <code>suppress=True</code> are set:</p>\n\n<pre><code>x = np.random.random(10)\nwith np.printoptions(precision=3, suppress=True):\n    print(x)\n    # [ 0.073  0.461  0.689  0.754  0.624  0.901  0.049  0.582  0.557  0.348]\n</code></pre>\n\n<p>But outside the <code>with-suite</code> the print options are back to default settings:</p>\n\n<pre><code>print(x)    \n# [ 0.07334334  0.46132615  0.68935231  0.75379645  0.62424021  0.90115836\n#   0.04879837  0.58207504  0.55694118  0.34768638]\n</code></pre>\n\n<p>If you are using an earlier version of NumPy, you can create the context manager\nyourself. For example,</p>\n\n<pre><code>import numpy as np\nimport contextlib\n\n@contextlib.contextmanager\ndef printoptions(*args, **kwargs):\n    original = np.get_printoptions()\n    np.set_printoptions(*args, **kwargs)\n    try:\n        yield\n    finally: \n        np.set_printoptions(**original)\n\nx = np.random.random(10)\nwith printoptions(precision=3, suppress=True):\n    print(x)\n    # [ 0.073  0.461  0.689  0.754  0.624  0.901  0.049  0.582  0.557  0.348]\n</code></pre>\n\n<hr>\n\n<p><strong>To prevent zeros from being stripped from the end of floats:</strong></p>\n\n<p><code>np.set_printoptions</code> now has a <code>formatter</code> parameter which allows you to specify a format function for each type.</p>\n\n<pre><code>np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\nprint(x)\n</code></pre>\n\n<p>which prints</p>\n\n<pre><code>[ 0.078  0.480  0.413  0.830  0.776  0.102  0.513  0.462  0.335  0.712]\n</code></pre>\n\n<p>instead of </p>\n\n<pre><code>[ 0.078  0.48   0.413  0.83   0.776  0.102  0.513  0.462  0.335  0.712]\n</code></pre>\n", "title": "How to pretty-print a numpy.array without scientific notation and with given precision?", "question_id": 2891790, "labels": [{"sentence": "you can use $shortcode$ to set the precision of the output $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "and $shortcode$ suppress the use of scientific notation for small number $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "see the doc for set_printoptions for other option", "label": "solution"}, {"sentence": "to apply print option locally use numpy 1 15 0 or late you could use the numpy printoptions context manager", "label": "solution"}, {"sentence": "for example inside the $shortcode$ $shortcode$ and $shortcode$ be set $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "but outside the $shortcode$ the print option be back to default setting $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "if you be use an early version of numpy you can create the context manager", "label": "solution"}, {"sentence": "yourself", "label": "other"}, {"sentence": "for example $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "to prevent zero from be strip from the end of float", "label": "other"}, {"sentence": "$shortcode$ now have a $shortcode$ parameter which allow you to specify a format function for each type $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "which print $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "instead of $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<p>Whenever the Python interpreter reads a source file, it does two things:</p>\n\n<ul>\n<li><p>it sets a few special variables like <code>__name__</code>, and then</p></li>\n<li><p>it executes all of the code found in the file.</p></li>\n</ul>\n\n<p>Let's see how this works and how it relates to your question about the <code>__name__</code> checks we always see in Python scripts.</p>\n\n<h1>Code Sample</h1>\n\n<p>Let's use a slightly different code sample to explore how imports and scripts work.  Suppose the following is in a file called <code>foo.py</code>.</p>\n\n<pre><code># Suppose this is foo.py.\n\nprint(\"before import\")\nimport math\n\nprint(\"before functionA\")\ndef functionA():\n    print(\"Function A\")\n\nprint(\"before functionB\")\ndef functionB():\n    print(\"Function B {}\".format(math.sqrt(100)))\n\nprint(\"before __name__ guard\")\nif __name__ == '__main__':\n    functionA()\n    functionB()\nprint(\"after __name__ guard\")\n</code></pre>\n\n<h1>Special Variables</h1>\n\n<p>When the Python interpeter reads a source file, it first defines a few special variables. In this case, we care about the <code>__name__</code> variable.</p>\n\n<p><strong>When Your Module Is the Main Program</strong></p>\n\n<p>If you are running your module (the source file) as the main program, e.g.</p>\n\n<pre><code>python foo.py\n</code></pre>\n\n<p>the interpreter will assign the hard-coded string <code>\"__main__\"</code> to the <code>__name__</code> variable, i.e.</p>\n\n<pre><code># It's as if the interpreter inserts this at the top\n# of your module when run as the main program.\n__name__ = \"__main__\" \n</code></pre>\n\n<p><strong>When Your Module Is Imported By Another</strong></p>\n\n<p>On the other hand, suppose some other module is the main program and it imports your module. This means there's a statement like this in the main program, or in some other module the main program imports:</p>\n\n<pre><code># Suppose this is in some other main program.\nimport foo\n</code></pre>\n\n<p>In this case, the interpreter will look at the filename of your module, <code>foo.py</code>, strip off the <code>.py</code>, and assign that string to your module's <code>__name__</code> variable, i.e.</p>\n\n<pre><code># It's as if the interpreter inserts this at the top\n# of your module when it's imported from another module.\n__name__ = \"foo\"\n</code></pre>\n\n<h1>Executing the Module's Code</h1>\n\n<p>After the special variables are set up, the interpreter executes all the code in the module, one statement at a time. You may want to open another window on the side with the code sample so you can follow along with this explanation.</p>\n\n<p><strong>Always</strong></p>\n\n<ol>\n<li><p>It prints the string <code>\"before import\"</code> (without quotes).</p></li>\n<li><p>It loads the <code>math</code> module and assigns it to a variable called <code>math</code>. This is equivalent to replacing <code>import math</code> with the following (note that <code>__import__</code> is a low-level function in Python that takes a string and triggers the actual import):</p></li>\n</ol>\n\n<pre><code># Find and load a module given its string name, \"math\",\n# then assign it to a local variable called math.\nmath = __import__(\"math\")\n</code></pre>\n\n<ol start=\"3\">\n<li><p>It prints the string <code>\"before functionA\"</code>.</p></li>\n<li><p>It executes the <code>def</code> block, creating a function object, then assigning that function object to a variable called <code>functionA</code>.</p></li>\n<li><p>It prints the string <code>\"before functionB\"</code>.</p></li>\n<li><p>It executes the second <code>def</code> block, creating another function object, then assigning it to a variable called <code>functionB</code>.</p></li>\n<li><p>It prints the string <code>\"before __name__ guard\"</code>.</p></li>\n</ol>\n\n<p><strong>Only When Your Module Is the Main Program</strong></p>\n\n<ol start=\"8\">\n<li>If your module is the main program, then it will see that <code>__name__</code> was indeed set to <code>\"__main__\"</code> and it calls the two functions, printing the strings <code>\"Function A\"</code> and <code>\"Function B 10.0\"</code>.</li>\n</ol>\n\n<p><strong>Only When Your Module Is Imported by Another</strong></p>\n\n<ol start=\"8\">\n<li>(<strong>instead</strong>) If your module is not the main program but was imported by another one, then <code>__name__</code> will be <code>\"foo\"</code>, not <code>\"__main__\"</code>, and it'll skip the body of the <code>if</code> statement.</li>\n</ol>\n\n<p><strong>Always</strong></p>\n\n<ol start=\"9\">\n<li>It will print the string <code>\"after __name__ guard\"</code> in both situations.</li>\n</ol>\n\n<p><strong><em>Summary</em></strong></p>\n\n<p>In summary, here's what'd be printed in the two cases:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code># What gets printed if foo is the main program\nbefore import\nbefore functionA\nbefore functionB\nbefore __name__ guard\nFunction A\nFunction B 10.0\nafter __name__ guard\n</code></pre>\n\n<pre class=\"lang-none prettyprint-override\"><code># What gets printed if foo is imported as a regular module\nbefore import\nbefore functionA\nbefore functionB\nbefore __name__ guard\nafter __name__ guard\n</code></pre>\n\n<h1>Why Does It Work This Way?</h1>\n\n<p>You might naturally wonder why anybody would want this.  Well, sometimes you want to write a <code>.py</code> file that can be both used by other programs and/or modules as a module, and can also be run as the main program itself.  Examples:</p>\n\n<ul>\n<li><p>Your module is a library, but you want to have a script mode where it runs some unit tests or a demo.</p></li>\n<li><p>Your module is only used as a main program, but it has some unit tests, and the testing framework works by importing <code>.py</code> files like your script and running special test functions. You don't want it to try running the script just because it's importing the module.</p></li>\n<li><p>Your module is mostly used as a main program, but it also provides a programmer-friendly API for advanced users.</p></li>\n</ul>\n\n<p>Beyond those examples, it's elegant that running a script in Python is just setting up a few magic variables and importing the script. \"Running\" the script is a side effect of importing the script's module.</p>\n\n<h1>Food for Thought</h1>\n\n<ul>\n<li><p>Question: Can I have multiple <code>__name__</code> checking blocks?  Answer: it's strange to do so, but the language won't stop you.</p></li>\n<li><p>Suppose the following is in <code>foo2.py</code>.  What happens if you say <code>python foo2.py</code> on the command-line? Why?</p></li>\n</ul>\n\n<pre class=\"lang-py prettyprint-override\"><code># Suppose this is foo2.py.\n\ndef functionA():\n    print(\"a1\")\n    from foo2 import functionB\n    print(\"a2\")\n    functionB()\n    print(\"a3\")\n\ndef functionB():\n    print(\"b\")\n\nprint(\"t1\")\nif __name__ == \"__main__\":\n    print(\"m1\")\n    functionA()\n    print(\"m2\")\nprint(\"t2\")\n</code></pre>\n\n<ul>\n<li>Now, figure out what will happen if you remove the <code>__name__</code> check in <code>foo3.py</code>:</li>\n</ul>\n\n<pre class=\"lang-py prettyprint-override\"><code># Suppose this is foo3.py.\n\ndef functionA():\n    print(\"a1\")\n    from foo3 import functionB\n    print(\"a2\")\n    functionB()\n    print(\"a3\")\n\ndef functionB():\n    print(\"b\")\n\nprint(\"t1\")\nprint(\"m1\")\nfunctionA()\nprint(\"m2\")\nprint(\"t2\")\n</code></pre>\n\n<ul>\n<li>What will this do when used as a script?  When imported as a module?</li>\n</ul>\n\n<pre class=\"lang-py prettyprint-override\"><code># Suppose this is in foo4.py\n__name__ = \"__main__\"\n\ndef bar():\n    print(\"bar\")\n\nprint(\"before __name__ guard\")\nif __name__ == \"__main__\":\n    bar()\nprint(\"after __name__ guard\")\n</code></pre>\n", "title": "What does if __name__ == &quot;__main__&quot;: do?", "question_id": 419163, "labels": [{"sentence": "whenever the python interpreter read a source file it do two thing", "label": "root_cause"}, {"sentence": "it set a few special variable like $shortcode$ and then", "label": "root_cause"}, {"sentence": "it execute all of the code find in the file", "label": "root_cause"}, {"sentence": "let u see how this work and how it relate to your question about the $shortcode$ check we always see in python script", "label": "other"}, {"sentence": "code sample", "label": "other"}, {"sentence": "let u use a slightly different code sample to explore how import and script work", "label": "other"}, {"sentence": "suppose the follow be in a file call $shortcode$ $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "special variable", "label": "other"}, {"sentence": "when the python interpeter read a source file it first define a few special variable", "label": "root_cause"}, {"sentence": "in this case we care about the $shortcode$ variable", "label": "other"}, {"sentence": "when your module be the main program", "label": "other"}, {"sentence": "if you be run your module the source file a the main program e g $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "the interpreter will assign the hard cod string $shortcode$ to the $shortcode$ variable i e $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "when your module be import by another", "label": "other"}, {"sentence": "on the other hand suppose some other module be the main program and it import your module", "label": "root_cause"}, {"sentence": "this mean there be a statement like this in the main program or in some other module the main program import $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "in this case the interpreter will look at the filename of your module $shortcode$ strip off the $shortcode$ and assign that string to your module s $shortcode$ variable i e $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "execute the module s code", "label": "other"}, {"sentence": "after the special variable be set up the interpreter execute all the code in the module one statement at a time", "label": "root_cause"}, {"sentence": "you may want to open another window on the side with the code sample so you can follow along with this explanation", "label": "root_cause"}, {"sentence": "always", "label": "other"}, {"sentence": "it print the string $shortcode$ without quote", "label": "other"}, {"sentence": "it load the $shortcode$ module and assign it to a variable call $shortcode$", "label": "other"}, {"sentence": "this be equivalent to replace $shortcode$ with the follow note that $shortcode$ be a low level function in python that take a string and trigger the actual import $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "it print the string $shortcode$", "label": "other"}, {"sentence": "it execute the $shortcode$ block create a function object then assign that function object to a variable call $shortcode$", "label": "root_cause"}, {"sentence": "it print the string $shortcode$", "label": "other"}, {"sentence": "it execute the second $shortcode$ block create another function object then assign it to a variable call $shortcode$", "label": "root_cause"}, {"sentence": "it print the string $shortcode$", "label": "other"}, {"sentence": "only when your module be the main program", "label": "other"}, {"sentence": "if your module be the main program then it will see that $shortcode$ be indeed set to $shortcode$ and it call the two function print the string $shortcode$ and $shortcode$", "label": "root_cause"}, {"sentence": "only when your module be import by another", "label": "other"}, {"sentence": "instead if your module be not the main program but be import by another one then $shortcode$ will be $shortcode$ not $shortcode$ and it will skip the body of the $shortcode$ statement", "label": "other"}, {"sentence": "always", "label": "other"}, {"sentence": "it will print the string $shortcode$ in both situation", "label": "other"}, {"sentence": "summary", "label": "other"}, {"sentence": "in summary here s what d be print in the two case $longcode$", "label": "other"}, {"sentence": "$longcode$ $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "why do it work this way", "label": "other"}, {"sentence": "you might naturally wonder why anybody would want this", "label": "other"}, {"sentence": "well sometimes you want to write a $shortcode$ file that can be both use by other program and/or module a a module and can also be run a the main program itself", "label": "root_cause"}, {"sentence": "example", "label": "other"}, {"sentence": "your module be a library but you want to have a script mode where it run some unit test or a demo", "label": "other"}, {"sentence": "your module be only use a a main program but it have some unit test and the test framework work by import $shortcode$ file like your script and run special test function", "label": "other"}, {"sentence": "you do not want it to try run the script just because it be import the module", "label": "other"}, {"sentence": "your module be mostly use a a main program but it also provide a programmer friendly api for advance user", "label": "other"}, {"sentence": "beyond those example it be elegant that run a script in python be just set up a few magic variable and import the script", "label": "other"}, {"sentence": "run the script be a side effect of import the script s module", "label": "other"}, {"sentence": "food for think", "label": "other"}, {"sentence": "question can i have multiple $shortcode$ check block", "label": "other"}, {"sentence": "answer it be strange to do so but the language will not stop you", "label": "other"}, {"sentence": "suppose the follow be in $shortcode$", "label": "other"}, {"sentence": "what happen if you say $shortcode$ on the command line", "label": "other"}, {"sentence": "why $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "now figure out what will happen if you remove the $shortcode$ check in $shortcode$ $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "what will this do when use a a script", "label": "other"}, {"sentence": "when import a a module $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<p><code>cd</code> into the directory or a parent directory (with the intended directory you will work nested in it).</p>\n\n<p>Note it must be a folder (<code>E:\\&gt;</code>   --- This will not work)</p>\n\n<p>Then just run the command <code>jupyter notebook</code></p>\n", "title": "How to change the Jupyter start-up folder", "question_id": 35254852, "labels": [{"sentence": "$shortcode$ into the directory or a parent directory with the intend directory you will work nest in it", "label": "solution"}, {"sentence": "note it must be a folder $shortcode$ this will not work", "label": "root_cause"}, {"sentence": "then just run the command $shortcode$", "label": "solution"}]}, {"answer_body": "<p>With debug turned off Django won't handle static files for you any more - your production web server (Apache or something) should take care of that.</p>\n", "title": "Why does DEBUG=False setting make my django Static Files Access fail?", "question_id": 5836674, "labels": [{"sentence": "with debug turn off django will not handle static file for you any more your production web server apache or something should take care of that", "label": "root_cause"}]}, {"answer_body": "<p>Use current_url element. Example:</p>\n\n<pre><code>print browser.current_url\n</code></pre>\n", "title": "How do I get current URL in Selenium Webdriver 2 Python?", "question_id": 15985339, "labels": [{"sentence": "use current_url element", "label": "solution"}, {"sentence": "example $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<p>conda is the package manager. Anaconda is a set of about a hundred packages including conda, numpy, scipy, ipython notebook, and so on. </p>\n\n<p>You installed Miniconda, which is a smaller alternative to Anaconda that is just conda and its dependencies, not those listed above. </p>\n\n<p>Once you have Miniconda, you can easily install Anaconda into it with <code>conda install anaconda</code>.</p>\n", "title": "What are the differences between Conda and Anaconda", "question_id": 30034840, "labels": [{"sentence": "conda be the package manager", "label": "root_cause"}, {"sentence": "anaconda be a set of about a hundred package include conda numpy scipy ipython notebook and so on", "label": "root_cause"}, {"sentence": "you instal miniconda which be a small alternative to anaconda that be just conda and it dependency not those list above", "label": "other"}, {"sentence": "once you have miniconda you can easily install anaconda into it with $shortcode$", "label": "other"}]}, {"answer_body": "<p>Just for completeness (but it's 2018, so maybe things changed since this question was posted): you can actually install a Jupyter Python kernel in your Django environment that will then connect (run under) a different Jupyter server/environment (one where you've installed widgets, extensions, changed the theme, etc.). <code>django_extensions</code> right now still does only part of the required work :-)</p>\n\n<p>This assumes you have a Jupyter virtual environment that's separate from Django's one and whose kernels/extensions are installed with <code>--user</code>. All the Jupyter extensions (and their dependencies) are installed in this venv instead of the Django's one/ones (you'll still need pandas, matplotlib, etc. in the Django environment if you need to use them together with Django code).</p>\n\n<p>In your Django virtual environment (that can run a different version of Python, including a version 2 interpreter) install the ipython kernel:</p>\n\n<pre><code>pip install -U ipykernel\nipython kernel install --user --name='environment_name' --display-name='Your Project'\n</code></pre>\n\n<p>This will create a kernel configuration directory with the specified -\u2013name in your user\u2019s Jupyter kernel directory (on Linux it's <code>~/.jupyter/</code> while on OSX it\u2019s <code>~/Library/Jupyter/</code>) containing its kernel.json file and images/icons (by default the default Jupyter icon for the kernel we\u2019re installing are used). This kernel will run inside the virtual environment what was active at creation, thus using the exact same version of python and all the installed modules used by our Django project.</p>\n\n<p>Running <code>./manage.py shell_plus --notebook</code> does something very similar, but in addition to requiring everything (including the Jupyter server and all the extensions) installed in the current venv, it\u2019s also unable to run notebooks in directories different from the project\u2019s root (the one containing <code>./manage.py</code>). In addition it\u2019ll run the kernel using the first executable called python it finds on the path, not the virtual environment\u2019s one, making it misbehave when not started from the command line inside an active Django virtual environment.</p>\n\n<p>To fix these problems so that we're able to create a Notebook running inside any Django project we have so configured and to be able to run notebooks stored anywhere on the filesystem, we need to:</p>\n\n<ol>\n<li>make sure the first \u2018argv\u2019 parameter contains the full path to the python interpreter contained in the virtual environment</li>\n<li>add (if not already present) an \u2018env\u2019 section that will contain shell environment variables, then use these to tell Python where to find our project and which Django settings it should use. We do this by adding something like the following:</li>\n</ol>\n\n<pre><code>   \"env\": {\n      \"DJANGO_SETTINGS_MODULE\": \"my_project.settings\",\n      \"PYTHONPATH\": \"$PYTHONPATH:/home/projectuser/projectfolder/my_project\"\n   }\n</code></pre>\n\n<ol start=\"3\">\n<li>optional: change \u2018display_name\u2019 to be human friendly and replace the icons.</li>\n</ol>\n\n<p>editing this environment kernel.json file you'll see something similar:</p>\n\n<pre><code>{\n \"display_name\": \"My Project\", \n \"language\": \"python\", \n \"env\": {\n  \"DJANGO_SETTINGS_MODULE\": \"my_project.settings\",\n  \"PYTHONPATH\": \"$PYTHONPATH:/home/projectuser/projectfolder/my_project\"\n },\n \"argv\": [\n  \"/home/projectuser/.pyenv/versions/2.7.15/envs/my_project_venv/bin/python\", \n  \"-m\", \n  \"ipykernel_launcher\", \n  \"-f\", \n  \"{connection_file}\",\n  \"--ext\",\n  \"django_extensions.management.notebook_extension\"\n ]\n}\n</code></pre>\n\n<p>Notable lines:</p>\n\n<ul>\n<li><p><strong>\"DJANGO_SETTINGS_MODULE\": \"my_project.settings\"</strong>: your settings, usually as seen inside your project's manage.py</p></li>\n<li><p><strong>\"PYTHONPATH\": \"$PYTHONPATH:/home/projectuser/projectfolder/my_project\"</strong>: PYTHONPATH is extended to include your project's main directory (the one containing manage.py) so that settings can be found even if the kernel isn't run in that exact directory (here django_extensions will use a generic <code>python</code>, thus running the wrong virtual environment unless the whole Jupyter server is launched from inside it: adding this to the kernel.json created by django_extensions will enable it to run notebooks anywhere in the Django project directory)</p></li>\n<li><p><strong>\"/home/projectuser/.pyenv/versions/2.7.15/envs/my_project_venv/bin/python\"</strong>: first argument (argv list) of the kernel execution, should be the full path to your project's virtual environment's python interpreter (this is another thing django_extensions gets wrong: fixing this will allow any notebook server to run that specific Django environment's kernel with all its installed modules)</p></li>\n<li><p><strong>\"django_extensions.management.notebook_extension\"</strong>: this is the extension that will load the 'shell_plus' functionality in the notebook (optional but useful :-) )</p></li>\n</ul>\n", "title": "How do I set up Jupyter/IPython Notebook for Django?", "question_id": 35483328, "labels": [{"sentence": "just for completeness but it be 2018 so maybe thing change since this question be post you can actually install a jupyter python kernel in your django environment that will then connect run under a different jupyter server/environment one where you have instal widget extension change the theme etc", "label": "root_cause"}, {"sentence": "$shortcode$ right now still do only part of the require work", "label": "solution"}, {"sentence": "this assume you have a jupyter virtual environment that be separate from django s one and whose kernels/extensions be instal with $shortcode$", "label": "root_cause"}, {"sentence": "all the jupyter extension and their dependency be instal in this venv instead of the django s one/ones you you will still need panda matplotlib etc", "label": "root_cause"}, {"sentence": "in the django environment if you need to use them together with django code", "label": "other"}, {"sentence": "in your django virtual environment that can run a different version of python include a version 2 interpreter install the ipython kernel $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "this will create a kernel configuration directory with the specify \u2013name in your user\u2019s jupyter kernel directory on linux it be $shortcode$ while on osx it\u2019s $shortcode$ contain it kernel json file and images/icons by default the default jupyter icon for the kernel we\u2019re instal be use", "label": "root_cause"}, {"sentence": "this kernel will run inside the virtual environment what be active at creation thus use the exact same version of python and all the instal module use by our django project", "label": "root_cause"}, {"sentence": "run $shortcode$ do something very similar but in addition to require everything include the jupyter server and all the extension instal in the current venv it\u2019s also unable to run notebook in directory different from the project\u2019s root the one contain $shortcode$", "label": "solution"}, {"sentence": "in addition it\u2019ll run the kernel use the first executable call python it find on the path not the virtual environment\u2019s one make it misbehave when not start from the command line inside an active django virtual environment", "label": "solution"}, {"sentence": "to fix these problem so that we be able to create a notebook run inside any django project we have so configure and to be able to run notebook store anywhere on the filesystem we need to", "label": "solution"}, {"sentence": "make sure the first \u2018argv\u2019 parameter contain the full path to the python interpreter contain in the virtual environment", "label": "solution"}, {"sentence": "add if not already present an \u2018env\u2019 section that will contain shell environment variable then use these to tell python where to find our project and which django setting it should use", "label": "solution"}, {"sentence": "we do this by add something like the follow $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "optional change \u2018display_name\u2019 to be human friendly and replace the icon", "label": "other"}, {"sentence": "edit this environment kernel json file you you will see something similar $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "notable line", "label": "other"}, {"sentence": "django_settings_module my_project setting your setting usually a see inside your project s manage py", "label": "other"}, {"sentence": "pythonpath $pythonpath /home/projectuser/projectfolder/my_project pythonpath be extend to include your project s main directory the one contain manage py so that setting can be find even if the kernel be not run in that exact directory here django_extensions will use a generic $shortcode$ thus run the wrong virtual environment unless the whole jupyter server be launch from inside it add this to the kernel json create by django_extensions will enable it to run notebook anywhere in the django project directory", "label": "other"}, {"sentence": "/home/projectuser/ pyenv/versions/2 7 15/envs/my_project_venv/bin/python first argument argv list of the kernel execution should be the full path to your project s virtual environment s python interpreter this be another thing django_extensions get wrong fix this will allow any notebook server to run that specific django environment s kernel with all it instal module", "label": "other"}, {"sentence": "django_extensions management notebook_extension this be the extension that will load the shell_plus functionality in the notebook optional but useful", "label": "other"}]}, {"answer_body": "<p>You mustn't use quotation marks around the name of the image files in markdown!</p>\n\n<p>If you carefully read your error message, you will see the two <code>%22</code> parts in the link. That is the html encoded quotation mark. </p>\n\n<p>You have to change the line</p>\n\n<pre><code>![title](\"img/picture.png\")\n</code></pre>\n\n<p>to </p>\n\n<pre><code>![title](img/picture.png)</code></pre>\n\n<p><strong>UPDATE</strong></p>\n\n<p>It is assumed, that you have the following file structure and that you run the  <code>jupyter notebook</code> command in the directory where the file <code>example.ipynb</code> (&lt;-- contains the markdown for the image)  is stored:</p>\n\n<pre><code>/\n+-- example.ipynb\n+-- img\n    +-- picture.png\n</code></pre>\n", "title": "How to embed image or picture in jupyter notebook, either from a local machine or from a web resource?", "question_id": 32370281, "labels": [{"sentence": "you must not use quotation mark around the name of the image file in markdown", "label": "other"}, {"sentence": "if you carefully read your error message you will see the two $shortcode$ part in the link", "label": "root_cause"}, {"sentence": "that be the html encode quotation mark", "label": "other"}, {"sentence": "you have to change the line $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "to $shortcode$", "label": "solution"}, {"sentence": "$shortcode$", "label": "other"}, {"sentence": "update", "label": "other"}, {"sentence": "it be assume that you have the follow file structure and that you run the $shortcode$ command in the directory where the file $shortcode$ < contain the markdown for the image be store $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}]}, {"answer_body": "<p>Yes, here is the answer given a NumPy array, <code>array</code>, and a value, <code>item</code>, to search for:</p>\n\n<pre><code>itemindex = numpy.where(array==item)\n</code></pre>\n\n<p>The result is a tuple with first all the row indices, then all the column indices.</p>\n\n<p>For example, if an array is two dimensions and it contained your item at two locations then</p>\n\n<pre><code>array[itemindex[0][0]][itemindex[1][0]]\n</code></pre>\n\n<p>would be equal to your item and so would</p>\n\n<pre><code>array[itemindex[0][1]][itemindex[1][1]]\n</code></pre>\n\n<p><a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html\" rel=\"noreferrer\">numpy.where</a></p>\n", "title": "Is there a NumPy function to return the first index of something in an array?", "question_id": 432112, "labels": [{"sentence": "yes here be the answer give a numpy array $shortcode$ and a value $shortcode$ to search for $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "the result be a tuple with first all the row index then all the column index", "label": "root_cause"}, {"sentence": "for example if an array be two dimension and it contain your item at two location then $longcode$", "label": "root_cause"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "would be equal to your item and so would $longcode$", "label": "other"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "numpy where", "label": "other"}]}, {"answer_body": "<p>Add the following to the top of your file   <code># coding=utf-8</code></p>\n\n<p>If you go to the link in the error you can seen the reason why:</p>\n\n<p><strong>Defining the Encoding</strong></p>\n\n<p><em>Python will default to ASCII as standard encoding if no other\n    encoding hints are given.\n    To define a source code encoding, a magic comment must\n    be placed into the source files either as first or second\n    line in the file, such as:\n          # coding=</em></p>\n", "title": "Python NLTK: SyntaxError: Non-ASCII character &#39;\\xc3&#39; in file (Senitment Analysis -NLP)", "question_id": 26899235, "labels": [{"sentence": "add the follow to the top of your file $shortcode$", "label": "solution"}, {"sentence": "if you go to the link in the error you can see the reason why", "label": "other"}, {"sentence": "define the encode", "label": "other"}, {"sentence": "python will default to ascii a standard encode if no other", "label": "root_cause"}, {"sentence": "encode hint be give", "label": "other"}, {"sentence": "to define a source code encode a magic comment must", "label": "other"}, {"sentence": "be place into the source file either a first or second", "label": "root_cause"}, {"sentence": "line in the file such a", "label": "other"}, {"sentence": "coding=", "label": "other"}]}, {"answer_body": "<p><code>in</code> is the intended way to test for the existence of a key in a <code>dict</code>.</p>\n\n<pre><code>d = dict()\n\nfor i in range(100):\n    key = i % 10\n    if key in d:\n        d[key] += 1\n    else:\n        d[key] = 1\n</code></pre>\n\n<p>If you wanted a default, you can always use <code>dict.get()</code>:</p>\n\n<pre><code>d = dict()\n\nfor i in range(100):\n    key = i % 10\n    d[key] = d.get(key, 0) + 1\n</code></pre>\n\n<p>... and if you wanted to always ensure a default value for any key you can use <code>defaultdict</code> from the <code>collections</code> module, like so:</p>\n\n<pre><code>from collections import defaultdict\n\nd = defaultdict(int)\n\nfor i in range(100):\n    d[i % 10] += 1\n</code></pre>\n\n<p>... but in general, the <code>in</code> keyword is the best way to do it.</p>\n", "title": "Check if a given key already exists in a dictionary", "question_id": 1602934, "labels": [{"sentence": "$shortcode$ be the intend way to test for the existence of a key in a $shortcode$ $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "if you want a default you can always use $shortcode$ $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "", "label": "other"}, {"sentence": "and if you want to always ensure a default value for any key you can use $shortcode$ from the $shortcode$ module like so $longcode$", "label": "solution"}, {"sentence": "$longcode$", "label": "other"}, {"sentence": "", "label": "other"}, {"sentence": "but in general the $shortcode$ keyword be the best way to do it", "label": "solution"}]}]